{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR预估\n",
    "\n",
    "资料&&代码整理by@寒小阳\n",
    "\n",
    "reference：\n",
    "* [《广告点击率预估是怎么回事？》](https://zhuanlan.zhihu.com/p/23499698)\n",
    "* [从ctr预估问题看看f(x)设计—DNN篇](https://zhuanlan.zhihu.com/p/28202287)\n",
    "* [Atomu2014 product_nets](https://github.com/Atomu2014/product-nets)\n",
    "\n",
    "关于CTR预估的背景推荐大家看欧阳辰老师在知乎的文章[《广告点击率预估是怎么回事？》](https://zhuanlan.zhihu.com/p/23499698)，感谢欧阳辰老师并在这里做一点小小的摘抄。\n",
    "\n",
    ">点击率预估是广告技术的核心算法之一，它是很多广告算法工程师喜爱的战场。一直想介绍一下点击率预估，但是涉及公式和模型理论太多，怕说不清楚，读者也不明白。所以，这段时间花了一些时间整理点击率预估的知识，希望在尽量不使用数据公式的情况下，把大道理讲清楚，给一些不愿意看公式的同学一个Cook Book。\n",
    "\n",
    "> ### 点击率预测是什么？\n",
    "\n",
    "> * 点击率预测是对每次广告的点击情况做出预测，可以判定这次为点击或不点击，也可以给出点击的概率，有时也称作pClick。\n",
    "\n",
    "> ### 点击率预测和推荐算法的不同？\n",
    "\n",
    "> * 广告中点击率预估需要给出精准的点击概率，A点击率0.3%  , B点击率0.13%等，需要结合出价用于排序使用；推荐算法很多时候只需要得出一个最优的次序A>B>C即可；\n",
    "\n",
    "> ### 搜索和非搜索广告点击率预测的区别\n",
    "\n",
    "> * 搜索中有强搜索信号-“查询词(Query)”，查询词和广告内容的匹配程度很大程度影响了点击概率； 点击率也高，PC搜索能到达百分之几的点击率。\n",
    "\n",
    "> * 非搜索广告（例如展示广告，信息流广告），点击率的计算很多来源于用户的兴趣和广告特征，上下文环境；移动信息流广告的屏幕比较大，用户关注度也比较集中，好位置也能到百分之几的点击率。对于很多文章底部的广告，点击率非常低，用户关注度也不高，常常是千分之几，甚至更低；\n",
    "\n",
    "> ### 如何衡量点击率预测的准确性？\n",
    "\n",
    "> AUC是常常被用于衡量点击率预估的准确性的方法；理解AUC之前，需要理解一下Precision/Recall；对于一个分类器，我们通常将结果分为：TP,TN,FP,FN。\n",
    "> ![](https://pic4.zhimg.com/80/v2-1641631d510e3c660c208780a0b9d11e_hd.jpg)\n",
    "\n",
    "\n",
    "> 本来用Precision=TP/(TP+FP)，Recall=TP/P，也可以用于评估点击率算法的好坏，毕竟这是一种监督学习，每一次预测都有正确答案。但是，这种方法对于测试数据样本的依赖性非常大，稍微不同的测试数据集合，结果差异非常大。那么，既然无法使用简单的单点Precision/Recall来描述，我们可以考虑使用一系列的点来描述准确性。做法如下：\n",
    "\n",
    "> * 找到一系列的测试数据，点击率预估分别会对每个测试数据给出点击/不点击，和Confidence Score。\n",
    "\n",
    "> * 按照给出的Score进行排序，那么考虑如果将Score作为一个Thresholds的话，考虑这个时候所有数据的 TP Rate 和 FP Rate； 当Thresholds分数非常高时，例如0.9，TP数很小，NP数很大，因此TP率不会太高； \n",
    "> ![](https://pic2.zhimg.com/80/v2-77e1e16ee58697a316cfe2728be86efe_hd.jpg)\n",
    "> ![](https://pic2.zhimg.com/80/v2-10666128633da6ea072a4c87f21d6bdf_hd.jpg)\n",
    "> ![](https://pic3.zhimg.com/80/v2-d70746453ced3e20a04f297169bd12bf_hd.jpg)\n",
    "> * 当选用不同Threshold时候，画出来的ROC曲线，以及下方AUC面积\n",
    "> * 我们计算这个曲线下面的面积就是所谓的AUC值；AUC值越大，预测约准确。\n",
    "\n",
    "\n",
    "> ### 为什么要使用AUC曲线\n",
    "\n",
    "> 既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。AUC对样本的比例变化有一定的容忍性。AUC的值通常在0.6-0.85之间。\n",
    "\n",
    "\n",
    "> ### 如何来进行点击率预测？\n",
    "\n",
    "> 点击率预测可以考虑为一个黑盒，输入一堆信号，输出点击的概率。这些信号就包括如下信号\n",
    "\n",
    "> * **广告**：历史点击率，文字，格式，图片等等\n",
    "> * **环境**：手机型号，时间媒体，位置，尺寸，曝光时间，网络IP，上网方式，代理等\n",
    "> * **用户**：基础属性（男女，年龄等），兴趣属性（游戏，旅游等），历史浏览，点击行为，电商行为\n",
    "> * **信号的粒度**：\n",
    "> `Low Level : 数据来自一些原始访问行为的记录，例如用户是否点击过Landing Page，流量IP等。这些特征可以用于粗选，模型简单，`\n",
    "> `High Level: 特征来自一些可解释的数据，例如兴趣标签，性别等`\n",
    "\n",
    "\n",
    "> * **特征编码Feature Encoding：**\n",
    "\n",
    "> `特征离散化：把连续的数字，变成离散化，例如温度值可以办成多个温度区间。`\n",
    "\n",
    "> `特征交叉： 把多个特征进行叫交叉的出的值，用于训练，这种值可以表示一些非线性的关系。例如，点击率预估中应用最多的就是广告跟用户的交叉特征、广告跟性别的交叉特征，广告跟年龄的交叉特征，广告跟手机平台的交叉特征，广告跟地域的交叉特征等等。`\n",
    "\n",
    "> * **特征选取（Feature Selection）：**\n",
    "\n",
    "> `特征选择就是选择那些靠谱的Feature，去掉冗余的Feature，对于搜索广告Query和广告的匹配程度很关键；对于展示广告，广告本身的历史表现，往往是最重要的Feature。`\n",
    "\n",
    "> * **独热编码（One-Hot encoding）**\n",
    "\n",
    "```假设有三组特征，分别表示年龄，城市，设备；\n",
    "\n",
    "[\"男\", \"女\"]\n",
    "\n",
    "[\"北京\", \"上海\", \"广州\"]\n",
    "\n",
    "[\"苹果\", \"小米\", \"华为\", \"微软\"]\n",
    "\n",
    "传统变化： 对每一组特征，使用枚举类型，从0开始；\n",
    "\n",
    "[\"男“，”上海“，”小米“]=[ 0,1,1]\n",
    "\n",
    "[\"女“，”北京“，”苹果“] =[1,0,0]\n",
    "\n",
    "传统变化后的数据不是连续的，而是随机分配的，不容易应用在分类器中。\n",
    "\n",
    " 热独编码是一种经典编码，是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。\n",
    "\n",
    "[\"男“，”上海“，”小米“]=[ 1,0,0,1,0,0,1,0,0]\n",
    "\n",
    "[\"女“，”北京“，”苹果“] =[0,1,1,0,0,1,0,0,0]\n",
    "\n",
    "经过热独编码，数据会变成稀疏的，方便分类器处理。\n",
    "```\n",
    "\n",
    "> ### 点击率预估整体过程：\n",
    "\n",
    "> 三个基本过程：特征工程，模型训练，线上服务\n",
    "\n",
    "> ![](https://pic3.zhimg.com/80/v2-a238723a7c09cd540c3c874f9a4777d2_hd.jpg)\n",
    "\n",
    "> * 特征工程：准备各种特征，编码，去掉冗余特征（用PCA等）\n",
    "\n",
    "> * 模型训练：选定训练，测试等数据集，计算AUC，如果AUC有提升，通常可以在进一步在线上分流实验。\n",
    "\n",
    "> * 线上服务：线上服务，需要实时计算CTR，实时计算相关特征和利用模型计算CTR，对于不同来源的CTR，可能需要一个Calibration的服务。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用tensorflow构建各种模型完成ctr预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "!head -5 ./data/train.txt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "!head -10 ./data/featindex.txt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import cPickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import coo_matrix"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# 读取数据，统计基本的信息，field等\n",
    "DTYPE = tf.float32\n",
    "\n",
    "FIELD_SIZES = [0] * 26\n",
    "with open('./data/featindex.txt') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip().split(':')\n",
    "        if len(line) > 1:\n",
    "            f = int(line[0]) - 1\n",
    "            FIELD_SIZES[f] += 1\n",
    "print('field sizes:', FIELD_SIZES)\n",
    "FIELD_OFFSETS = [sum(FIELD_SIZES[:i]) for i in range(len(FIELD_SIZES))]\n",
    "INPUT_DIM = sum(FIELD_SIZES)\n",
    "OUTPUT_DIM = 1\n",
    "STDDEV = 1e-3\n",
    "MINVAL = -1e-3\n",
    "MAXVAL = 1e-3"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 读取libsvm格式数据成稀疏矩阵形式\n",
    "# 0 5:1 9:1 140858:1 445908:1 446177:1 446293:1 449140:1 490778:1 491626:1 491634:1 491641:1 491645:1 491648:1 491668:1 491700:1 491708:1\n",
    "def read_data(file_name):\n",
    "    X = []\n",
    "    D = []\n",
    "    y = []\n",
    "    with open(file_name) as fin:\n",
    "        for line in fin:\n",
    "            fields = line.strip().split()\n",
    "            y_i = int(fields[0])\n",
    "            X_i = [int(x.split(':')[0]) for x in fields[1:]]\n",
    "            D_i = [int(x.split(':')[1]) for x in fields[1:]]\n",
    "            y.append(y_i)\n",
    "            X.append(X_i)\n",
    "            D.append(D_i)\n",
    "    y = np.reshape(np.array(y), [-1])\n",
    "    X = libsvm_2_coo(zip(X, D), (len(X), INPUT_DIM)).tocsr()\n",
    "    return X, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 数据乱序\n",
    "def shuffle(data):\n",
    "    X, y = data\n",
    "    ind = np.arange(X.shape[0])\n",
    "    for i in range(7):\n",
    "        np.random.shuffle(ind)\n",
    "    return X[ind], y[ind]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 工具函数，libsvm格式转成coo稀疏存储格式\n",
    "def libsvm_2_coo(libsvm_data, shape):\n",
    "    coo_rows = []\n",
    "    coo_cols = []\n",
    "    coo_data = []\n",
    "    n = 0\n",
    "    for x, d in libsvm_data:\n",
    "        coo_rows.extend([n] * len(x))\n",
    "        coo_cols.extend(x)\n",
    "        coo_data.extend(d)\n",
    "        n += 1\n",
    "    coo_rows = np.array(coo_rows)\n",
    "    coo_cols = np.array(coo_cols)\n",
    "    coo_data = np.array(coo_data)\n",
    "    return coo_matrix((coo_data, (coo_rows, coo_cols)), shape=shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# csr转成输入格式\n",
    "def csr_2_input(csr_mat):\n",
    "    if not isinstance(csr_mat, list):\n",
    "        coo_mat = csr_mat.tocoo()\n",
    "        indices = np.vstack((coo_mat.row, coo_mat.col)).transpose()\n",
    "        values = csr_mat.data\n",
    "        shape = csr_mat.shape\n",
    "        return indices, values, shape\n",
    "    else:\n",
    "        inputs = []\n",
    "        for csr_i in csr_mat:\n",
    "            inputs.append(csr_2_input(csr_i))\n",
    "        return inputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 数据切片\n",
    "def slice(csr_data, start=0, size=-1):\n",
    "    if not isinstance(csr_data[0], list):\n",
    "        if size == -1 or start + size >= csr_data[0].shape[0]:\n",
    "            slc_data = csr_data[0][start:]\n",
    "            slc_labels = csr_data[1][start:]\n",
    "        else:\n",
    "            slc_data = csr_data[0][start:start + size]\n",
    "            slc_labels = csr_data[1][start:start + size]\n",
    "    else:\n",
    "        if size == -1 or start + size >= csr_data[0][0].shape[0]:\n",
    "            slc_data = []\n",
    "            for d_i in csr_data[0]:\n",
    "                slc_data.append(d_i[start:])\n",
    "            slc_labels = csr_data[1][start:]\n",
    "        else:\n",
    "            slc_data = []\n",
    "            for d_i in csr_data[0]:\n",
    "                slc_data.append(d_i[start:start + size])\n",
    "            slc_labels = csr_data[1][start:start + size]\n",
    "    return csr_2_input(slc_data), slc_labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 数据切分\n",
    "def split_data(data, skip_empty=True):\n",
    "    fields = []\n",
    "    for i in range(len(FIELD_OFFSETS) - 1):\n",
    "        start_ind = FIELD_OFFSETS[i]\n",
    "        end_ind = FIELD_OFFSETS[i + 1]\n",
    "        if skip_empty and start_ind == end_ind:\n",
    "            continue\n",
    "        field_i = data[0][:, start_ind:end_ind]\n",
    "        fields.append(field_i)\n",
    "    fields.append(data[0][:, FIELD_OFFSETS[-1]:])\n",
    "    return fields, data[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 在tensorflow中初始化各种参数变量\n",
    "def init_var_map(init_vars, init_path=None):\n",
    "    if init_path is not None:\n",
    "        load_var_map = pkl.load(open(init_path, 'rb'))\n",
    "        print('load variable map from', init_path, load_var_map.keys())\n",
    "    var_map = {}\n",
    "    for var_name, var_shape, init_method, dtype in init_vars:\n",
    "        if init_method == 'zero':\n",
    "            var_map[var_name] = tf.Variable(tf.zeros(var_shape, dtype=dtype), name=var_name, dtype=dtype)\n",
    "        elif init_method == 'one':\n",
    "            var_map[var_name] = tf.Variable(tf.ones(var_shape, dtype=dtype), name=var_name, dtype=dtype)\n",
    "        elif init_method == 'normal':\n",
    "            var_map[var_name] = tf.Variable(tf.random_normal(var_shape, mean=0.0, stddev=STDDEV, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif init_method == 'tnormal':\n",
    "            var_map[var_name] = tf.Variable(tf.truncated_normal(var_shape, mean=0.0, stddev=STDDEV, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif init_method == 'uniform':\n",
    "            var_map[var_name] = tf.Variable(tf.random_uniform(var_shape, minval=MINVAL, maxval=MAXVAL, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif init_method == 'xavier':\n",
    "            maxval = np.sqrt(6. / np.sum(var_shape))\n",
    "            minval = -maxval\n",
    "            var_map[var_name] = tf.Variable(tf.random_uniform(var_shape, minval=minval, maxval=maxval, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif isinstance(init_method, int) or isinstance(init_method, float):\n",
    "            var_map[var_name] = tf.Variable(tf.ones(var_shape, dtype=dtype) * init_method, name=var_name, dtype=dtype)\n",
    "        elif init_method in load_var_map:\n",
    "            if load_var_map[init_method].shape == tuple(var_shape):\n",
    "                var_map[var_name] = tf.Variable(load_var_map[init_method], name=var_name, dtype=dtype)\n",
    "            else:\n",
    "                print('BadParam: init method', init_method, 'shape', var_shape, load_var_map[init_method].shape)\n",
    "        else:\n",
    "            print('BadParam: init method', init_method)\n",
    "    return var_map"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 不同的激活函数选择\n",
    "def activate(weights, activation_function):\n",
    "    if activation_function == 'sigmoid':\n",
    "        return tf.nn.sigmoid(weights)\n",
    "    elif activation_function == 'softmax':\n",
    "        return tf.nn.softmax(weights)\n",
    "    elif activation_function == 'relu':\n",
    "        return tf.nn.relu(weights)\n",
    "    elif activation_function == 'tanh':\n",
    "        return tf.nn.tanh(weights)\n",
    "    elif activation_function == 'elu':\n",
    "        return tf.nn.elu(weights)\n",
    "    elif activation_function == 'none':\n",
    "        return weights\n",
    "    else:\n",
    "        return weights"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 不同的优化器选择\n",
    "def get_optimizer(opt_algo, learning_rate, loss):\n",
    "    if opt_algo == 'adaldeta':\n",
    "        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'ftrl':\n",
    "        return tf.train.FtrlOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'gd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'padagrad':\n",
    "        return tf.train.ProximalAdagradOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'pgd':\n",
    "        return tf.train.ProximalGradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'rmsprop':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "    else:\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 工具函数\n",
    "# 提示：tf.slice(input_, begin, size, name=None)：按照指定的下标范围抽取连续区域的子集\n",
    "#   tf.gather(params, indices, validate_indices=None, name=None)：按照指定的下标集合从axis=0中抽取子集，适合抽取不连续区域的子集\n",
    "def gather_2d(params, indices):\n",
    "    shape = tf.shape(params)\n",
    "    flat = tf.reshape(params, [-1])\n",
    "    flat_idx = indices[:, 0] * shape[1] + indices[:, 1]\n",
    "    flat_idx = tf.reshape(flat_idx, [-1])\n",
    "    return tf.gather(flat, flat_idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def gather_3d(params, indices):\n",
    "    shape = tf.shape(params)\n",
    "    flat = tf.reshape(params, [-1])\n",
    "    flat_idx = indices[:, 0] * shape[1] * shape[2] + indices[:, 1] * shape[2] + indices[:, 2]\n",
    "    flat_idx = tf.reshape(flat_idx, [-1])\n",
    "    return tf.gather(flat, flat_idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def gather_4d(params, indices):\n",
    "    shape = tf.shape(params)\n",
    "    flat = tf.reshape(params, [-1])\n",
    "    flat_idx = indices[:, 0] * shape[1] * shape[2] * shape[3] + \\\n",
    "               indices[:, 1] * shape[2] * shape[3] + indices[:, 2] * shape[3] + indices[:, 3]\n",
    "    flat_idx = tf.reshape(flat_idx, [-1])\n",
    "    return tf.gather(flat, flat_idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 池化2d\n",
    "def max_pool_2d(params, k):\n",
    "    _, indices = tf.nn.top_k(params, k, sorted=False)\n",
    "    shape = tf.shape(indices)\n",
    "    r1 = tf.reshape(tf.range(shape[0]), [-1, 1])\n",
    "    r1 = tf.tile(r1, [1, k])\n",
    "    r1 = tf.reshape(r1, [-1, 1])\n",
    "    indices = tf.concat([r1, tf.reshape(indices, [-1, 1])], 1)\n",
    "    return tf.reshape(gather_2d(params, indices), [-1, k])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 池化3d\n",
    "def max_pool_3d(params, k):\n",
    "    _, indices = tf.nn.top_k(params, k, sorted=False)\n",
    "    shape = tf.shape(indices)\n",
    "    r1 = tf.reshape(tf.range(shape[0]), [-1, 1])\n",
    "    r2 = tf.reshape(tf.range(shape[1]), [-1, 1])\n",
    "    r1 = tf.tile(r1, [1, k * shape[1]])\n",
    "    r2 = tf.tile(r2, [1, k])\n",
    "    r1 = tf.reshape(r1, [-1, 1])\n",
    "    r2 = tf.tile(tf.reshape(r2, [-1, 1]), [shape[0], 1])\n",
    "    indices = tf.concat([r1, r2, tf.reshape(indices, [-1, 1])], 1)\n",
    "    return tf.reshape(gather_3d(params, indices), [-1, shape[1], k])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 池化4d\n",
    "def max_pool_4d(params, k):\n",
    "    _, indices = tf.nn.top_k(params, k, sorted=False)\n",
    "    shape = tf.shape(indices)\n",
    "    r1 = tf.reshape(tf.range(shape[0]), [-1, 1])\n",
    "    r2 = tf.reshape(tf.range(shape[1]), [-1, 1])\n",
    "    r3 = tf.reshape(tf.range(shape[2]), [-1, 1])\n",
    "    r1 = tf.tile(r1, [1, shape[1] * shape[2] * k])\n",
    "    r2 = tf.tile(r2, [1, shape[2] * k])\n",
    "    r3 = tf.tile(r3, [1, k])\n",
    "    r1 = tf.reshape(r1, [-1, 1])\n",
    "    r2 = tf.tile(tf.reshape(r2, [-1, 1]), [shape[0], 1])\n",
    "    r3 = tf.tile(tf.reshape(r3, [-1, 1]), [shape[0] * shape[1], 1])\n",
    "    indices = tf.concat([r1, r2, r3, tf.reshape(indices, [-1, 1])], 1)\n",
    "    return tf.reshape(gather_4d(params, indices), [-1, shape[1], shape[2], k])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义不同的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 定义基类模型\n",
    "dtype = DTYPE\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.sess = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.layer_keeps = None\n",
    "        self.vars = None\n",
    "        self.keep_prob_train = None\n",
    "        self.keep_prob_test = None\n",
    "\n",
    "    # run model\n",
    "    def run(self, fetches, X=None, y=None, mode='train'):\n",
    "            # 通过feed_dict传入数据\n",
    "            feed_dict = {}\n",
    "            if type(self.X) is list:\n",
    "                for i in range(len(X)):\n",
    "                    feed_dict[self.X[i]] = X[i]\n",
    "            else:\n",
    "                feed_dict[self.X] = X\n",
    "            if y is not None:\n",
    "                feed_dict[self.y] = y\n",
    "            if self.layer_keeps is not None:\n",
    "                if mode == 'train':\n",
    "                    feed_dict[self.layer_keeps] = self.keep_prob_train\n",
    "                elif mode == 'test':\n",
    "                    feed_dict[self.layer_keeps] = self.keep_prob_test\n",
    "            #通过session.run去执行op\n",
    "            return self.sess.run(fetches, feed_dict)\n",
    "\n",
    "    # 模型参数持久化\n",
    "    def dump(self, model_path):\n",
    "        var_map = {}\n",
    "        for name, var in self.vars.iteritems():\n",
    "            var_map[name] = self.run(var)\n",
    "        pkl.dump(var_map, open(model_path, 'wb'))\n",
    "        print('model dumped at', model_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.LR逻辑回归\n",
    "![](https://pic3.zhimg.com/80/v2-09c0c9a25fa46886f92404fef41bbb82_hd.jpg)\n",
    "输入输出:{X,y}<br>\n",
    "映射函数f(x)：单层单节点的“DNN”, 宽而不深，sigmoid(wx+b)输出概率，需要大量的人工特征工程，非线性来源于特征处理<br>\n",
    "损失函数：logloss/... + L1/L2/...<br>\n",
    "优化方法：sgd/...<br>\n",
    "评估：logloss/auc/...<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class LR(Model):\n",
    "    def __init__(self, input_dim=None, output_dim=1, init_path=None, opt_algo='gd', learning_rate=1e-2, l2_weight=0,\n",
    "                 random_seed=None):\n",
    "        Model.__init__(self)\n",
    "        # 声明参数\n",
    "        init_vars = [('w', [input_dim, output_dim], 'xavier', dtype),\n",
    "                     ('b', [output_dim], 'zero', dtype)]\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if random_seed is not None:\n",
    "                tf.set_random_seed(random_seed)\n",
    "            # 用稀疏的placeholder\n",
    "            self.X = tf.sparse_placeholder(dtype)\n",
    "            self.y = tf.placeholder(dtype)\n",
    "            # init参数\n",
    "            self.vars = init_var_map(init_vars, init_path)\n",
    "\n",
    "            w = self.vars['w']\n",
    "            b = self.vars['b']\n",
    "            # sigmoid(wx+b)\n",
    "            xw = tf.sparse_tensor_dense_matmul(self.X, w)\n",
    "            logits = tf.reshape(xw + b, [-1])\n",
    "            self.y_prob = tf.sigmoid(logits)\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=logits)) + \\\n",
    "                        l2_weight * tf.nn.l2_loss(xw)\n",
    "            self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "            # GPU设定\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            # 初始化图里的参数\n",
    "            tf.global_variables_initializer().run(session=self.sess)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import progressbar\n",
    "train_file = './data/train.txt'\n",
    "test_file = './data/test.txt'\n",
    "\n",
    "input_dim = INPUT_DIM\n",
    "\n",
    "# 读取数据\n",
    "#train_data = read_data(train_file)\n",
    "#test_data = read_data(test_file)\n",
    "train_data = pkl.load(open('./data/train.pkl', 'rb'))\n",
    "#train_data = shuffle(train_data)\n",
    "test_data = pkl.load(open('./data/test.pkl', 'rb'))\n",
    "# pkl.dump(train_data, open('./data/train.pkl', 'wb'))\n",
    "# pkl.dump(test_data, open('./data/test.pkl', 'wb'))\n",
    "\n",
    "# 输出数据信息维度\n",
    "if train_data[1].ndim > 1:\n",
    "    print('label must be 1-dim')\n",
    "    exit(0)\n",
    "print('read finish')\n",
    "print('train data size:', train_data[0].shape)\n",
    "print('test data size:', test_data[0].shape)\n",
    "\n",
    "# 训练集与测试集\n",
    "train_size = train_data[0].shape[0]\n",
    "test_size = test_data[0].shape[0]\n",
    "num_feas = len(FIELD_SIZES)\n",
    "\n",
    "# 超参数设定\n",
    "min_round = 1\n",
    "num_round = 200\n",
    "early_stop_round = 5\n",
    "# train + val\n",
    "batch_size = 1024\n",
    "\n",
    "field_sizes = FIELD_SIZES\n",
    "field_offsets = FIELD_OFFSETS\n",
    "\n",
    "# 逻辑回归参数设定\n",
    "lr_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'opt_algo': 'gd',\n",
    "    'learning_rate': 0.1,\n",
    "    'l2_weight': 0,\n",
    "    'random_seed': 0\n",
    "}\n",
    "print(lr_params)\n",
    "model = LR(**lr_params)\n",
    "print(\"training LR...\")\n",
    "def train(model):\n",
    "    history_score = []\n",
    "    # 执行num_round轮\n",
    "    for i in range(num_round):\n",
    "        # 主要的2个op是优化器和损失\n",
    "        fetches = [model.optimizer, model.loss]\n",
    "        if batch_size > 0:\n",
    "            ls = []\n",
    "            # 进度条工具\n",
    "            bar = progressbar.ProgressBar()\n",
    "            print('[%d]\\ttraining...' % i)\n",
    "            for j in bar(range(int(train_size / batch_size + 1))):\n",
    "                X_i, y_i = slice(train_data, j * batch_size, batch_size)\n",
    "                # 训练，run op\n",
    "                _, l = model.run(fetches, X_i, y_i)\n",
    "                ls.append(l)\n",
    "        elif batch_size == -1:\n",
    "            X_i, y_i = slice(train_data)\n",
    "            _, l = model.run(fetches, X_i, y_i)\n",
    "            ls = [l]\n",
    "        train_preds = []\n",
    "        print('[%d]\\tevaluating...' % i)\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for j in bar(range(int(train_size / 10000 + 1))):\n",
    "            X_i, _ = slice(train_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            train_preds.extend(preds)\n",
    "        test_preds = []\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for j in bar(range(int(test_size / 10000 + 1))):\n",
    "            X_i, _ = slice(test_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            test_preds.extend(preds)\n",
    "        # 把预估的结果和真实结果拿出来计算auc\n",
    "        train_score = roc_auc_score(train_data[1], train_preds)\n",
    "        test_score = roc_auc_score(test_data[1], test_preds)\n",
    "        # 输出auc信息\n",
    "        print('[%d]\\tloss (with l2 norm):%f\\ttrain-auc: %f\\teval-auc: %f' % (i, np.mean(ls), train_score, test_score))\n",
    "        history_score.append(test_score)\n",
    "        # early stopping\n",
    "        if i > min_round and i > early_stop_round:\n",
    "            if np.argmax(history_score) == i - early_stop_round and history_score[-1] - history_score[\n",
    "                        -1 * early_stop_round] < 1e-5:\n",
    "                print('early stop\\nbest iteration:\\n[%d]\\teval-auc: %f' % (\n",
    "                    np.argmax(history_score), np.max(history_score)))\n",
    "                break\n",
    "\n",
    "train(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.FM\n",
    "FM可以视作有二次交叉的LR，为了控制参数量和充分学习，提出了user vector和item vector的概念\n",
    "![](https://pic2.zhimg.com/80/v2-b4941534912e895542a52eda50f39810_hd.jpg)\n",
    "![](https://pic2.zhimg.com/80/v2-098dc05dca6fa4c77d45510cb0951677_hd.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "class FM(Model):\n",
    "    def __init__(self, input_dim=None, output_dim=1, factor_order=10, init_path=None, opt_algo='gd', learning_rate=1e-2,\n",
    "                 l2_w=0, l2_v=0, random_seed=None):\n",
    "        Model.__init__(self)\n",
    "        # 一次、二次交叉、偏置项\n",
    "        init_vars = [('w', [input_dim, output_dim], 'xavier', dtype),\n",
    "                     ('v', [input_dim, factor_order], 'xavier', dtype),\n",
    "                     ('b', [output_dim], 'zero', dtype)]\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if random_seed is not None:\n",
    "                tf.set_random_seed(random_seed)\n",
    "            self.X = tf.sparse_placeholder(dtype)\n",
    "            self.y = tf.placeholder(dtype)\n",
    "            self.vars = init_var_map(init_vars, init_path)\n",
    "\n",
    "            w = self.vars['w']\n",
    "            v = self.vars['v']\n",
    "            b = self.vars['b']\n",
    "            \n",
    "            # [(x1+x2+x3)^2 - (x1^2+x2^2+x3^2)]/2\n",
    "            # 先计算所有的交叉项，再减去平方项(自己和自己相乘)\n",
    "            X_square = tf.SparseTensor(self.X.indices, tf.square(self.X.values), tf.to_int64(tf.shape(self.X)))\n",
    "            xv = tf.square(tf.sparse_tensor_dense_matmul(self.X, v))\n",
    "            p = 0.5 * tf.reshape(\n",
    "                tf.reduce_sum(xv - tf.sparse_tensor_dense_matmul(X_square, tf.square(v)), 1),\n",
    "                [-1, output_dim])\n",
    "            xw = tf.sparse_tensor_dense_matmul(self.X, w)\n",
    "            logits = tf.reshape(xw + b + p, [-1])\n",
    "            self.y_prob = tf.sigmoid(logits)\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)) + \\\n",
    "                        l2_w * tf.nn.l2_loss(xw) + \\\n",
    "                        l2_v * tf.nn.l2_loss(xv)\n",
    "            self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "\n",
    "            #GPU设定\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            # 图中所有variable初始化\n",
    "            tf.global_variables_initializer().run(session=self.sess)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import progressbar\n",
    "train_file = './data/train.txt'\n",
    "test_file = './data/test.txt'\n",
    "\n",
    "input_dim = INPUT_DIM\n",
    "train_data = pkl.load(open('./data/train.pkl', 'rb'))\n",
    "train_data = shuffle(train_data)\n",
    "test_data = pkl.load(open('./data/test.pkl', 'rb'))\n",
    "\n",
    "if train_data[1].ndim > 1:\n",
    "    print('label must be 1-dim')\n",
    "    exit(0)\n",
    "print('read finish')\n",
    "print('train data size:', train_data[0].shape)\n",
    "print('test data size:', test_data[0].shape)\n",
    "\n",
    "# 训练集与测试集\n",
    "train_size = train_data[0].shape[0]\n",
    "test_size = test_data[0].shape[0]\n",
    "num_feas = len(FIELD_SIZES)\n",
    "\n",
    "# 超参数设定\n",
    "min_round = 1\n",
    "num_round = 200\n",
    "early_stop_round = 5\n",
    "batch_size = 1024\n",
    "\n",
    "field_sizes = FIELD_SIZES\n",
    "field_offsets = FIELD_OFFSETS\n",
    "\n",
    "# FM参数设定\n",
    "fm_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'factor_order': 10,\n",
    "    'opt_algo': 'gd',\n",
    "    'learning_rate': 0.1,\n",
    "    'l2_w': 0,\n",
    "    'l2_v': 0,\n",
    "}\n",
    "print(fm_params)\n",
    "model = FM(**fm_params)\n",
    "print(\"training FM...\")\n",
    "\n",
    "def train(model):\n",
    "    history_score = []\n",
    "    for i in range(num_round):\n",
    "        # 同样是优化器和损失两个op\n",
    "        fetches = [model.optimizer, model.loss]\n",
    "        if batch_size > 0:\n",
    "            ls = []\n",
    "            bar = progressbar.ProgressBar()\n",
    "            print('[%d]\\ttraining...' % i)\n",
    "            for j in bar(range(int(train_size / batch_size + 1))):\n",
    "                X_i, y_i = slice(train_data, j * batch_size, batch_size)\n",
    "                # 训练\n",
    "                _, l = model.run(fetches, X_i, y_i)\n",
    "                ls.append(l)\n",
    "        elif batch_size == -1:\n",
    "            X_i, y_i = slice(train_data)\n",
    "            _, l = model.run(fetches, X_i, y_i)\n",
    "            ls = [l]\n",
    "        train_preds = []\n",
    "        print('[%d]\\tevaluating...' % i)\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for j in bar(range(int(train_size / 10000 + 1))):\n",
    "            X_i, _ = slice(train_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            train_preds.extend(preds)\n",
    "        test_preds = []\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for j in bar(range(int(test_size / 10000 + 1))):\n",
    "            X_i, _ = slice(test_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            test_preds.extend(preds)\n",
    "        train_score = roc_auc_score(train_data[1], train_preds)\n",
    "        test_score = roc_auc_score(test_data[1], test_preds)\n",
    "        print('[%d]\\tloss (with l2 norm):%f\\ttrain-auc: %f\\teval-auc: %f' % (i, np.mean(ls), train_score, test_score))\n",
    "        history_score.append(test_score)\n",
    "        if i > min_round and i > early_stop_round:\n",
    "            if np.argmax(history_score) == i - early_stop_round and history_score[-1] - history_score[\n",
    "                        -1 * early_stop_round] < 1e-5:\n",
    "                print('early stop\\nbest iteration:\\n[%d]\\teval-auc: %f' % (\n",
    "                    np.argmax(history_score), np.max(history_score)))\n",
    "                break\n",
    "\n",
    "train(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN\n",
    "FNN的考虑是模型的capacity可以进一步提升，以对更复杂的场景建模。<br>\n",
    "FNN可以视作FM + MLP = LR + MF + MLP\n",
    "![](https://pic4.zhimg.com/80/v2-d9ffb1e0ff7707503d4aed085492d3c7_hd.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class FNN(Model):\n",
    "    def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None,\n",
    "                 embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=1e-2, random_seed=None):\n",
    "        Model.__init__(self)\n",
    "        init_vars = []\n",
    "        num_inputs = len(field_sizes)\n",
    "        for i in range(num_inputs):\n",
    "            init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))\n",
    "        node_in = num_inputs * embed_size\n",
    "        for i in range(len(layer_sizes)):\n",
    "            init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))\n",
    "            init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero', dtype))\n",
    "            node_in = layer_sizes[i]\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if random_seed is not None:\n",
    "                tf.set_random_seed(random_seed)\n",
    "            self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]\n",
    "            self.y = tf.placeholder(dtype)\n",
    "            self.keep_prob_train = 1 - np.array(drop_out)\n",
    "            self.keep_prob_test = np.ones_like(drop_out)\n",
    "            self.layer_keeps = tf.placeholder(dtype)\n",
    "            self.vars = init_var_map(init_vars, init_path)\n",
    "            w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]\n",
    "            xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)\n",
    "            l = xw\n",
    "\n",
    "            for i in range(len(layer_sizes)):\n",
    "                wi = self.vars['w%d' % i]\n",
    "                bi = self.vars['b%d' % i]\n",
    "                print(l.shape, wi.shape, bi.shape)\n",
    "                l = tf.nn.dropout(\n",
    "                    activate(\n",
    "                        tf.matmul(l, wi) + bi,\n",
    "                        layer_acts[i]),\n",
    "                    self.layer_keeps[i])\n",
    "\n",
    "            l = tf.squeeze(l)\n",
    "            self.y_prob = tf.sigmoid(l)\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))\n",
    "            if layer_l2 is not None:\n",
    "                self.loss += embed_l2 * tf.nn.l2_loss(xw)\n",
    "                for i in range(len(layer_sizes)):\n",
    "                    wi = self.vars['w%d' % i]\n",
    "                    self.loss += layer_l2[i] * tf.nn.l2_loss(wi)\n",
    "            self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            tf.global_variables_initializer().run(session=self.sess)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import progressbar\n",
    "train_file = './data/train.txt'\n",
    "test_file = './data/test.txt'\n",
    "\n",
    "input_dim = INPUT_DIM\n",
    "train_data = pkl.load(open('./data/train.pkl', 'rb'))\n",
    "train_data = shuffle(train_data)\n",
    "test_data = pkl.load(open('./data/test.pkl', 'rb'))\n",
    "\n",
    "if train_data[1].ndim > 1:\n",
    "    print('label must be 1-dim')\n",
    "    exit(0)\n",
    "print('read finish')\n",
    "print('train data size:', train_data[0].shape)\n",
    "print('test data size:', test_data[0].shape)\n",
    "\n",
    "train_size = train_data[0].shape[0]\n",
    "test_size = test_data[0].shape[0]\n",
    "num_feas = len(FIELD_SIZES)\n",
    "\n",
    "min_round = 1\n",
    "num_round = 200\n",
    "early_stop_round = 5\n",
    "batch_size = 1024\n",
    "\n",
    "field_sizes = FIELD_SIZES\n",
    "field_offsets = FIELD_OFFSETS\n",
    "\n",
    "train_data = split_data(train_data)\n",
    "test_data = split_data(test_data)\n",
    "tmp = []\n",
    "for x in field_sizes:\n",
    "    if x > 0:\n",
    "        tmp.append(x)\n",
    "field_sizes = tmp\n",
    "print('remove empty fields', field_sizes)\n",
    "    \n",
    "fnn_params = {\n",
    "    'field_sizes': field_sizes,\n",
    "    'embed_size': 10,\n",
    "    'layer_sizes': [500, 1],\n",
    "    'layer_acts': ['relu', None],\n",
    "    'drop_out': [0, 0],\n",
    "    'opt_algo': 'gd',\n",
    "    'learning_rate': 0.1,\n",
    "    'embed_l2': 0,\n",
    "    'layer_l2': [0, 0],\n",
    "    'random_seed': 0\n",
    "}\n",
    "print(fnn_params)\n",
    "model = FNN(**fnn_params)\n",
    "\n",
    "def train(model):\n",
    "    history_score = []\n",
    "    for i in range(num_round):\n",
    "        fetches = [model.optimizer, model.loss]\n",
    "        if batch_size > 0:\n",
    "            ls = []\n",
    "            bar = progressbar.ProgressBar()\n",
    "            print('[%d]\\ttraining...' % i)\n",
    "            for j in bar(range(int(train_size / batch_size + 1))):\n",
    "                X_i, y_i = slice(train_data, j * batch_size, batch_size)\n",
    "                _, l = model.run(fetches, X_i, y_i)\n",
    "                ls.append(l)\n",
    "        elif batch_size == -1:\n",
    "            X_i, y_i = slice(train_data)\n",
    "            _, l = model.run(fetches, X_i, y_i)\n",
    "            ls = [l]\n",
    "        train_preds = []\n",
    "        print('[%d]\\tevaluating...' % i)\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for j in bar(range(int(train_size / 10000 + 1))):\n",
    "            X_i, _ = slice(train_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            train_preds.extend(preds)\n",
    "        test_preds = []\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for j in bar(range(int(test_size / 10000 + 1))):\n",
    "            X_i, _ = slice(test_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            test_preds.extend(preds)\n",
    "        train_score = roc_auc_score(train_data[1], train_preds)\n",
    "        test_score = roc_auc_score(test_data[1], test_preds)\n",
    "        print('[%d]\\tloss (with l2 norm):%f\\ttrain-auc: %f\\teval-auc: %f' % (i, np.mean(ls), train_score, test_score))\n",
    "        history_score.append(test_score)\n",
    "        if i > min_round and i > early_stop_round:\n",
    "            if np.argmax(history_score) == i - early_stop_round and history_score[-1] - history_score[\n",
    "                        -1 * early_stop_round] < 1e-5:\n",
    "                print('early stop\\nbest iteration:\\n[%d]\\teval-auc: %f' % (\n",
    "                    np.argmax(history_score), np.max(history_score)))\n",
    "                break\n",
    "\n",
    "train(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCPM\n",
    "reference：[ctr模型汇总](https://zhuanlan.zhihu.com/p/32523455)\n",
    "\n",
    "FM只能学习特征的二阶组合，但CNN能学习更高阶的组合，可学习的阶数和卷积的视野相关。\n",
    "![](https://img-blog.csdn.net/20171211204240715?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRGFueUhnYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "mbedding层：e1, e2…en是某特定用户被展示的一系列广告。如果在预测广告是否会点击时不考虑历史展示广告的点击情况，则n=1。同时embedding矩阵的具体值是随着模型训练学出来的。Embedding矩阵为S，向量维度为d。\n",
    "\n",
    "卷积层：卷积参数W有d*w个，即对于矩阵S，上图每一列对应一个参数不共享的一维卷积，其视野为w，卷积共有d个，每个输出向量维度为(n+w-1)，输出矩阵维度d*(n+w-1)。因为对于ctr预估而言，矩阵S每一列都对应特定的描述维度，所以需要分别处理，得到的输出矩阵的每一列就都是描述广告特定方面的特征。\n",
    "\n",
    "Pooling层：flexible p-max pooling。\n",
    "![](https://pic1.zhimg.com/80/v2-1c76210b014826e02ebbadf07168715b_hd.jpg)\n",
    "L是模型总卷积层数，n是输入序列长度，pi就是第i层的pooling参数。这样最后一层卷积层都是输出3个最大的元素，长度固定方便后面接全连接层。同时这个指数型的参数，一开始改变比较小，几乎都是n，后面就减少得比较快。这样可以防止在模型浅层的时候就损失太多信息，众所周知深度模型在前面几层最好不要做得太简单，容易损失很多信息。文章还提到p-max pooling输出的几个最大的元素是保序的，可输入时的顺序一致，这点对于保留序列信息是重要的。\n",
    "\n",
    "激活层：tanh\n",
    "\n",
    "最后，\n",
    "![](https://pic3.zhimg.com/80/v2-1c8e3a5f520c66e62312b458b1308d79_hd.jpg)\n",
    "Fij是指低i层的第j个feature map。感觉是不同输入通道的卷积参数也不共享，对应输出是所有输入通道卷积的输出的求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class CCPM(Model):\n",
    "    def __init__(self, field_sizes=None, embed_size=10, filter_sizes=None, layer_acts=None, drop_out=None,\n",
    "                 init_path=None, opt_algo='gd', learning_rate=1e-2, random_seed=None):\n",
    "        Model.__init__(self)\n",
    "        init_vars = []\n",
    "        num_inputs = len(field_sizes)\n",
    "        for i in range(num_inputs):\n",
    "            init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))\n",
    "        init_vars.append(('f1', [embed_size, filter_sizes[0], 1, 2], 'xavier', dtype))\n",
    "        init_vars.append(('f2', [embed_size, filter_sizes[1], 2, 2], 'xavier', dtype))\n",
    "        init_vars.append(('w1', [2 * 3 * embed_size, 1], 'xavier', dtype))\n",
    "        init_vars.append(('b1', [1], 'zero', dtype))\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if random_seed is not None:\n",
    "                tf.set_random_seed(random_seed)\n",
    "            self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]\n",
    "            self.y = tf.placeholder(dtype)\n",
    "            self.keep_prob_train = 1 - np.array(drop_out)\n",
    "            self.keep_prob_test = np.ones_like(drop_out)\n",
    "            self.layer_keeps = tf.placeholder(dtype)\n",
    "            self.vars = init_var_map(init_vars, init_path)\n",
    "            w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]\n",
    "            xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)\n",
    "            l = xw\n",
    "\n",
    "            l = tf.transpose(tf.reshape(l, [-1, num_inputs, embed_size, 1]), [0, 2, 1, 3])\n",
    "            f1 = self.vars['f1']\n",
    "            l = tf.nn.conv2d(l, f1, [1, 1, 1, 1], 'SAME')\n",
    "            l = tf.transpose(\n",
    "                max_pool_4d(\n",
    "                    tf.transpose(l, [0, 1, 3, 2]),\n",
    "                    int(num_inputs / 2)),\n",
    "                [0, 1, 3, 2])\n",
    "            f2 = self.vars['f2']\n",
    "            l = tf.nn.conv2d(l, f2, [1, 1, 1, 1], 'SAME')\n",
    "            l = tf.transpose(\n",
    "                max_pool_4d(\n",
    "                    tf.transpose(l, [0, 1, 3, 2]), 3),\n",
    "                [0, 1, 3, 2])\n",
    "            l = tf.nn.dropout(\n",
    "                activate(\n",
    "                    tf.reshape(l, [-1, embed_size * 3 * 2]),\n",
    "                    layer_acts[0]),\n",
    "                self.layer_keeps[0])\n",
    "            w1 = self.vars['w1']\n",
    "            b1 = self.vars['b1']\n",
    "            l = tf.matmul(l, w1) + b1\n",
    "\n",
    "            l = tf.squeeze(l)\n",
    "            self.y_prob = tf.sigmoid(l)\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))\n",
    "            self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            tf.global_variables_initializer().run(session=self.sess)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNN\n",
    "reference:<br>\n",
    "[深度学习在CTR预估中的应用](https://zhuanlan.zhihu.com/p/35484389)\n",
    "\n",
    "可以视作FNN+product layer\n",
    "![](https://yxzf.github.io/images/deeplearning/dnn_ctr/pnn.png)\n",
    "\n",
    "PNN和FNN的主要不同在于除了得到z向量，还增加了一个p向量，即Product向量。Product向量由每个category field的feature vector做inner product 或则 outer product 得到，作者认为这样做有助于特征交叉。另外PNN中Embeding层不再由FM生成，可以在整个网络中训练得到。\n",
    "\n",
    "对比 FNN 网络，PNN的区别在于中间多了一层 Product Layer 层。Product Layer 层由两部分组成，左边z为 embedding 层的线性部分，右边为 embedding 层的特征交叉部分。\n",
    "\n",
    "除了 Product Layer 不同，PNN 和 FNN 的 MLP 结构是一样的。这种 product 思想来源于，在 CTR 预估中，认为特征之间的关系更多是一种 and“且”的关系，而非 add\"加”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。\n",
    "\n",
    "根据 product 的方式不同，可以分为 inner product (IPNN) 和 outer product (OPNN)，如下图所示。\n",
    "\n",
    "![](https://pic4.zhimg.com/v2-c30b0f9983345382d31a30d4eed516d3_r.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class PNN1(Model):\n",
    "    def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None,\n",
    "                 embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=1e-2, random_seed=None):\n",
    "        Model.__init__(self)\n",
    "        init_vars = []\n",
    "        num_inputs = len(field_sizes)\n",
    "        for i in range(num_inputs):\n",
    "            init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))\n",
    "        num_pairs = int(num_inputs * (num_inputs - 1) / 2)\n",
    "        node_in = num_inputs * embed_size + num_pairs\n",
    "        # node_in = num_inputs * (embed_size + num_inputs)\n",
    "        for i in range(len(layer_sizes)):\n",
    "            init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))\n",
    "            init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero', dtype))\n",
    "            node_in = layer_sizes[i]\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if random_seed is not None:\n",
    "                tf.set_random_seed(random_seed)\n",
    "            self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]\n",
    "            self.y = tf.placeholder(dtype)\n",
    "            self.keep_prob_train = 1 - np.array(drop_out)\n",
    "            self.keep_prob_test = np.ones_like(drop_out)\n",
    "            self.layer_keeps = tf.placeholder(dtype)\n",
    "            self.vars = init_var_map(init_vars, init_path)\n",
    "            w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]\n",
    "            xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)\n",
    "            xw3d = tf.reshape(xw, [-1, num_inputs, embed_size])\n",
    "\n",
    "            row = []\n",
    "            col = []\n",
    "            for i in range(num_inputs-1):\n",
    "                for j in range(i+1, num_inputs):\n",
    "                    row.append(i)\n",
    "                    col.append(j)\n",
    "            # batch * pair * k\n",
    "            p = tf.transpose(\n",
    "                # pair * batch * k\n",
    "                tf.gather(\n",
    "                    # num * batch * k\n",
    "                    tf.transpose(\n",
    "                        xw3d, [1, 0, 2]),\n",
    "                    row),\n",
    "                [1, 0, 2])\n",
    "            # batch * pair * k\n",
    "            q = tf.transpose(\n",
    "                tf.gather(\n",
    "                    tf.transpose(\n",
    "                        xw3d, [1, 0, 2]),\n",
    "                    col),\n",
    "                [1, 0, 2])\n",
    "            p = tf.reshape(p, [-1, num_pairs, embed_size])\n",
    "            q = tf.reshape(q, [-1, num_pairs, embed_size])\n",
    "            ip = tf.reshape(tf.reduce_sum(p * q, [-1]), [-1, num_pairs])\n",
    "\n",
    "            # simple but redundant\n",
    "            # batch * n * 1 * k, batch * 1 * n * k\n",
    "            # ip = tf.reshape(\n",
    "            #     tf.reduce_sum(\n",
    "            #         tf.expand_dims(xw3d, 2) *\n",
    "            #         tf.expand_dims(xw3d, 1),\n",
    "            #         3),\n",
    "            #     [-1, num_inputs**2])\n",
    "            l = tf.concat([xw, ip], 1)\n",
    "\n",
    "            for i in range(len(layer_sizes)):\n",
    "                wi = self.vars['w%d' % i]\n",
    "                bi = self.vars['b%d' % i]\n",
    "                l = tf.nn.dropout(\n",
    "                    activate(\n",
    "                        tf.matmul(l, wi) + bi,\n",
    "                        layer_acts[i]),\n",
    "                    self.layer_keeps[i])\n",
    "\n",
    "            l = tf.squeeze(l)\n",
    "            self.y_prob = tf.sigmoid(l)\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))\n",
    "            if layer_l2 is not None:\n",
    "                self.loss += embed_l2 * tf.nn.l2_loss(xw)\n",
    "                for i in range(len(layer_sizes)):\n",
    "                    wi = self.vars['w%d' % i]\n",
    "                    self.loss += layer_l2[i] * tf.nn.l2_loss(wi)\n",
    "            self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            tf.global_variables_initializer().run(session=self.sess)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class PNN2(Model):\n",
    "    def __init__(self, field_sizes=None, embed_size=10, layer_sizes=None, layer_acts=None, drop_out=None,\n",
    "                 embed_l2=None, layer_l2=None, init_path=None, opt_algo='gd', learning_rate=1e-2, random_seed=None,\n",
    "                 layer_norm=True):\n",
    "        Model.__init__(self)\n",
    "        init_vars = []\n",
    "        num_inputs = len(field_sizes)\n",
    "        for i in range(num_inputs):\n",
    "            init_vars.append(('embed_%d' % i, [field_sizes[i], embed_size], 'xavier', dtype))\n",
    "        num_pairs = int(num_inputs * (num_inputs - 1) / 2)\n",
    "        node_in = num_inputs * embed_size + num_pairs\n",
    "        init_vars.append(('kernel', [embed_size, num_pairs, embed_size], 'xavier', dtype))\n",
    "        for i in range(len(layer_sizes)):\n",
    "            init_vars.append(('w%d' % i, [node_in, layer_sizes[i]], 'xavier', dtype))\n",
    "            init_vars.append(('b%d' % i, [layer_sizes[i]], 'zero',  dtype))\n",
    "            node_in = layer_sizes[i]\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if random_seed is not None:\n",
    "                tf.set_random_seed(random_seed)\n",
    "            self.X = [tf.sparse_placeholder(dtype) for i in range(num_inputs)]\n",
    "            self.y = tf.placeholder(dtype)\n",
    "            self.keep_prob_train = 1 - np.array(drop_out)\n",
    "            self.keep_prob_test = np.ones_like(drop_out)\n",
    "            self.layer_keeps = tf.placeholder(dtype)\n",
    "            self.vars = init_var_map(init_vars, init_path)\n",
    "            w0 = [self.vars['embed_%d' % i] for i in range(num_inputs)]\n",
    "            xw = tf.concat([tf.sparse_tensor_dense_matmul(self.X[i], w0[i]) for i in range(num_inputs)], 1)\n",
    "            xw3d = tf.reshape(xw, [-1, num_inputs, embed_size])\n",
    "\n",
    "            row = []\n",
    "            col = []\n",
    "            for i in range(num_inputs - 1):\n",
    "                for j in range(i + 1, num_inputs):\n",
    "                    row.append(i)\n",
    "                    col.append(j)\n",
    "            # batch * pair * k\n",
    "            p = tf.transpose(\n",
    "                # pair * batch * k\n",
    "                tf.gather(\n",
    "                    # num * batch * k\n",
    "                    tf.transpose(\n",
    "                        xw3d, [1, 0, 2]),\n",
    "                    row),\n",
    "                [1, 0, 2])\n",
    "            # batch * pair * k\n",
    "            q = tf.transpose(\n",
    "                tf.gather(\n",
    "                    tf.transpose(\n",
    "                        xw3d, [1, 0, 2]),\n",
    "                    col),\n",
    "                [1, 0, 2])\n",
    "            # b * p * k\n",
    "            p = tf.reshape(p, [-1, num_pairs, embed_size])\n",
    "            # b * p * k\n",
    "            q = tf.reshape(q, [-1, num_pairs, embed_size])\n",
    "            # k * p * k\n",
    "            k = self.vars['kernel']\n",
    "\n",
    "            # batch * 1 * pair * k\n",
    "            p = tf.expand_dims(p, 1)\n",
    "            # batch * pair\n",
    "            kp = tf.reduce_sum(\n",
    "                # batch * pair * k\n",
    "                tf.multiply(\n",
    "                    # batch * pair * k\n",
    "                    tf.transpose(\n",
    "                        # batch * k * pair\n",
    "                        tf.reduce_sum(\n",
    "                            # batch * k * pair * k\n",
    "                            tf.multiply(\n",
    "                                p, k),\n",
    "                            -1),\n",
    "                        [0, 2, 1]),\n",
    "                    q),\n",
    "                -1)\n",
    "\n",
    "            #\n",
    "            # if layer_norm:\n",
    "            #     # x_mean, x_var = tf.nn.moments(xw, [1], keep_dims=True)\n",
    "            #     # xw = (xw - x_mean) / tf.sqrt(x_var)\n",
    "            #     # x_g = tf.Variable(tf.ones([num_inputs * embed_size]), name='x_g')\n",
    "            #     # x_b = tf.Variable(tf.zeros([num_inputs * embed_size]), name='x_b')\n",
    "            #     # x_g = tf.Print(x_g, [x_g[:10], x_b])\n",
    "            #     # xw = xw * x_g + x_b\n",
    "            #     p_mean, p_var = tf.nn.moments(op, [1], keep_dims=True)\n",
    "            #     op = (op - p_mean) / tf.sqrt(p_var)\n",
    "            #     p_g = tf.Variable(tf.ones([embed_size**2]), name='p_g')\n",
    "            #     p_b = tf.Variable(tf.zeros([embed_size**2]), name='p_b')\n",
    "            #     # p_g = tf.Print(p_g, [p_g[:10], p_b])\n",
    "            #     op = op * p_g + p_b\n",
    "\n",
    "            l = tf.concat([xw, kp], 1)\n",
    "            for i in range(len(layer_sizes)):\n",
    "                wi = self.vars['w%d' % i]\n",
    "                bi = self.vars['b%d' % i]\n",
    "                l = tf.nn.dropout(\n",
    "                    activate(\n",
    "                        tf.matmul(l, wi) + bi,\n",
    "                        layer_acts[i]),\n",
    "                    self.layer_keeps[i])\n",
    "\n",
    "            l = tf.squeeze(l)\n",
    "            self.y_prob = tf.sigmoid(l)\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=l, labels=self.y))\n",
    "            if layer_l2 is not None:\n",
    "                self.loss += embed_l2 * tf.nn.l2_loss(xw)#tf.concat(w0, 0))\n",
    "                for i in range(len(layer_sizes)):\n",
    "                    wi = self.vars['w%d' % i]\n",
    "                    self.loss += layer_l2[i] * tf.nn.l2_loss(wi)\n",
    "            self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            tf.global_variables_initializer().run(session=self.sess)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
