{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推荐系统\n",
    "背景参考：[推荐系统简介](https://blog.csdn.net/dq_dm/article/details/39755999)\n",
    "\n",
    "随着信息技术和互联网的发展，人们逐渐从信息匮乏的时代走入了信息过载(information overload)的时代。在这个时代，无论是信息消费者还是信息生产者都遇到了很大的挑战。\n",
    "\n",
    "对于信息消费者，从大量信息中找到自己感兴趣的信息是一件非常困难的事情；对于信息生产者，让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。\n",
    "\n",
    "推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。（注：信息也统称为“物品”）。\n",
    "\n",
    "![](https://img-blog.csdn.net/20141003155010031?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRFFfRE0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、推荐系统VS搜索引擎\n",
    "\n",
    "解决信息过载的两种方法：\n",
    "\n",
    "* 1.1 搜索引擎\n",
    "\n",
    "优点：可以让用户通过搜索关键词找到自己需要的信息。\n",
    "\n",
    "缺点：（1）需要用户主动提供准确的关键词来寻找信息，当用户无法找到准确描述自己需求的关键词时，搜索引擎就无能为力了；（2）用户在使用同一个关键字搜索信息时，得到的结果是相同的；（3）信息及其传播是多样化的，而用户对信息的需求是多元化和个性化的，那么通过以搜索引擎为代表的信息检索系统获得的结果不能满足用户的个性化需求，仍然无法很好地解决信息超载问题。\n",
    "\n",
    "* 1.2 个性化推荐系统\n",
    "\n",
    "优点：（1）不需要用户提供明确的需求，它是根据用户的信息需求、兴趣等，将用户感兴趣的信息、产品等推荐给用户；（2）从物品的角度出发，推荐系统可以更好地发掘物品的长尾（long tail）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、推荐系统的概念和定义\n",
    "\n",
    "推荐系统的定义有不少，但被广泛接受的推荐系统的概念和定义是Resnick和Varian在1997年给出的：“它是利用电子商务网站向客户提供商品信息和建议，帮助用户决定应该购买什么产品，模拟销售人员帮助客户完成购买过程”。\n",
    "\n",
    "推荐系统有3个重要的模块：用户建模模块、推荐对象建模模块、推荐算法模块。通用的推荐系统模型流程如图2所示。推荐系统把用户模型中兴趣需求信息和推荐对象模型中的特征信息匹配，同时使用相应的推荐算法进行计算筛选，找到用户可能感兴趣的推荐对象，然后推荐给用户。\n",
    "\n",
    "![](https://img-blog.csdn.net/20141003155317043?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRFFfRE0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、推荐算法\n",
    "\n",
    "推荐算法（或叫推荐策略）是整个推荐系统中最核心和关键的部分，在很大程度上决定了推荐系统类型和性能的优劣，推荐算法的研究是推荐系统最为繁荣的部分，大量的论文著作都关注了这个方面。目前，出现的推荐算法有很多，对其分类的标准也没有一个统一的标准，但受到大家公认的推荐算法基本包括以下几种：基于内容的推荐、协同过滤推荐、基于关联规则的推荐、基于知识的推荐、组合（混合）推荐及其他推荐。以下来介绍各种推荐策略及其优缺点。\n",
    "\n",
    "##### 3.1 基于内容的推荐\n",
    "\n",
    "基于内容的推荐（content-based recommendation）方法源于信息获取领域，是信息检索领域的重要研究内容。该方法是根据用户已经选择的对象，从推荐对象中选择其他特征相似的对象作为推荐结果。\n",
    "\n",
    "（1）这一推荐算法首先提取推荐对象的内容特征，和用户模型中的用户兴趣偏好匹配，匹配度较高的推荐对象就可作为推荐结果推荐给用户。例如在进行音乐推荐时，系统分析用户以前选择的音乐的共性，找到用户的兴趣点。然后从其他音乐中选择和用户兴趣点相似的音乐推荐给用户；\n",
    "\n",
    "（2）计算推荐对象的内容特征和用户模型中兴趣特征二者之间的相似性是该推荐策略中一个关键部分；\n",
    "\n",
    "（3）计算所得的值按其大小排序，将最靠前的若干个对象作为推荐结果呈现给用户。\n",
    "\n",
    "基于内容的推荐算法中的关键就是用户模型描述和推荐对象内容特征描述。其中对推荐对象内容进行特征提取，目前对文本内容进行特征提取方法比较成熟，如浏览页面的推荐、新闻推荐等。但网上的多媒体信息大量涌现，而对这些多媒体数据进行特征提取还有待技术支持，所以多媒体信息还没有大量用于基于内容的推荐。\n",
    "\n",
    "基于内容的推荐的优点如下：\n",
    "\n",
    "（1）简单、有效，推荐结果直观，容易理解，不需要领域知识。\n",
    "\n",
    "（2）不需要用户的历史数据，如对对象的评价等。\n",
    "\n",
    "（3）没有关于新推荐对象出现的冷启动问题。\n",
    "\n",
    "（4）没有稀疏问题。\n",
    "\n",
    "（5）比较成熟的分类学习方法能够为该方法提供支持，如数据挖掘、聚类分析等。\n",
    "\n",
    " 基于内容的推荐的缺点如下：\n",
    "\n",
    "（1）该方法的广泛应用受到了推荐对象特征提取能力的限制较为严重。因为多媒体资源 没有有效的特征提取方法，比如图像、视频、音乐等。既使文本资源，其特征提取方法也只能反映资源的一部分内容，例如，难以提取网页内容的质量，这些特征可能影响到用户的满意度。\n",
    "\n",
    "（2）很难出现新的推荐结果。推荐对象的内容特征和用户的兴趣偏好匹配才能获得推荐，用户将仅限于获得跟以前类似的推荐结果，很难为用户发现新的感兴趣的信息。\n",
    "\n",
    "（3）存在新用户出现时的冷启动问题。当新用户出现时，系统较难获得该用户的兴趣偏好，就不能和推荐对象的内容特征进行匹配，该用户将较难获得满意的推荐结果。\n",
    "\n",
    "（4）对推荐对象内容分类方法需要的数据量较大。目前，尽管分类方法很多，但构造分类器时需要的数据量巨大，给分类带来一定困难。\n",
    "\n",
    "（5）不同语言的描述的用户模型和推荐对象模型无法兼容也是基于内容推荐系统面临的又一个大的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 协同过滤推荐\n",
    "\n",
    "协同过滤推荐（collaborative filtering recommendation）是推荐算法中最成功的算法，它于20世纪90年代开始研究并促进了整个推荐系统研究的繁荣。大量论文和研究都属于该类别。比如Grundy书籍推荐系统、Tapestry邮件处理系统，Grou-plens、Ringo等推荐系统都属于该类推荐。\n",
    "\n",
    "协同过滤推荐的基本思想借鉴了日常在选购商品、选择用餐饭店、选择看哪部电影等等的方法。如果自己身边的很多朋友都选购某种商品，那么自己就会很大概率的选择该商品。或者用户喜欢某类商品，当看到和这类商品相似商品并且其他用户对此类商品评价很高时，则购买的概率就会很大。协同推荐的用户模型为用户-项目评价矩阵。\n",
    "\n",
    "协同过滤推荐一般分为两类：基于用户的协同推荐(User-based Collaborative Filtering)(或基于内存的协同推荐(Mem-ory-Based Collaborative Filtering))、基于项目的协同推荐(Item-Based Collaborative Filtering)。\n",
    "\n",
    "![](https://img-blog.csdn.net/20141003160035934?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRFFfRE0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![](https://img-blog.csdn.net/20141003155928046?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRFFfRE0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 基于关联规则的推荐\n",
    "\n",
    "基于关联规则的推荐系统(AssociationRule一basedReeommender)是以关联规则挖掘算法为基础，把用户已购买(或者喜好)的项目作为规则头，把规则体作为待选推荐对象。\n",
    "\n",
    "##### 3.4 基于知识的推荐\n",
    "\n",
    "基于内容的过滤和协同过滤技术若没有经过足够数据的训练则其推荐质量非常低。基于知识的推荐技术不依赖客户对商品的评分数据量，而是通过推断用户的需要和偏好来作出推荐。总结查阅的文献，可将基于知识的推荐技术分为三类：数据库知识发现KDD(Knowledge Dis-covery in Database)、基于案例推理CBR(Case Based Reasoning)和知识推理KR(Knowledge Reasoning)。\n",
    "\n",
    "##### 3.5 组合（混合）推荐\n",
    "\n",
    "各种推荐方法都有各自的优缺点，在实际应用中可以针对具体问题采用推荐算法的组合进行推荐，即所谓的组合推荐。组合推荐的目的是通过组合不同的推荐算法，达到扬长避短的目的，从而产生更符合用户需求的推荐。理论上讲可以有很多种的推荐组合方法，但目前研究和应用最多的组合推荐是把基于内容的推荐和系统过滤推荐的组合。把它们组合方法根据应用场景不同而不同，主要混合思路有两种：\n",
    "\n",
    "（1）推荐结果混合：这是一种最简单的混合方法，就是分别是用两种或多种推荐方法产生推荐结果，然后采用某种算法把推荐结果进行混合而得到最终推荐。如何从众多推荐结果中选择用户需要的推荐结果成为该算法的一个重要研究点。比如采用投票机制来组合推荐结果，采用一定的标准对两者产生的推荐结果判断，从而选择其中之一，利用预测打分的线性组合进行推荐。\n",
    "\n",
    "（2）推荐算法的混合：以某种推荐算法为框架，混合另外的推荐算法，例如协同推荐的框架内混合基于内容的推荐（或相反）、基于协同推荐的框架内混合基于网络结构的推荐，社会网络分析法的推荐框架内混合基于内容的推荐，基于网络结构的推荐和基于社会网络分析法的推荐的混合等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、推荐系统的评测指标\n",
    "\n",
    "本节将介绍各种推荐系统的评测指标。这些评测指标可用于评价推荐系统各方面的性能。这些指标有些可以定量计算，有些只能定性描述，有些可以通过离线实验计算，有些需要通过用户调查获得，还有些只能在线评测。\n",
    "\n",
    "##### 4.1  用户满意度\n",
    "\n",
    "用户作为推荐系统的参与者，其满意度是评测推荐系统的最重要指标。但是，用户满意度没有办法离线计算，只能通过用户调查或者在线实验获得。\n",
    "\n",
    "用户调查获得用户满意度主要是通过调查问卷的形式。用户对推荐系统的满意度分为不同的层次。\n",
    "\n",
    "在线系统中，用户满意度主要通过一些对用户行为的统计得到。（1）电子商务网站中，用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以利用购买率度量用户的满意度（2）有些网站会通过设计一些用户反馈界面收集用户满意度，有对推荐结果满意或者不满意的反馈按钮，统计两种按钮的单击情况（3）更一般的情况下，我们可以用点击率、用户停留时间和转化率等指标度量用户的满意度\n",
    "\n",
    "##### 4.2  预测准确度\n",
    "\n",
    "预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的推荐系统离线评测指标。\n",
    "\n",
    "（1）需要一个包含用户的历史行为记录的离线数据集\n",
    "\n",
    "（2）将该训练集通过时间分成训练集和测试集\n",
    "\n",
    "（3）通过在训练集上建立用户的行为和兴趣模型预测用户在测试集上的行为\n",
    "\n",
    "（4）计算预测行为和测试集上实际行为的重合度作为预测准确度\n",
    "\n",
    "由于离线的推荐算法有不同的研究方向，因此下面将针对不同的研究方向介绍它们的预测准确度指标。\n",
    "\n",
    "评分预测\n",
    "\n",
    "预测用户对物品评分的行为称为评分预测。\n",
    "\n",
    "评分预测的预测准确度一般通过均方根误差和平均绝对误差计算。\n",
    "\n",
    "TopN推荐\n",
    "\n",
    "主要计算召回率和准确率。准确率就是指我推荐的n个物品中有多少个是对的，其所占的比重。召回率则是指正确结果中有多少比率的物品出现在了推荐结果中。两者的不同就是前者已推荐结果个数当除数，后者已正确结果个数当除数。\n",
    "\n",
    "##### 4.3  覆盖率\n",
    "\n",
    "覆盖率最简单的定义：推荐系统能够推荐出来的物品占总物品集合的比例\n",
    "\n",
    "用来描述一个推荐系统对物品长尾的发掘能力。\n",
    "\n",
    "就是指推荐出来的结果能不能很好的覆盖所有的商品，是不是所有的商品都有被推荐的机会。最简单的方法就是计算所有被推荐的商品占物品总数的比重，当然这个比较粗糙，更精确一点的可以用信息熵和基尼系数来度量。\n",
    "\n",
    "一个好的推荐系统不仅需要有比较高的用户满意度，也要有较高的覆盖率。\n",
    "\n",
    "研究表明现在主流的推荐算法都具有马太效应（强者更强，弱者更弱）。\n",
    "\n",
    "##### 4.4  多样性\n",
    "\n",
    "为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域，即推荐结果具有多样性。\n",
    "\n",
    "多样性推荐列表的好处用一句俗话表述就是“不在一棵树上吊死”。\n",
    "\n",
    "多样性描述了推荐列表中物品两两之间的不相似性，因此多样性和相似性是对应的。可以根据物品间的相似度来计算，一个推荐列表中如果所有物品间的相似度都比较高，那么往往说明都是同一类物品，缺乏多样性。\n",
    "\n",
    "比如我看电影，我既喜欢看格斗类的电影，同时又喜欢爱装文艺，那么给我的推荐列表中就应该这两个类型的电影都有，而且得根据我爱好比例来推荐，比如我平时80%是看格斗类的，20%是看文艺类的，那么推荐结果中最好也是这个比例。\n",
    "\n",
    "##### 4.5  新颖性\n",
    "\n",
    "新颖的推荐是指给用户推荐那些他们以前没有听说过的物品\n",
    "\n",
    "在一个网站中实现新颖性的最简单方法：把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。\n",
    "\n",
    "不能说系统推荐的物品其实我都知道，那这样推荐系统就完全失去了存在的意义，一般都希望推荐一些用户不知道的商品或者没看过没买过的商品。方法一是取出已经看到过买过的商品，但这还不够，一般会计算推荐商品的平均流行度，因为通常越不热门的物品越会让用户觉得新颖。比如我爱周星驰，那么推荐《临岐》就很有新颖性，因为大家都不知道这是周星驰出演的。\n",
    "\n",
    "##### 4.6  惊喜度\n",
    "\n",
    "惊喜度是最近这几年推荐系统领域最热门的话题。\n",
    "\n",
    "惊喜度和新颖性是有区别的：（1）如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高；（2）推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。\n",
    "\n",
    "提高推荐惊喜度需要提高推荐结果的用户满意度，同时降低推荐结果和用户历史兴趣度的相似度。\n",
    "\n",
    "##### 4.7  信任度\n",
    "\n",
    "度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。如果用户信任推荐系统，那么往往会增加与推荐系统的互动，从而获得更好的个性化推荐。\n",
    "\n",
    "提高推荐系统的信任度的两种方法：（1）增加推荐系统的透明度（如，提供推荐解释）；（2）考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释\n",
    "\n",
    "##### 4.8  实时性\n",
    "\n",
    "物品（新闻、微博等）具有很强的时效性，需要在物品还具有时效性时就将它们推荐给用户。\n",
    "\n",
    "推荐系统的实时性包括两个方面：\n",
    "\n",
    "（1）推荐系统需要实时地更新推荐列表来满足用户新的行为变化。\n",
    "\n",
    "（2）推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。\n",
    "\n",
    "##### 4.9  健壮性\n",
    "\n",
    "任何一个能带来利益的算法系统都会被人攻击，这方面最典型的例子就是搜索引擎（作弊和反作弊斗争）。\n",
    "\n",
    "推荐系统目前也遇到了同样的作弊问题，而健壮性指标衡量了一个推荐系统抗击作弊的能力。\n",
    "\n",
    "最著名的作弊方法：行为注入攻击。\n",
    "\n",
    "算法健壮性的评测主要利用模拟攻击。\n",
    "\n",
    "在实际系统中，提高系统的健壮性，除了选择健壮性高的算法，还有以下方法：（1）设计推荐推荐系统时尽量使用代价比较高的用户行为，比如有用户购买行为和用户浏览行为，那么主要应该使用用户购买行为，因为购买需要付费；（2）在使用数据前，进行攻击检测，从而对数据进行清理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习用于推荐系统\n",
    "推荐阅读：[deep learning 可以用来做推荐系统吗？](https://www.zhihu.com/question/20830906)\n",
    "\n",
    "> from 知乎 facebook推荐 宋一松 \n",
    "\n",
    "> 这个得分两方面来说。学术上，总体来说，deep learning (更确切地说，神经网络）不仅可以用来做推荐系统，还要比之前的手段表现要好。主要的优越性在于神经网络可以更好地表现稀疏的特征（sparse feature）而不需要太担心过度拟合。也正因为如此，无论是Google还是Facebook，都在逐步地把推荐系统替换成基于神经网络的（文末有附相关论文）。\n",
    "\n",
    "> 不过，既然回答这个问题了，就得说说另一方面的事情。如果说这么些年计算机科学的发展教会了我们什么道理，那就是在这个领域很少有什么绝对的更好或者更差，大多数时候都是trade-off（取舍），我们需要找的是收益与成本上最有效率的那个点。只不过技术进步会时常使得我们评价取舍的体系发生变化，从而导出不一样的结果。举例来说，互联网时代CDN和缓存的大规模出现是由于储存硬件的成本下降从而使我们更可能以空间换时间。而深度学习本身在学术上的再度复出也是由于基于算力的取舍发生了变化。\n",
    "\n",
    "> 学术的目的在于推动人类知识的边界，所以毫无疑问深度学习是未来的方向。然而在工业实践上是不是应该用深度学习做推荐系统，这取决于在每个具体情况下对收益与成本的考量。\n",
    "\n",
    "> 收益：\n",
    "\n",
    "> **1. 有没有必须上深度学习才能实现的提升？**\n",
    "> * 正如上文所说，神经网络的优越性在于对稀疏特征的表达。这对那些内容类型和用户品味五花八门的内容平台极为重要，比如Google和Facebook。而对于还处在中前期，内容和用户群调性都比较一致的产品，神经网络的独有优势并不是十分强。\n",
    "\n",
    "> **2. 是不是只要上了深度学习就能提升？**\n",
    "> * 一个明显的因素就是feature的数量和质量。如果feature上做得不够多，上了神经网络也没用。\n",
    "\n",
    "> 成本：\n",
    "> * 1. 用于开发神经网络的人力机会成本把开发资源放到对基础性工作的完善上，比如做feature上，可以有多少提升。\n",
    "> * 2. 维持额外算力的财务机会成本把钱用来推广/发工资/买好电脑来提高开发效率，会不会收效更好。\n",
    "\n",
    "> 总之，我相信神经网络一定是未来的方向，但目前，它在工业实践上是不是一个最优的方案，需要各家具体问题具体分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.MF(matrix factorization)\n",
    "代码 from [OpenLearning4DeepRecsys](https://github.com/Leavingseason/OpenLearning4DeepRecsys/tree/master/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0  RMSE(train): 0.828885  RMSE(test): 0.831282   RMSE(eval): 0.824168   LOSS(train): 0.828911  minutes: 0.022461\n",
      "Iteration 1  RMSE(train): 0.823735  RMSE(test): 0.826516   RMSE(eval): 0.819457   LOSS(train): 0.823769  minutes: 0.021060\n",
      "Iteration 2  RMSE(train): 0.819151  RMSE(test): 0.822296   RMSE(eval): 0.815296   LOSS(train): 0.819196  minutes: 0.021213\n",
      "Iteration 3  RMSE(train): 0.815020  RMSE(test): 0.818516   RMSE(eval): 0.811574   LOSS(train): 0.815080  minutes: 0.020559\n",
      "Iteration 4  RMSE(train): 0.811261  RMSE(test): 0.815094   RMSE(eval): 0.808211   LOSS(train): 0.811339  minutes: 0.020464\n",
      "Iteration 5  RMSE(train): 0.807808  RMSE(test): 0.811969   RMSE(eval): 0.805143   LOSS(train): 0.807907  minutes: 0.020868\n",
      "Iteration 6  RMSE(train): 0.804613  RMSE(test): 0.809092   RMSE(eval): 0.802321   LOSS(train): 0.804735  minutes: 0.020892\n",
      "Iteration 7  RMSE(train): 0.801638  RMSE(test): 0.806427   RMSE(eval): 0.799708   LOSS(train): 0.801784  minutes: 0.021434\n",
      "Iteration 8  RMSE(train): 0.798851  RMSE(test): 0.803942   RMSE(eval): 0.797275   LOSS(train): 0.799023  minutes: 0.020467\n",
      "Iteration 9  RMSE(train): 0.796227  RMSE(test): 0.801615   RMSE(eval): 0.794995   LOSS(train): 0.796427  minutes: 0.020853\n",
      "Iteration 10  RMSE(train): 0.793745  RMSE(test): 0.799427   RMSE(eval): 0.792852   LOSS(train): 0.793974  minutes: 0.023399\n",
      "Iteration 11  RMSE(train): 0.791391  RMSE(test): 0.797362   RMSE(eval): 0.790829   LOSS(train): 0.791651  minutes: 0.020777\n",
      "Iteration 12  RMSE(train): 0.789151  RMSE(test): 0.795407   RMSE(eval): 0.788912   LOSS(train): 0.789442  minutes: 0.020789\n",
      "Iteration 13  RMSE(train): 0.787013  RMSE(test): 0.793550   RMSE(eval): 0.787092   LOSS(train): 0.787337  minutes: 0.022166\n",
      "Iteration 14  RMSE(train): 0.784967  RMSE(test): 0.791783   RMSE(eval): 0.785360   LOSS(train): 0.785325  minutes: 0.020302\n",
      "Iteration 15  RMSE(train): 0.783006  RMSE(test): 0.790097   RMSE(eval): 0.783706   LOSS(train): 0.783398  minutes: 0.021501\n",
      "Iteration 16  RMSE(train): 0.781123  RMSE(test): 0.788486   RMSE(eval): 0.782125   LOSS(train): 0.781551  minutes: 0.022210\n",
      "Iteration 17  RMSE(train): 0.779310  RMSE(test): 0.786943   RMSE(eval): 0.780611   LOSS(train): 0.779774  minutes: 0.020859\n",
      "Iteration 18  RMSE(train): 0.777563  RMSE(test): 0.785464   RMSE(eval): 0.779158   LOSS(train): 0.778064  minutes: 0.021124\n",
      "Iteration 19  RMSE(train): 0.775877  RMSE(test): 0.784044   RMSE(eval): 0.777763   LOSS(train): 0.776416  minutes: 0.022013\n",
      "Iteration 20  RMSE(train): 0.774248  RMSE(test): 0.782678   RMSE(eval): 0.776420   LOSS(train): 0.774826  minutes: 0.021173\n",
      "Iteration 21  RMSE(train): 0.772673  RMSE(test): 0.781364   RMSE(eval): 0.775127   LOSS(train): 0.773290  minutes: 0.020564\n",
      "Iteration 22  RMSE(train): 0.771147  RMSE(test): 0.780097   RMSE(eval): 0.773881   LOSS(train): 0.771803  minutes: 0.021008\n",
      "Iteration 23  RMSE(train): 0.769668  RMSE(test): 0.778875   RMSE(eval): 0.772679   LOSS(train): 0.770365  minutes: 0.022594\n",
      "Iteration 24  RMSE(train): 0.768234  RMSE(test): 0.777695   RMSE(eval): 0.771517   LOSS(train): 0.768971  minutes: 0.020457\n",
      "Iteration 25  RMSE(train): 0.766841  RMSE(test): 0.776554   RMSE(eval): 0.770393   LOSS(train): 0.767619  minutes: 0.020667\n",
      "Iteration 26  RMSE(train): 0.765487  RMSE(test): 0.775451   RMSE(eval): 0.769307   LOSS(train): 0.766307  minutes: 0.022057\n",
      "Iteration 27  RMSE(train): 0.764170  RMSE(test): 0.774383   RMSE(eval): 0.768254   LOSS(train): 0.765033  minutes: 0.020422\n",
      "Iteration 28  RMSE(train): 0.762889  RMSE(test): 0.773348   RMSE(eval): 0.767234   LOSS(train): 0.763794  minutes: 0.020863\n",
      "Iteration 29  RMSE(train): 0.761641  RMSE(test): 0.772345   RMSE(eval): 0.766246   LOSS(train): 0.762589  minutes: 0.021280\n",
      "Iteration 30  RMSE(train): 0.760427  RMSE(test): 0.771372   RMSE(eval): 0.765286   LOSS(train): 0.761418  minutes: 0.021766\n",
      "Iteration 31  RMSE(train): 0.759242  RMSE(test): 0.770428   RMSE(eval): 0.764355   LOSS(train): 0.760277  minutes: 0.020191\n",
      "Iteration 32  RMSE(train): 0.758086  RMSE(test): 0.769511   RMSE(eval): 0.763451   LOSS(train): 0.759165  minutes: 0.020746\n",
      "Iteration 33  RMSE(train): 0.756958  RMSE(test): 0.768620   RMSE(eval): 0.762571   LOSS(train): 0.758082  minutes: 0.022417\n",
      "Iteration 34  RMSE(train): 0.755858  RMSE(test): 0.767754   RMSE(eval): 0.761717   LOSS(train): 0.757026  minutes: 0.020648\n",
      "Iteration 35  RMSE(train): 0.754782  RMSE(test): 0.766911   RMSE(eval): 0.760885   LOSS(train): 0.755995  minutes: 0.020338\n",
      "Iteration 36  RMSE(train): 0.753731  RMSE(test): 0.766091   RMSE(eval): 0.760076   LOSS(train): 0.754990  minutes: 0.020573\n",
      "Iteration 37  RMSE(train): 0.752704  RMSE(test): 0.765293   RMSE(eval): 0.759288   LOSS(train): 0.754008  minutes: 0.020654\n",
      "Iteration 38  RMSE(train): 0.751699  RMSE(test): 0.764516   RMSE(eval): 0.758520   LOSS(train): 0.753049  minutes: 0.020530\n",
      "Iteration 39  RMSE(train): 0.750717  RMSE(test): 0.763758   RMSE(eval): 0.757772   LOSS(train): 0.752112  minutes: 0.020384\n",
      "Iteration 40  RMSE(train): 0.749754  RMSE(test): 0.763020   RMSE(eval): 0.757043   LOSS(train): 0.751196  minutes: 0.020342\n",
      "Iteration 41  RMSE(train): 0.748813  RMSE(test): 0.762300   RMSE(eval): 0.756332   LOSS(train): 0.750301  minutes: 0.020128\n",
      "Iteration 42  RMSE(train): 0.747891  RMSE(test): 0.761598   RMSE(eval): 0.755639   LOSS(train): 0.749426  minutes: 0.020415\n",
      "Iteration 43  RMSE(train): 0.746988  RMSE(test): 0.760913   RMSE(eval): 0.754962   LOSS(train): 0.748569  minutes: 0.020220\n",
      "Iteration 44  RMSE(train): 0.746103  RMSE(test): 0.760244   RMSE(eval): 0.754302   LOSS(train): 0.747731  minutes: 0.020419\n",
      "Iteration 45  RMSE(train): 0.745235  RMSE(test): 0.759591   RMSE(eval): 0.753657   LOSS(train): 0.746910  minutes: 0.020459\n",
      "Iteration 46  RMSE(train): 0.744385  RMSE(test): 0.758953   RMSE(eval): 0.753027   LOSS(train): 0.746107  minutes: 0.020365\n",
      "Iteration 47  RMSE(train): 0.743550  RMSE(test): 0.758330   RMSE(eval): 0.752411   LOSS(train): 0.745320  minutes: 0.020409\n",
      "Iteration 48  RMSE(train): 0.742732  RMSE(test): 0.757722   RMSE(eval): 0.751810   LOSS(train): 0.744549  minutes: 0.020400\n",
      "Iteration 49  RMSE(train): 0.741929  RMSE(test): 0.757126   RMSE(eval): 0.751222   LOSS(train): 0.743794  minutes: 0.020325\n",
      "Iteration 50  RMSE(train): 0.741142  RMSE(test): 0.756544   RMSE(eval): 0.750647   LOSS(train): 0.743054  minutes: 0.020622\n",
      "Iteration 51  RMSE(train): 0.740368  RMSE(test): 0.755975   RMSE(eval): 0.750085   LOSS(train): 0.742329  minutes: 0.020340\n",
      "Iteration 52  RMSE(train): 0.739609  RMSE(test): 0.755419   RMSE(eval): 0.749535   LOSS(train): 0.741617  minutes: 0.020849\n",
      "Iteration 53  RMSE(train): 0.738864  RMSE(test): 0.754874   RMSE(eval): 0.748997   LOSS(train): 0.740920  minutes: 0.020480\n",
      "Iteration 54  RMSE(train): 0.738131  RMSE(test): 0.754341   RMSE(eval): 0.748470   LOSS(train): 0.740235  minutes: 0.020865\n",
      "Iteration 55  RMSE(train): 0.737412  RMSE(test): 0.753819   RMSE(eval): 0.747955   LOSS(train): 0.739564  minutes: 0.020630\n",
      "Iteration 56  RMSE(train): 0.736704  RMSE(test): 0.753309   RMSE(eval): 0.747450   LOSS(train): 0.738905  minutes: 0.020407\n",
      "Iteration 57  RMSE(train): 0.736009  RMSE(test): 0.752808   RMSE(eval): 0.746955   LOSS(train): 0.738258  minutes: 0.019964\n",
      "Iteration 58  RMSE(train): 0.735327  RMSE(test): 0.752318   RMSE(eval): 0.746471   LOSS(train): 0.737624  minutes: 0.023071\n",
      "Iteration 59  RMSE(train): 0.734655  RMSE(test): 0.751838   RMSE(eval): 0.745997   LOSS(train): 0.737001  minutes: 0.025801\n",
      "Iteration 60  RMSE(train): 0.733995  RMSE(test): 0.751368   RMSE(eval): 0.745532   LOSS(train): 0.736388  minutes: 0.021730\n",
      "Iteration 61  RMSE(train): 0.733346  RMSE(test): 0.750907   RMSE(eval): 0.745076   LOSS(train): 0.735788  minutes: 0.022770\n",
      "Iteration 62  RMSE(train): 0.732706  RMSE(test): 0.750455   RMSE(eval): 0.744630   LOSS(train): 0.735196  minutes: 0.024181\n",
      "Iteration 63  RMSE(train): 0.732077  RMSE(test): 0.750012   RMSE(eval): 0.744192   LOSS(train): 0.734616  minutes: 0.022317\n",
      "Iteration 64  RMSE(train): 0.731459  RMSE(test): 0.749577   RMSE(eval): 0.743762   LOSS(train): 0.734046  minutes: 0.022535\n",
      "Iteration 65  RMSE(train): 0.730850  RMSE(test): 0.749151   RMSE(eval): 0.743341   LOSS(train): 0.733485  minutes: 0.023079\n",
      "Iteration 66  RMSE(train): 0.730251  RMSE(test): 0.748733   RMSE(eval): 0.742928   LOSS(train): 0.732935  minutes: 0.022187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67  RMSE(train): 0.729661  RMSE(test): 0.748323   RMSE(eval): 0.742523   LOSS(train): 0.732394  minutes: 0.020595\n",
      "Iteration 68  RMSE(train): 0.729081  RMSE(test): 0.747920   RMSE(eval): 0.742125   LOSS(train): 0.731861  minutes: 0.020433\n",
      "Iteration 69  RMSE(train): 0.728509  RMSE(test): 0.747526   RMSE(eval): 0.741735   LOSS(train): 0.731338  minutes: 0.022404\n",
      "Iteration 70  RMSE(train): 0.727946  RMSE(test): 0.747137   RMSE(eval): 0.741352   LOSS(train): 0.730824  minutes: 0.021968\n",
      "Iteration 71  RMSE(train): 0.727391  RMSE(test): 0.746757   RMSE(eval): 0.740976   LOSS(train): 0.730317  minutes: 0.023113\n",
      "Iteration 72  RMSE(train): 0.726844  RMSE(test): 0.746384   RMSE(eval): 0.740606   LOSS(train): 0.729818  minutes: 0.023761\n",
      "Iteration 73  RMSE(train): 0.726306  RMSE(test): 0.746016   RMSE(eval): 0.740244   LOSS(train): 0.729328  minutes: 0.021316\n",
      "Iteration 74  RMSE(train): 0.725775  RMSE(test): 0.745656   RMSE(eval): 0.739888   LOSS(train): 0.728846  minutes: 0.024623\n",
      "Iteration 75  RMSE(train): 0.725253  RMSE(test): 0.745302   RMSE(eval): 0.739538   LOSS(train): 0.728372  minutes: 0.022593\n",
      "Iteration 76  RMSE(train): 0.724738  RMSE(test): 0.744954   RMSE(eval): 0.739194   LOSS(train): 0.727905  minutes: 0.023401\n",
      "Iteration 77  RMSE(train): 0.724230  RMSE(test): 0.744612   RMSE(eval): 0.738857   LOSS(train): 0.727445  minutes: 0.021880\n",
      "Iteration 78  RMSE(train): 0.723729  RMSE(test): 0.744277   RMSE(eval): 0.738525   LOSS(train): 0.726993  minutes: 0.023386\n",
      "Iteration 79  RMSE(train): 0.723235  RMSE(test): 0.743947   RMSE(eval): 0.738199   LOSS(train): 0.726547  minutes: 0.020992\n",
      "Iteration 80  RMSE(train): 0.722749  RMSE(test): 0.743623   RMSE(eval): 0.737878   LOSS(train): 0.726108  minutes: 0.021680\n",
      "Iteration 81  RMSE(train): 0.722269  RMSE(test): 0.743304   RMSE(eval): 0.737563   LOSS(train): 0.725677  minutes: 0.023709\n",
      "Iteration 82  RMSE(train): 0.721796  RMSE(test): 0.742991   RMSE(eval): 0.737254   LOSS(train): 0.725252  minutes: 0.020170\n",
      "Iteration 83  RMSE(train): 0.721329  RMSE(test): 0.742683   RMSE(eval): 0.736949   LOSS(train): 0.724832  minutes: 0.023997\n",
      "Iteration 84  RMSE(train): 0.720868  RMSE(test): 0.742380   RMSE(eval): 0.736650   LOSS(train): 0.724420  minutes: 0.021721\n",
      "Iteration 85  RMSE(train): 0.720414  RMSE(test): 0.742081   RMSE(eval): 0.736355   LOSS(train): 0.724013  minutes: 0.023961\n",
      "Iteration 86  RMSE(train): 0.719966  RMSE(test): 0.741788   RMSE(eval): 0.736066   LOSS(train): 0.723613  minutes: 0.021935\n",
      "Iteration 87  RMSE(train): 0.719524  RMSE(test): 0.741500   RMSE(eval): 0.735781   LOSS(train): 0.723219  minutes: 0.020844\n",
      "Iteration 88  RMSE(train): 0.719088  RMSE(test): 0.741217   RMSE(eval): 0.735500   LOSS(train): 0.722830  minutes: 0.023873\n",
      "Iteration 89  RMSE(train): 0.718656  RMSE(test): 0.740937   RMSE(eval): 0.735225   LOSS(train): 0.722446  minutes: 0.021512\n",
      "Iteration 90  RMSE(train): 0.718231  RMSE(test): 0.740663   RMSE(eval): 0.734953   LOSS(train): 0.722068  minutes: 0.022402\n",
      "Iteration 91  RMSE(train): 0.717813  RMSE(test): 0.740393   RMSE(eval): 0.734686   LOSS(train): 0.721697  minutes: 0.022344\n",
      "Iteration 92  RMSE(train): 0.717398  RMSE(test): 0.740127   RMSE(eval): 0.734424   LOSS(train): 0.721330  minutes: 0.023966\n",
      "Iteration 93  RMSE(train): 0.716989  RMSE(test): 0.739865   RMSE(eval): 0.734165   LOSS(train): 0.720968  minutes: 0.020705\n",
      "Iteration 94  RMSE(train): 0.716585  RMSE(test): 0.739608   RMSE(eval): 0.733910   LOSS(train): 0.720611  minutes: 0.023926\n",
      "Iteration 95  RMSE(train): 0.716187  RMSE(test): 0.739354   RMSE(eval): 0.733660   LOSS(train): 0.720260  minutes: 0.022306\n",
      "Iteration 96  RMSE(train): 0.715793  RMSE(test): 0.739104   RMSE(eval): 0.733413   LOSS(train): 0.719913  minutes: 0.022145\n",
      "Iteration 97  RMSE(train): 0.715405  RMSE(test): 0.738858   RMSE(eval): 0.733170   LOSS(train): 0.719572  minutes: 0.024875\n",
      "Iteration 98  RMSE(train): 0.715021  RMSE(test): 0.738616   RMSE(eval): 0.732931   LOSS(train): 0.719235  minutes: 0.024375\n",
      "Iteration 99  RMSE(train): 0.714641  RMSE(test): 0.738378   RMSE(eval): 0.732695   LOSS(train): 0.718902  minutes: 0.021752\n",
      "Iteration 100  RMSE(train): 0.714267  RMSE(test): 0.738143   RMSE(eval): 0.732463   LOSS(train): 0.718574  minutes: 0.024351\n",
      "Iteration 101  RMSE(train): 0.713896  RMSE(test): 0.737912   RMSE(eval): 0.732234   LOSS(train): 0.718250  minutes: 0.022528\n",
      "Iteration 102  RMSE(train): 0.713531  RMSE(test): 0.737684   RMSE(eval): 0.732009   LOSS(train): 0.717931  minutes: 0.023427\n",
      "Iteration 103  RMSE(train): 0.713170  RMSE(test): 0.737460   RMSE(eval): 0.731787   LOSS(train): 0.717617  minutes: 0.022609\n",
      "Iteration 104  RMSE(train): 0.712813  RMSE(test): 0.737239   RMSE(eval): 0.731569   LOSS(train): 0.717306  minutes: 0.024090\n",
      "Iteration 105  RMSE(train): 0.712461  RMSE(test): 0.737021   RMSE(eval): 0.731354   LOSS(train): 0.716999  minutes: 0.021354\n",
      "Iteration 106  RMSE(train): 0.712112  RMSE(test): 0.736806   RMSE(eval): 0.731141   LOSS(train): 0.716697  minutes: 0.023656\n",
      "Iteration 107  RMSE(train): 0.711768  RMSE(test): 0.736595   RMSE(eval): 0.730932   LOSS(train): 0.716399  minutes: 0.025039\n",
      "Iteration 108  RMSE(train): 0.711427  RMSE(test): 0.736386   RMSE(eval): 0.730726   LOSS(train): 0.716104  minutes: 0.025628\n",
      "Iteration 109  RMSE(train): 0.711091  RMSE(test): 0.736180   RMSE(eval): 0.730523   LOSS(train): 0.715814  minutes: 0.023503\n",
      "Iteration 110  RMSE(train): 0.710759  RMSE(test): 0.735978   RMSE(eval): 0.730323   LOSS(train): 0.715527  minutes: 0.023635\n",
      "Iteration 111  RMSE(train): 0.710430  RMSE(test): 0.735779   RMSE(eval): 0.730126   LOSS(train): 0.715244  minutes: 0.022054\n",
      "Iteration 112  RMSE(train): 0.710105  RMSE(test): 0.735582   RMSE(eval): 0.729931   LOSS(train): 0.714964  minutes: 0.021685\n",
      "Iteration 113  RMSE(train): 0.709784  RMSE(test): 0.735388   RMSE(eval): 0.729739   LOSS(train): 0.714688  minutes: 0.023075\n",
      "Iteration 114  RMSE(train): 0.709466  RMSE(test): 0.735196   RMSE(eval): 0.729550   LOSS(train): 0.714416  minutes: 0.024492\n",
      "Iteration 115  RMSE(train): 0.709152  RMSE(test): 0.735008   RMSE(eval): 0.729364   LOSS(train): 0.714147  minutes: 0.021935\n",
      "Iteration 116  RMSE(train): 0.708842  RMSE(test): 0.734822   RMSE(eval): 0.729180   LOSS(train): 0.713882  minutes: 0.022241\n",
      "Iteration 117  RMSE(train): 0.708534  RMSE(test): 0.734639   RMSE(eval): 0.728999   LOSS(train): 0.713619  minutes: 0.021054\n",
      "Iteration 118  RMSE(train): 0.708231  RMSE(test): 0.734458   RMSE(eval): 0.728820   LOSS(train): 0.713361  minutes: 0.022296\n",
      "Iteration 119  RMSE(train): 0.707930  RMSE(test): 0.734279   RMSE(eval): 0.728644   LOSS(train): 0.713104  minutes: 0.022161\n",
      "Iteration 120  RMSE(train): 0.707633  RMSE(test): 0.734103   RMSE(eval): 0.728470   LOSS(train): 0.712852  minutes: 0.022130\n",
      "Iteration 121  RMSE(train): 0.707339  RMSE(test): 0.733930   RMSE(eval): 0.728298   LOSS(train): 0.712603  minutes: 0.022031\n",
      "Iteration 122  RMSE(train): 0.707048  RMSE(test): 0.733759   RMSE(eval): 0.728129   LOSS(train): 0.712356  minutes: 0.023326\n",
      "Iteration 123  RMSE(train): 0.706761  RMSE(test): 0.733590   RMSE(eval): 0.727962   LOSS(train): 0.712114  minutes: 0.021552\n",
      "Iteration 124  RMSE(train): 0.706477  RMSE(test): 0.733423   RMSE(eval): 0.727797   LOSS(train): 0.711873  minutes: 0.021184\n",
      "Iteration 125  RMSE(train): 0.706195  RMSE(test): 0.733259   RMSE(eval): 0.727635   LOSS(train): 0.711636  minutes: 0.021400\n",
      "Iteration 126  RMSE(train): 0.705916  RMSE(test): 0.733097   RMSE(eval): 0.727475   LOSS(train): 0.711401  minutes: 0.020833\n",
      "Iteration 127  RMSE(train): 0.705641  RMSE(test): 0.732937   RMSE(eval): 0.727316   LOSS(train): 0.711169  minutes: 0.021259\n",
      "Iteration 128  RMSE(train): 0.705368  RMSE(test): 0.732779   RMSE(eval): 0.727160   LOSS(train): 0.710941  minutes: 0.021770\n",
      "Iteration 129  RMSE(train): 0.705099  RMSE(test): 0.732623   RMSE(eval): 0.727006   LOSS(train): 0.710715  minutes: 0.021391\n",
      "Iteration 130  RMSE(train): 0.704832  RMSE(test): 0.732470   RMSE(eval): 0.726854   LOSS(train): 0.710492  minutes: 0.021894\n",
      "Iteration 131  RMSE(train): 0.704568  RMSE(test): 0.732318   RMSE(eval): 0.726704   LOSS(train): 0.710271  minutes: 0.023160\n",
      "Iteration 132  RMSE(train): 0.704307  RMSE(test): 0.732168   RMSE(eval): 0.726555   LOSS(train): 0.710053  minutes: 0.021575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 133  RMSE(train): 0.704048  RMSE(test): 0.732020   RMSE(eval): 0.726409   LOSS(train): 0.709837  minutes: 0.022457\n",
      "Iteration 134  RMSE(train): 0.703792  RMSE(test): 0.731875   RMSE(eval): 0.726265   LOSS(train): 0.709624  minutes: 0.027232\n",
      "Iteration 135  RMSE(train): 0.703538  RMSE(test): 0.731730   RMSE(eval): 0.726122   LOSS(train): 0.709414  minutes: 0.024548\n",
      "Iteration 136  RMSE(train): 0.703288  RMSE(test): 0.731588   RMSE(eval): 0.725982   LOSS(train): 0.709206  minutes: 0.022925\n",
      "Iteration 137  RMSE(train): 0.703039  RMSE(test): 0.731448   RMSE(eval): 0.725843   LOSS(train): 0.709000  minutes: 0.026056\n",
      "Iteration 138  RMSE(train): 0.702793  RMSE(test): 0.731309   RMSE(eval): 0.725706   LOSS(train): 0.708797  minutes: 0.025045\n",
      "Iteration 139  RMSE(train): 0.702551  RMSE(test): 0.731172   RMSE(eval): 0.725570   LOSS(train): 0.708597  minutes: 0.023115\n",
      "Iteration 140  RMSE(train): 0.702309  RMSE(test): 0.731037   RMSE(eval): 0.725436   LOSS(train): 0.708398  minutes: 0.022345\n",
      "Iteration 141  RMSE(train): 0.702071  RMSE(test): 0.730904   RMSE(eval): 0.725304   LOSS(train): 0.708201  minutes: 0.021223\n",
      "Iteration 142  RMSE(train): 0.701835  RMSE(test): 0.730772   RMSE(eval): 0.725174   LOSS(train): 0.708007  minutes: 0.021679\n",
      "Iteration 143  RMSE(train): 0.701601  RMSE(test): 0.730642   RMSE(eval): 0.725045   LOSS(train): 0.707815  minutes: 0.022322\n",
      "Iteration 144  RMSE(train): 0.701369  RMSE(test): 0.730514   RMSE(eval): 0.724918   LOSS(train): 0.707626  minutes: 0.021066\n",
      "Iteration 145  RMSE(train): 0.701140  RMSE(test): 0.730387   RMSE(eval): 0.724792   LOSS(train): 0.707438  minutes: 0.022873\n",
      "Iteration 146  RMSE(train): 0.700913  RMSE(test): 0.730262   RMSE(eval): 0.724668   LOSS(train): 0.707253  minutes: 0.020280\n",
      "Iteration 147  RMSE(train): 0.700688  RMSE(test): 0.730138   RMSE(eval): 0.724546   LOSS(train): 0.707069  minutes: 0.021206\n",
      "Iteration 148  RMSE(train): 0.700466  RMSE(test): 0.730016   RMSE(eval): 0.724425   LOSS(train): 0.706889  minutes: 0.020892\n",
      "Iteration 149  RMSE(train): 0.700244  RMSE(test): 0.729895   RMSE(eval): 0.724305   LOSS(train): 0.706708  minutes: 0.021002\n",
      "Iteration 150  RMSE(train): 0.700027  RMSE(test): 0.729776   RMSE(eval): 0.724187   LOSS(train): 0.706532  minutes: 0.023036\n",
      "Iteration 151  RMSE(train): 0.699811  RMSE(test): 0.729658   RMSE(eval): 0.724070   LOSS(train): 0.706357  minutes: 0.021820\n",
      "Iteration 152  RMSE(train): 0.699596  RMSE(test): 0.729542   RMSE(eval): 0.723955   LOSS(train): 0.706183  minutes: 0.021622\n",
      "Iteration 153  RMSE(train): 0.699384  RMSE(test): 0.729427   RMSE(eval): 0.723841   LOSS(train): 0.706012  minutes: 0.021693\n",
      "Iteration 154  RMSE(train): 0.699174  RMSE(test): 0.729313   RMSE(eval): 0.723728   LOSS(train): 0.705843  minutes: 0.021230\n",
      "Iteration 155  RMSE(train): 0.698965  RMSE(test): 0.729201   RMSE(eval): 0.723617   LOSS(train): 0.705675  minutes: 0.021653\n",
      "Iteration 156  RMSE(train): 0.698760  RMSE(test): 0.729090   RMSE(eval): 0.723507   LOSS(train): 0.705510  minutes: 0.023884\n",
      "Iteration 157  RMSE(train): 0.698556  RMSE(test): 0.728980   RMSE(eval): 0.723399   LOSS(train): 0.705346  minutes: 0.022243\n",
      "Iteration 158  RMSE(train): 0.698353  RMSE(test): 0.728873   RMSE(eval): 0.723291   LOSS(train): 0.705183  minutes: 0.022091\n",
      "Iteration 159  RMSE(train): 0.698152  RMSE(test): 0.728765   RMSE(eval): 0.723185   LOSS(train): 0.705023  minutes: 0.021208\n",
      "Iteration 160  RMSE(train): 0.697953  RMSE(test): 0.728660   RMSE(eval): 0.723080   LOSS(train): 0.704864  minutes: 0.021098\n",
      "Iteration 161  RMSE(train): 0.697756  RMSE(test): 0.728555   RMSE(eval): 0.722976   LOSS(train): 0.704707  minutes: 0.021827\n",
      "Iteration 162  RMSE(train): 0.697561  RMSE(test): 0.728452   RMSE(eval): 0.722874   LOSS(train): 0.704552  minutes: 0.021526\n",
      "Iteration 163  RMSE(train): 0.697368  RMSE(test): 0.728350   RMSE(eval): 0.722773   LOSS(train): 0.704398  minutes: 0.020900\n",
      "Iteration 164  RMSE(train): 0.697177  RMSE(test): 0.728249   RMSE(eval): 0.722673   LOSS(train): 0.704246  minutes: 0.021129\n",
      "Iteration 165  RMSE(train): 0.696987  RMSE(test): 0.728150   RMSE(eval): 0.722573   LOSS(train): 0.704095  minutes: 0.020932\n",
      "Iteration 166  RMSE(train): 0.696799  RMSE(test): 0.728051   RMSE(eval): 0.722476   LOSS(train): 0.703947  minutes: 0.020864\n",
      "Iteration 167  RMSE(train): 0.696612  RMSE(test): 0.727954   RMSE(eval): 0.722379   LOSS(train): 0.703800  minutes: 0.024227\n",
      "Iteration 168  RMSE(train): 0.696427  RMSE(test): 0.727858   RMSE(eval): 0.722283   LOSS(train): 0.703654  minutes: 0.021882\n",
      "Iteration 169  RMSE(train): 0.696245  RMSE(test): 0.727762   RMSE(eval): 0.722189   LOSS(train): 0.703510  minutes: 0.020966\n",
      "Iteration 170  RMSE(train): 0.696063  RMSE(test): 0.727669   RMSE(eval): 0.722095   LOSS(train): 0.703367  minutes: 0.022335\n",
      "Iteration 171  RMSE(train): 0.695883  RMSE(test): 0.727575   RMSE(eval): 0.722003   LOSS(train): 0.703226  minutes: 0.022494\n",
      "Iteration 172  RMSE(train): 0.695705  RMSE(test): 0.727483   RMSE(eval): 0.721912   LOSS(train): 0.703086  minutes: 0.020488\n",
      "Iteration 173  RMSE(train): 0.695528  RMSE(test): 0.727393   RMSE(eval): 0.721821   LOSS(train): 0.702948  minutes: 0.021507\n",
      "Iteration 174  RMSE(train): 0.695353  RMSE(test): 0.727303   RMSE(eval): 0.721732   LOSS(train): 0.702811  minutes: 0.021433\n",
      "Iteration 175  RMSE(train): 0.695179  RMSE(test): 0.727214   RMSE(eval): 0.721644   LOSS(train): 0.702676  minutes: 0.020466\n",
      "Iteration 176  RMSE(train): 0.695007  RMSE(test): 0.727126   RMSE(eval): 0.721557   LOSS(train): 0.702542  minutes: 0.021402\n",
      "Iteration 177  RMSE(train): 0.694837  RMSE(test): 0.727039   RMSE(eval): 0.721470   LOSS(train): 0.702409  minutes: 0.021245\n",
      "Iteration 178  RMSE(train): 0.694668  RMSE(test): 0.726953   RMSE(eval): 0.721385   LOSS(train): 0.702278  minutes: 0.020802\n",
      "Iteration 179  RMSE(train): 0.694500  RMSE(test): 0.726869   RMSE(eval): 0.721300   LOSS(train): 0.702148  minutes: 0.020985\n",
      "Iteration 180  RMSE(train): 0.694333  RMSE(test): 0.726785   RMSE(eval): 0.721216   LOSS(train): 0.702019  minutes: 0.022985\n",
      "Iteration 181  RMSE(train): 0.694169  RMSE(test): 0.726702   RMSE(eval): 0.721134   LOSS(train): 0.701893  minutes: 0.021000\n",
      "Iteration 182  RMSE(train): 0.694006  RMSE(test): 0.726620   RMSE(eval): 0.721052   LOSS(train): 0.701767  minutes: 0.021743\n",
      "Iteration 183  RMSE(train): 0.693844  RMSE(test): 0.726538   RMSE(eval): 0.720971   LOSS(train): 0.701643  minutes: 0.020421\n",
      "Iteration 184  RMSE(train): 0.693683  RMSE(test): 0.726458   RMSE(eval): 0.720891   LOSS(train): 0.701519  minutes: 0.021392\n",
      "Iteration 185  RMSE(train): 0.693524  RMSE(test): 0.726379   RMSE(eval): 0.720812   LOSS(train): 0.701396  minutes: 0.020664\n",
      "Iteration 186  RMSE(train): 0.693366  RMSE(test): 0.726300   RMSE(eval): 0.720734   LOSS(train): 0.701275  minutes: 0.020398\n",
      "Iteration 187  RMSE(train): 0.693209  RMSE(test): 0.726223   RMSE(eval): 0.720657   LOSS(train): 0.701156  minutes: 0.022540\n",
      "Iteration 188  RMSE(train): 0.693054  RMSE(test): 0.726146   RMSE(eval): 0.720580   LOSS(train): 0.701037  minutes: 0.021825\n",
      "Iteration 189  RMSE(train): 0.692900  RMSE(test): 0.726070   RMSE(eval): 0.720504   LOSS(train): 0.700920  minutes: 0.021522\n",
      "Iteration 190  RMSE(train): 0.692748  RMSE(test): 0.725995   RMSE(eval): 0.720429   LOSS(train): 0.700804  minutes: 0.020219\n",
      "Iteration 191  RMSE(train): 0.692596  RMSE(test): 0.725920   RMSE(eval): 0.720355   LOSS(train): 0.700689  minutes: 0.022079\n",
      "Iteration 192  RMSE(train): 0.692446  RMSE(test): 0.725847   RMSE(eval): 0.720282   LOSS(train): 0.700575  minutes: 0.024734\n",
      "Iteration 193  RMSE(train): 0.692297  RMSE(test): 0.725774   RMSE(eval): 0.720209   LOSS(train): 0.700462  minutes: 0.024727\n",
      "Iteration 194  RMSE(train): 0.692150  RMSE(test): 0.725702   RMSE(eval): 0.720137   LOSS(train): 0.700351  minutes: 0.025130\n",
      "Iteration 195  RMSE(train): 0.692003  RMSE(test): 0.725631   RMSE(eval): 0.720066   LOSS(train): 0.700240  minutes: 0.024592\n",
      "Iteration 196  RMSE(train): 0.691858  RMSE(test): 0.725561   RMSE(eval): 0.719996   LOSS(train): 0.700130  minutes: 0.023235\n",
      "Iteration 197  RMSE(train): 0.691714  RMSE(test): 0.725491   RMSE(eval): 0.719926   LOSS(train): 0.700022  minutes: 0.023601\n",
      "Iteration 198  RMSE(train): 0.691571  RMSE(test): 0.725422   RMSE(eval): 0.719857   LOSS(train): 0.699915  minutes: 0.024289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199  RMSE(train): 0.691429  RMSE(test): 0.725354   RMSE(eval): 0.719789   LOSS(train): 0.699809  minutes: 0.026132\n",
      "Iteration 200  RMSE(train): 0.691289  RMSE(test): 0.725286   RMSE(eval): 0.719722   LOSS(train): 0.699703  minutes: 0.021074\n",
      "Iteration 201  RMSE(train): 0.691149  RMSE(test): 0.725220   RMSE(eval): 0.719655   LOSS(train): 0.699599  minutes: 0.021203\n",
      "Iteration 202  RMSE(train): 0.691011  RMSE(test): 0.725154   RMSE(eval): 0.719589   LOSS(train): 0.699497  minutes: 0.020805\n",
      "Iteration 203  RMSE(train): 0.690874  RMSE(test): 0.725088   RMSE(eval): 0.719524   LOSS(train): 0.699394  minutes: 0.021460\n",
      "Iteration 204  RMSE(train): 0.690738  RMSE(test): 0.725024   RMSE(eval): 0.719459   LOSS(train): 0.699293  minutes: 0.020503\n",
      "Iteration 205  RMSE(train): 0.690603  RMSE(test): 0.724960   RMSE(eval): 0.719395   LOSS(train): 0.699193  minutes: 0.020526\n",
      "Iteration 206  RMSE(train): 0.690469  RMSE(test): 0.724896   RMSE(eval): 0.719331   LOSS(train): 0.699094  minutes: 0.022752\n",
      "Iteration 207  RMSE(train): 0.690336  RMSE(test): 0.724834   RMSE(eval): 0.719268   LOSS(train): 0.698995  minutes: 0.024297\n",
      "Iteration 208  RMSE(train): 0.690204  RMSE(test): 0.724772   RMSE(eval): 0.719206   LOSS(train): 0.698898  minutes: 0.022132\n",
      "Iteration 209  RMSE(train): 0.690073  RMSE(test): 0.724710   RMSE(eval): 0.719145   LOSS(train): 0.698801  minutes: 0.021108\n",
      "Iteration 210  RMSE(train): 0.689944  RMSE(test): 0.724650   RMSE(eval): 0.719084   LOSS(train): 0.698706  minutes: 0.024649\n",
      "Iteration 211  RMSE(train): 0.689814  RMSE(test): 0.724590   RMSE(eval): 0.719024   LOSS(train): 0.698611  minutes: 0.022129\n",
      "Iteration 212  RMSE(train): 0.689687  RMSE(test): 0.724530   RMSE(eval): 0.718964   LOSS(train): 0.698517  minutes: 0.020538\n",
      "Iteration 213  RMSE(train): 0.689560  RMSE(test): 0.724472   RMSE(eval): 0.718905   LOSS(train): 0.698425  minutes: 0.023917\n",
      "Iteration 214  RMSE(train): 0.689434  RMSE(test): 0.724413   RMSE(eval): 0.718847   LOSS(train): 0.698333  minutes: 0.023073\n",
      "Iteration 215  RMSE(train): 0.689310  RMSE(test): 0.724355   RMSE(eval): 0.718789   LOSS(train): 0.698242  minutes: 0.022573\n",
      "Iteration 216  RMSE(train): 0.689186  RMSE(test): 0.724299   RMSE(eval): 0.718731   LOSS(train): 0.698152  minutes: 0.025237\n",
      "Iteration 217  RMSE(train): 0.689063  RMSE(test): 0.724242   RMSE(eval): 0.718675   LOSS(train): 0.698062  minutes: 0.023391\n",
      "Iteration 218  RMSE(train): 0.688941  RMSE(test): 0.724186   RMSE(eval): 0.718618   LOSS(train): 0.697974  minutes: 0.022944\n",
      "Iteration 219  RMSE(train): 0.688820  RMSE(test): 0.724131   RMSE(eval): 0.718563   LOSS(train): 0.697886  minutes: 0.022960\n",
      "Iteration 220  RMSE(train): 0.688700  RMSE(test): 0.724076   RMSE(eval): 0.718508   LOSS(train): 0.697799  minutes: 0.024005\n",
      "Iteration 221  RMSE(train): 0.688581  RMSE(test): 0.724022   RMSE(eval): 0.718453   LOSS(train): 0.697713  minutes: 0.023025\n",
      "Iteration 222  RMSE(train): 0.688463  RMSE(test): 0.723968   RMSE(eval): 0.718399   LOSS(train): 0.697628  minutes: 0.021425\n",
      "Iteration 223  RMSE(train): 0.688346  RMSE(test): 0.723915   RMSE(eval): 0.718346   LOSS(train): 0.697543  minutes: 0.026445\n",
      "Iteration 224  RMSE(train): 0.688229  RMSE(test): 0.723862   RMSE(eval): 0.718293   LOSS(train): 0.697459  minutes: 0.021640\n",
      "Iteration 225  RMSE(train): 0.688114  RMSE(test): 0.723811   RMSE(eval): 0.718240   LOSS(train): 0.697377  minutes: 0.022281\n",
      "Iteration 226  RMSE(train): 0.687999  RMSE(test): 0.723759   RMSE(eval): 0.718189   LOSS(train): 0.697294  minutes: 0.020981\n",
      "Iteration 227  RMSE(train): 0.687885  RMSE(test): 0.723708   RMSE(eval): 0.718137   LOSS(train): 0.697213  minutes: 0.021375\n",
      "Iteration 228  RMSE(train): 0.687772  RMSE(test): 0.723657   RMSE(eval): 0.718086   LOSS(train): 0.697132  minutes: 0.023272\n",
      "Iteration 229  RMSE(train): 0.687660  RMSE(test): 0.723607   RMSE(eval): 0.718036   LOSS(train): 0.697053  minutes: 0.024228\n",
      "Iteration 230  RMSE(train): 0.687548  RMSE(test): 0.723558   RMSE(eval): 0.717986   LOSS(train): 0.696973  minutes: 0.024066\n",
      "Iteration 231  RMSE(train): 0.687438  RMSE(test): 0.723509   RMSE(eval): 0.717937   LOSS(train): 0.696895  minutes: 0.021222\n",
      "Iteration 232  RMSE(train): 0.687329  RMSE(test): 0.723460   RMSE(eval): 0.717888   LOSS(train): 0.696817  minutes: 0.020597\n",
      "Iteration 233  RMSE(train): 0.687220  RMSE(test): 0.723412   RMSE(eval): 0.717839   LOSS(train): 0.696740  minutes: 0.022046\n",
      "Iteration 234  RMSE(train): 0.687111  RMSE(test): 0.723365   RMSE(eval): 0.717791   LOSS(train): 0.696663  minutes: 0.021657\n",
      "Iteration 235  RMSE(train): 0.687005  RMSE(test): 0.723318   RMSE(eval): 0.717743   LOSS(train): 0.696588  minutes: 0.020525\n",
      "Iteration 236  RMSE(train): 0.686898  RMSE(test): 0.723271   RMSE(eval): 0.717696   LOSS(train): 0.696513  minutes: 0.020413\n",
      "Iteration 237  RMSE(train): 0.686793  RMSE(test): 0.723225   RMSE(eval): 0.717650   LOSS(train): 0.696439  minutes: 0.020551\n",
      "Iteration 238  RMSE(train): 0.686688  RMSE(test): 0.723179   RMSE(eval): 0.717604   LOSS(train): 0.696365  minutes: 0.020398\n",
      "Iteration 239  RMSE(train): 0.686584  RMSE(test): 0.723134   RMSE(eval): 0.717557   LOSS(train): 0.696293  minutes: 0.020638\n",
      "Iteration 240  RMSE(train): 0.686480  RMSE(test): 0.723089   RMSE(eval): 0.717512   LOSS(train): 0.696220  minutes: 0.020469\n",
      "Iteration 241  RMSE(train): 0.686378  RMSE(test): 0.723045   RMSE(eval): 0.717467   LOSS(train): 0.696148  minutes: 0.020283\n",
      "Iteration 242  RMSE(train): 0.686276  RMSE(test): 0.723001   RMSE(eval): 0.717423   LOSS(train): 0.696078  minutes: 0.020356\n",
      "Iteration 243  RMSE(train): 0.686175  RMSE(test): 0.722957   RMSE(eval): 0.717379   LOSS(train): 0.696007  minutes: 0.020975\n",
      "Iteration 244  RMSE(train): 0.686074  RMSE(test): 0.722914   RMSE(eval): 0.717335   LOSS(train): 0.695937  minutes: 0.020513\n",
      "Iteration 245  RMSE(train): 0.685975  RMSE(test): 0.722872   RMSE(eval): 0.717292   LOSS(train): 0.695868  minutes: 0.020195\n",
      "Iteration 246  RMSE(train): 0.685876  RMSE(test): 0.722830   RMSE(eval): 0.717249   LOSS(train): 0.695800  minutes: 0.020253\n",
      "Iteration 247  RMSE(train): 0.685778  RMSE(test): 0.722787   RMSE(eval): 0.717206   LOSS(train): 0.695732  minutes: 0.020220\n",
      "Iteration 248  RMSE(train): 0.685680  RMSE(test): 0.722746   RMSE(eval): 0.717164   LOSS(train): 0.695665  minutes: 0.020947\n",
      "Iteration 249  RMSE(train): 0.685583  RMSE(test): 0.722705   RMSE(eval): 0.717123   LOSS(train): 0.695598  minutes: 0.020486\n",
      "Iteration 250  RMSE(train): 0.685487  RMSE(test): 0.722664   RMSE(eval): 0.717081   LOSS(train): 0.695532  minutes: 0.020141\n",
      "Iteration 251  RMSE(train): 0.685391  RMSE(test): 0.722624   RMSE(eval): 0.717040   LOSS(train): 0.695466  minutes: 0.020287\n",
      "Iteration 252  RMSE(train): 0.685297  RMSE(test): 0.722584   RMSE(eval): 0.717000   LOSS(train): 0.695401  minutes: 0.023236\n",
      "Iteration 253  RMSE(train): 0.685203  RMSE(test): 0.722545   RMSE(eval): 0.716960   LOSS(train): 0.695337  minutes: 0.021045\n",
      "Iteration 254  RMSE(train): 0.685109  RMSE(test): 0.722505   RMSE(eval): 0.716920   LOSS(train): 0.695273  minutes: 0.020443\n",
      "Iteration 255  RMSE(train): 0.685016  RMSE(test): 0.722467   RMSE(eval): 0.716881   LOSS(train): 0.695210  minutes: 0.021953\n",
      "Iteration 256  RMSE(train): 0.684924  RMSE(test): 0.722428   RMSE(eval): 0.716841   LOSS(train): 0.695147  minutes: 0.020158\n",
      "Iteration 257  RMSE(train): 0.684833  RMSE(test): 0.722390   RMSE(eval): 0.716803   LOSS(train): 0.695085  minutes: 0.020462\n",
      "Iteration 258  RMSE(train): 0.684742  RMSE(test): 0.722353   RMSE(eval): 0.716764   LOSS(train): 0.695024  minutes: 0.020774\n",
      "Iteration 259  RMSE(train): 0.684652  RMSE(test): 0.722316   RMSE(eval): 0.716726   LOSS(train): 0.694963  minutes: 0.023212\n",
      "Iteration 260  RMSE(train): 0.684562  RMSE(test): 0.722279   RMSE(eval): 0.716689   LOSS(train): 0.694902  minutes: 0.023143\n",
      "Iteration 261  RMSE(train): 0.684473  RMSE(test): 0.722242   RMSE(eval): 0.716651   LOSS(train): 0.694842  minutes: 0.025970\n",
      "Iteration 262  RMSE(train): 0.684385  RMSE(test): 0.722206   RMSE(eval): 0.716614   LOSS(train): 0.694783  minutes: 0.023256\n",
      "Iteration 263  RMSE(train): 0.684298  RMSE(test): 0.722170   RMSE(eval): 0.716578   LOSS(train): 0.694724  minutes: 0.027211\n",
      "Iteration 264  RMSE(train): 0.684210  RMSE(test): 0.722134   RMSE(eval): 0.716541   LOSS(train): 0.694665  minutes: 0.020932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265  RMSE(train): 0.684123  RMSE(test): 0.722099   RMSE(eval): 0.716505   LOSS(train): 0.694607  minutes: 0.023101\n",
      "Iteration 266  RMSE(train): 0.684038  RMSE(test): 0.722064   RMSE(eval): 0.716470   LOSS(train): 0.694550  minutes: 0.023055\n",
      "Iteration 267  RMSE(train): 0.683953  RMSE(test): 0.722030   RMSE(eval): 0.716434   LOSS(train): 0.694493  minutes: 0.020278\n",
      "Iteration 268  RMSE(train): 0.683867  RMSE(test): 0.721995   RMSE(eval): 0.716399   LOSS(train): 0.694435  minutes: 0.022529\n",
      "Iteration 269  RMSE(train): 0.683783  RMSE(test): 0.721961   RMSE(eval): 0.716364   LOSS(train): 0.694380  minutes: 0.022336\n",
      "Iteration 270  RMSE(train): 0.683699  RMSE(test): 0.721928   RMSE(eval): 0.716330   LOSS(train): 0.694324  minutes: 0.020756\n",
      "Iteration 271  RMSE(train): 0.683617  RMSE(test): 0.721894   RMSE(eval): 0.716296   LOSS(train): 0.694270  minutes: 0.020651\n",
      "Iteration 272  RMSE(train): 0.683534  RMSE(test): 0.721861   RMSE(eval): 0.716262   LOSS(train): 0.694215  minutes: 0.020975\n",
      "Iteration 273  RMSE(train): 0.683452  RMSE(test): 0.721829   RMSE(eval): 0.716228   LOSS(train): 0.694161  minutes: 0.021551\n",
      "Iteration 274  RMSE(train): 0.683371  RMSE(test): 0.721797   RMSE(eval): 0.716195   LOSS(train): 0.694107  minutes: 0.021140\n",
      "Iteration 275  RMSE(train): 0.683290  RMSE(test): 0.721764   RMSE(eval): 0.716162   LOSS(train): 0.694054  minutes: 0.020771\n",
      "Iteration 276  RMSE(train): 0.683210  RMSE(test): 0.721732   RMSE(eval): 0.716130   LOSS(train): 0.694002  minutes: 0.021369\n",
      "Iteration 277  RMSE(train): 0.683130  RMSE(test): 0.721701   RMSE(eval): 0.716097   LOSS(train): 0.693949  minutes: 0.020929\n",
      "Iteration 278  RMSE(train): 0.683050  RMSE(test): 0.721670   RMSE(eval): 0.716065   LOSS(train): 0.693897  minutes: 0.020675\n",
      "Iteration 279  RMSE(train): 0.682972  RMSE(test): 0.721639   RMSE(eval): 0.716033   LOSS(train): 0.693846  minutes: 0.020431\n",
      "Iteration 280  RMSE(train): 0.682894  RMSE(test): 0.721608   RMSE(eval): 0.716002   LOSS(train): 0.693795  minutes: 0.021196\n",
      "Iteration 281  RMSE(train): 0.682816  RMSE(test): 0.721578   RMSE(eval): 0.715971   LOSS(train): 0.693744  minutes: 0.021408\n",
      "Iteration 282  RMSE(train): 0.682739  RMSE(test): 0.721548   RMSE(eval): 0.715940   LOSS(train): 0.693694  minutes: 0.021203\n",
      "Iteration 283  RMSE(train): 0.682662  RMSE(test): 0.721518   RMSE(eval): 0.715909   LOSS(train): 0.693644  minutes: 0.020933\n",
      "Iteration 284  RMSE(train): 0.682586  RMSE(test): 0.721489   RMSE(eval): 0.715878   LOSS(train): 0.693595  minutes: 0.020413\n",
      "Iteration 285  RMSE(train): 0.682510  RMSE(test): 0.721459   RMSE(eval): 0.715848   LOSS(train): 0.693546  minutes: 0.020818\n",
      "Iteration 286  RMSE(train): 0.682436  RMSE(test): 0.721430   RMSE(eval): 0.715818   LOSS(train): 0.693498  minutes: 0.021514\n"
     ]
    }
   ],
   "source": [
    "# %load bmf.py\n",
    "'''\n",
    "Created on Mar 3, 2017\n",
    "\n",
    "@author: v-lianji\n",
    "'''\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import data_reader \n",
    "import math\n",
    "from time import clock\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model(user_indices, item_indices, rank, ratings, user_cnt, item_cnt, lr, lamb, mu, init_value):\n",
    "\t\n",
    "\t\n",
    "\tW_user = tf.Variable(tf.truncated_normal([user_cnt, rank], stddev=init_value/math.sqrt(float(rank)), mean=0), name = 'user_embedding', dtype=tf.float32)\n",
    "\tW_item = tf.Variable(tf.truncated_normal([item_cnt, rank], stddev=init_value/math.sqrt(float(rank)), mean=0), name = 'item_embedding', dtype=tf.float32)\n",
    "\t\n",
    "\tW_user_bias = tf.concat([W_user, tf.ones((user_cnt,1), dtype=tf.float32)], 1, name='user_embedding_bias')\n",
    "\tW_item_bias = tf.concat([tf.ones((item_cnt,1), dtype=tf.float32), W_item], 1, name='item_embedding_bias')\n",
    "\t\n",
    "\tuser_feature = tf.nn.embedding_lookup(W_user_bias, user_indices, name = 'user_feature')\n",
    "\titem_feature = tf.nn.embedding_lookup(W_item_bias, item_indices, name = 'item_feature')\t\n",
    "\t\n",
    "\t\n",
    "\tpreds = tf.add(tf.reduce_sum( tf.multiply(user_feature , item_feature) , 1), mu)\n",
    "\t\n",
    "\tsquare_error = tf.sqrt(tf.reduce_mean( tf.squared_difference(preds, ratings)))\n",
    "\tloss = square_error + lamb*(tf.reduce_mean(tf.nn.l2_loss(W_user)) + tf.reduce_mean(tf.nn.l2_loss(W_item)))\n",
    "\t\t\n",
    "\ttf.summary.scalar('square_error', square_error)\n",
    "\ttf.summary.scalar('loss', loss)\n",
    "\tmerged_summary = tf.summary.merge_all()\n",
    "\t#tf.global_variables_initializer()\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)   # tf.train.AdadeltaOptimizer(learning_rate=lr).minimize(loss)    #\n",
    "\n",
    "\treturn train_step, square_error, loss, merged_summary\n",
    "\n",
    "def grid_search_params():\n",
    "\n",
    "\tdataset = data_reader.sparse_data_repos(10000,10005)\n",
    "\tdataset.load_trainging_ratings(r'data/userbook_unique_compactid_train.txt')\n",
    "\tdataset.load_test_ratings(r'data/userbook_unique_compactid_valid.txt')\n",
    "\tdataset.load_eval_ratings(r'data/userbook_unique_compactid_test.txt')\n",
    "\tlog_file = r'BMF_log.csv'\n",
    "\t\n",
    "\twt = open(log_file,'w')\n",
    "\trank = 16\n",
    "\tlambs=[0.00003,0.00005,0.0001]\n",
    "\tbatch_sizes=[500]\n",
    "\tn_eopch=2000\n",
    "\tlrs=[0.1]\n",
    "\tinit_values = [0.01]\n",
    "\t#mu=dataset.training_ratings_score.mean()\n",
    "\tmu = np.asarray(dataset.training_ratings_score, dtype=np.float32).mean() \n",
    "\twt.write('rank,lr,lamb,mu,n_eopch,batch_size,best_train_rmse,best_test_rmse,best_eval_rmse,best_epoch,init_value,minutes\\n')\n",
    "\tfor lamb in lambs:\n",
    "\t\tfor lr in lrs:\n",
    "\t\t\tfor init_value in init_values:\n",
    "\t\t\t\tfor batch_size in batch_sizes:\n",
    "\t\t\t\t\trun_with_parameter(dataset,rank,lr,lamb,mu,n_eopch,batch_size,wt, init_value)\n",
    "\twt.close()\n",
    "\n",
    "def run_with_parameter(dataset,rank,lr,lamb,mu,n_eopch,batch_size,wt, init_value):\n",
    "\tstart = clock()\n",
    "\ttf.reset_default_graph()\n",
    "\tbest_train_rmse, best_test_rmse, best_eval_rmse, best_eopch_idx = single_run(dataset,rank,dataset.n_user,dataset.n_item,lr,lamb,mu,n_eopch,batch_size,True, init_value)\n",
    "\tend = clock()\n",
    "\twt.write('%d,%f,%f,%f,%d,%d,%f,%f,%f,%d,%f,%f\\n' %(rank,lr,lamb,mu,n_eopch,batch_size,best_train_rmse, best_test_rmse, best_eval_rmse,best_eopch_idx,init_value,(end-start)/60))\n",
    "\twt.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def single_run(dataset,rank,user_cnt,item_cnt,lr,lamb,mu,n_eopch,batch_size,is_eval_on, init_value):\n",
    "\t\n",
    "\tuser_indices =  tf.placeholder(tf.int32,[None])\n",
    "\titem_indices =  tf.placeholder(tf.int32,[None])\n",
    "\tratings = tf.placeholder(tf.float32, [None])\t\n",
    "\n",
    "\n",
    "\ttrain_step, square_error, loss, merged_summary = build_model(user_indices, item_indices, rank, ratings, user_cnt, item_cnt, lr, lamb, mu, init_value)\n",
    "\t\n",
    "\tsess = tf.Session()\n",
    "\tinit = tf.global_variables_initializer()\n",
    "\tsess.run(init) \n",
    "\t\n",
    "\t#print(sess.run(user_embeddings))\n",
    "\t\n",
    "\ttrain_writer = tf.summary.FileWriter(r'logs', sess.graph)\n",
    "\t\n",
    "\tn_instances = len(dataset.training_ratings_user)\n",
    "\n",
    "\tbest_train_rmse, best_test_rmse, best_eval_rmse = -1, -1, -1\n",
    "\tbest_eopch_idx = -1 \n",
    "\tfor ite in range(n_eopch):\n",
    "\t\t#print(ite)\n",
    "\t\tstart = clock()\n",
    "\t\tfor i in range(n_instances//batch_size):\n",
    "\t\t\tstart_idx = i * batch_size \n",
    "\t\t\tend_idx = start_idx + batch_size\n",
    "\t\t\tcur_user_indices, cur_item_indices, cur_label = dataset.training_ratings_user[start_idx:end_idx], dataset.training_ratings_item[start_idx:end_idx],dataset.training_ratings_score[start_idx:end_idx]\n",
    "\t\t\t\n",
    "\t\t\tsess.run(train_step, { user_indices : cur_user_indices, item_indices : cur_item_indices, ratings : cur_label})\t\n",
    "\t\t\t\n",
    "\t\terror_traing = sess.run(square_error, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\n",
    "\t\terror_test = sess.run(square_error, { user_indices : dataset.test_ratings_user, item_indices : dataset.test_ratings_item, ratings : dataset.test_ratings_score})\n",
    "\t\tif is_eval_on:\n",
    "\t\t\terror_eval = sess.run(square_error, { user_indices : dataset.eval_ratings_user, item_indices : dataset.eval_ratings_item, ratings : dataset.eval_ratings_score})\n",
    "\t\telse: \n",
    "\t\t\terror_eval = -1\n",
    "\t\t\t\n",
    "\t\tif best_test_rmse<0 or best_test_rmse>error_test:\n",
    "\t\t\tbest_train_rmse, best_test_rmse, best_eval_rmse = error_traing,error_test, error_eval \n",
    "\t\t\tbest_eopch_idx = ite \n",
    "\t\telse:\n",
    "\t\t\tif ite - best_eopch_idx>10:\n",
    "\t\t\t\tbreak \n",
    "\t\t\t\n",
    "\t\tloss_traing = sess.run(loss, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\n",
    "\t\t#loss_test = sess.run(loss, { user_feature : test_user_feature, item_feature : test_item_feature, ratings : test_label})\n",
    "\t\tsummary = sess.run(merged_summary, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\n",
    "\t\ttrain_writer.add_summary(summary, ite)\n",
    "\t\tend = clock()\n",
    "\t\tprint(\"Iteration %d  RMSE(train): %f  RMSE(test): %f   RMSE(eval): %f   LOSS(train): %f  minutes: %f\" %(ite, error_traing, error_test, error_eval, loss_traing, (end-start)/60))\n",
    "\t\t\n",
    "\t\n",
    "\ttrain_writer.close()\n",
    "\t\n",
    "\treturn best_train_rmse, best_test_rmse, best_eval_rmse,best_eopch_idx\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\tgrid_search_params()\n",
    "\t#run()\n",
    "\tpass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Collaborative Deep Learning for Recommender Systems\n",
    "[paper下载地址](https://arxiv.org/pdf/1409.2944v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心思想\n",
    "推荐大致可以分为两类，基于内容推荐，基于用户行为的协同。该论文想做的是把内容和用户行为结合起来，并且使用了deep learning，希望解决content稀疏的问题。\n",
    "\n",
    "#### 具体实现\n",
    "解决content稀疏的问题，一般方法是使用stacked denoising autoencoder(SDAE)去学习item的distributed representation，输入是item content的词袋表示。\n",
    "这篇paper的做法是，结合SDAE，以及user vector，一起来拟合user-item矩阵，这个时候的loss就是sdae的loss + user-item 的loss，最小化这个loss就能完成相应的优化。\n",
    "\n",
    "![](https://img-blog.csdn.net/20161226001918229?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ2VuaXVzbHV6aA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "实际预估的过程也比较清晰，item content经过sdae得到item vector，然后点乘user vector得到结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Paper的PPT介绍](http://wanghao.in/slides/CDL_slides_long.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现见paper作者mxnet版本，以及用keras和tensorflow实现的版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Deep Neural Networks for YouTube Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:[Deep Neural Networks for YouTube Recommendations论文精读](https://zhuanlan.zhihu.com/p/25343518)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在推荐系统领域，特别是YouTube的所在视频推荐领域，主要面临三个挑战：\n",
    "> * 规模大：用户和视频的数量都很大，只能适应小规模数据集的算法就不考虑了。\n",
    "> * 更新快：youtube视频更新频率很高，每秒有小时级别的视频上传，需要在新发布视频和已有存量视频间进行balance。更新快（实时性）的另一方面的体现是用户实时行为切换很快，模型需要很好的追踪用户的实时行为。\n",
    "> * 噪音：噪音主要体现在用户的历史行为往往是稀疏的并且是不完整的，并且没有一个明确的ground truth的满意度signal，我们面对的都是noisy implicit feedback signals。噪音另一个方面就是视频本身很多数据都是非结构化的。这两点对算法的鲁棒性提出了很高的挑战。\n",
    "\n",
    ">之所以要在推荐系统中应用DNN解决问题，一个重要原因是google内部在机器学习问题上的通用solution的趋势正转移到Deep learning，系统实际部署在基于tensorflow的Google\n",
    "Brain上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总体结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ![](https://pic1.zhimg.com/v2-533f102bd97b2b8cdf25639cfb0ab3e9_r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 整个推荐系统分为candidate generation（召回）和Ranking（排序）两个阶段。召回阶段通过i2i/u2i/u2u/user profile等方式“粗糙”的召回候选商品，召回阶段视频的数量是百级别了；排序阶段对Matching后的视频采用更精细的特征计算user-item之间的排序分，作为最终输出推荐结果的依据。\n",
    "\n",
    "> 之所以把推荐系统划分成 召回 和 排序 两个阶段，主要是从性能方面考虑的。召回阶段面临的是百万级视频，单个视频的性能开销必须很小；而排序阶段的算法则非常消耗资源，不可能对所有视频都算一遍，实际上即便资源充足也完全没有必要，因为往往来说通不过召回粗选的视频，大部分在排序阶段排名也很低。接下来分别从召回和排序阶段展开介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回\n",
    "**问题建模**\n",
    "\n",
    "> 我们把推荐问题建模成一个“超大规模多分类”问题。即在时刻t，为用户U（上下文信息C）在视频库V中精准的预测出视频i的类别（每个具体的视频视为一个类别，i即为一个类别），用数学公式表达如下：\n",
    ">![](https://pic4.zhimg.com/v2-adde74b978b971e588c002038c390e0b_r.jpg)\n",
    "> 很显然上式为一个softmax多分类器的形式。向量$u\\in R^N$是<user, context>信息的高纬“embedding”，而向量$v_{j}\\in R^N$则是视频 j 的embedding向量。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习用户的embedding向量u。用公式表达DNN就是在拟合函数$u = f_{DNN}(user_info, context_info)$。\n",
    "而这种超大规模分类问题上，至少要有几百万个类别，实际训练采用的是Negative Sampe，类似于word2vec的Skip-Gram方法。\n",
    "\n",
    "**模型架构**\n",
    "> ![](https://pic1.zhimg.com/v2-7f97ddd40285e08b64546e3a54a5d64a_r.jpg)\n",
    "> 整个模型架构是包含三个隐层的DNN结构。输入是用户浏览历史、搜索历史、人口统计学信息和其余上下文信息concat成的输入向量；输出分线上和离线训练两个部分。\n",
    "> 离线训练阶段输出层为softmax层，输出上面公式表达的概率。而线上则直接利用user向量查询相关商品，最重要问题是在性能。我们利用ANN/近似最近邻算法（比如类似局部敏感哈希/Locality Sensitive Hashing、KD-Tree、K-means Tree）为用户提供最相关的N个视频。\n",
    "\n",
    "**主要特征**\n",
    "> 类似于word2vec的做法，每个视频都会被embedding到固定维度的向量中。用户的观看视频历史则是通过变长的视频序列表达，最终通过加权平均（可根据重要性和时间进行加权）得到固定维度的watch vector作为DNN的输入。\n",
    "> 除历史观看视频外的其他signal：\n",
    "\n",
    "> 其实熟悉Skip-Gram方法的同学很容易看出来，把推荐问题定义为“超大规模多分类”问题的数学公式和word2vec的Skip-Gram方法的公式基本相同，所不同的是user_vec是通过DNN学习到的，而引入DNN的好处则是任意的连续特征和离散特征可以很容易添加到模型当中。同样的，推荐系统常用的矩阵分解方法虽然也能得到user_vec和item_vec，但同样是不能嵌入更多feature。\n",
    "\n",
    "> **主要特征**：\n",
    "> * **历史搜索query**：把历史搜索的query分词后的token的embedding向量进行加权平均，能够反映用户的整体搜索历史状态\n",
    "> * **人口统计学信息**：性别、年龄、地域等\n",
    "> * **其他上下文信息**：设备、登录状态等\n",
    "\n",
    "> **“Example Age” （视频上传时间）特征**\n",
    "\n",
    "> 视频网络的时效性是很重要的，每秒YouTube上都有大量新视频被上传，而对用户来讲，哪怕牺牲相关性代价，用户还是更倾向于更新的视频。当然我们不会单纯的因为一个视频新就直接推荐给用户。\n",
    "\n",
    "> 因为机器学习系统在训练阶段都是利用过去的行为预估未来，因此通常对过去的行为有个隐式的bias。视频网站视频的分布是高度非静态（non-stationary）的，但我们的推荐系统产生的视频集合在视频的分布，基本上反映的是训练所取时间段的平均的观看喜好的视频。因此我们我们把样本的 “age” 作为一个feature加入模型训练中。从下图可以很清楚的看出，加入“example age” feature后和经验分布更为match。\n",
    "> ![](https://pic2.zhimg.com/v2-83f523875baab074b340b6ccea2eba02_r.jpg)\n",
    "\n",
    "**label and context selection**\n",
    "\n",
    "> 在有监督学习问题中，最重要的选择是label了，几个设计如下：\n",
    "\n",
    "> **使用更广的数据源**：不仅仅使用推荐场景的数据进行训练，其他场景比如搜索等的数据也要用到，这样也能为推荐场景提供一些explore。\n",
    "> **为每个用户生成固定数量训练样本**：我们在实际中发现的一个practical lessons，如果为每个用户固定样本数量上限，平等的对待每个用户，避免loss被少数active用户domanate，能明显提升线上效果。\n",
    "> **抛弃序列信息**：我们在实现时尝试的是去掉序列信息，对过去观看视频/历史搜索query的embedding向量进行加权平均。这点其实违反直觉，可能原因是模型对负反馈没有很好的建模。\n",
    "> **不对称的共同浏览（asymmetric co-watch）问题**：所谓asymmetric co-watch值的是用户在浏览视频时候，往往都是序列式的，开始看一些比较流行的，逐渐找到细分的视频。下图所示图(a)是hled-out方式，利用上下文信息预估中间的一个视频；图(b)是predicting next watch的方式，则是利用上文信息，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。_而实际上，传统的协同过滤类的算法，都是隐含的采用图(a)的held-out方式，忽略了不对称的浏览模式。_\n",
    "> ![](https://pic4.zhimg.com/v2-4c34494e753fa7ad6525f3533db31147_r.jpg)\n",
    "\n",
    "**不同网络深度和特征的实验**\n",
    "\n",
    "> 简单介绍下网络构建过程，采用的经典的“tower”模式搭建网络，基本同之前所示的网络架构，所有的视频和search token都embedded到256维的向量中，开始input层直接全连接到256维的softmax层，依次增加网络深度（+512-->+1024-->+2048--> ...）。\n",
    "> ![](https://pic3.zhimg.com/v2-1d5006249332a8c98b6731cb0f8c45a4_r.jpg)\n",
    "\n",
    "> 下图反映了不同网络深度（横坐标）下不同特征组合情况下的holdout-MAP（纵坐标）。可以很明显看出，增加了观看历史之外的特征很明显的提升了预测得准确率；从网络深度看，随着网络深度加大，预测准确率在提升，但继续增加第四层网络已经收益不大了。\n",
    "> ![](https://pic4.zhimg.com/v2-5b6b9563aa2e8f21518fc5cfad04c1d0_r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 排序\n",
    "> Ranking阶段的最重要任务就是精准的预估用户对视频的喜好程度。不同于召回阶段面临的是百万级的候选视频集，Ranking阶段面对的只是百级别的商品集，因此我们可以使用更多更精细的feature来刻画视频（item）以及用户与视频（user-item）的关系。比如用户可能很喜欢某个视频，但如果list页的用的“缩略图”选择不当，用户也许不会点击，等等。\n",
    "\n",
    "> 此外，召回阶段的来源往往很多，没法直接比较。Ranking阶段另一个关键的作用是能够把不同来源的数据进行有效的ensemble。\n",
    "\n",
    "> 在目标的设定方面，单纯CTR指标是有迷惑性的，有些靠关键词吸引用户高点击的视频未必能够被播放。因此设定的目标基本与期望的观看时长相关，具体的目标调整则根据线上的A/B进行调整。\n",
    "\n",
    "**模型架构**\n",
    "\n",
    "> Ranking阶段的模型和Matching基本相似，不同的是training最后一层是一个weighted LR层，而serving阶段激励函数用的是$e^{x}$。\n",
    "> ![](https://pic4.zhimg.com/v2-33f93002d2d7f42f50e617e03ef659bd_r.jpg)\n",
    "\n",
    "**特征表达**\n",
    "> **a). Feature Engineering**：\n",
    "\n",
    "> 尽管深度学习在图像、语音和NLP等场景都能实现end-to-end的训练，没有了人工特征工程工作。然而在搜索和推荐场景，我们的很难吧原始数据直接作为FNN的输入，特征工程仍然很重要。而特征工程中最难的是如何建模用户时序行为（temporal sequence of user actions），并且关联这些行为和要rank的item。\n",
    "\n",
    "> 我们发现最重要的Signal是描述用户与商品本身或相似商品之间交互的Signal，这与Facebook在14年提出LR+GBDT模型的paper（Practical Lessons from Predicting Clicks on Ads at Facebook）中得到的结论是一致的。比如我们要度量用户对视频的喜欢，可以考虑用户与视频所在频道间的关系：\n",
    "\n",
    "> * 数量特征：浏览该频道的次数？\n",
    "> * 时间特征：比如最近一次浏览该频道距离现在的时间？\n",
    "\n",
    "> 这两个连续特征的最大好处是具备非常强的泛化能力。另外除了这两个偏正向的特征，用户对于视频所在频道的一些PV但不点击的行为，即负反馈Signal同样非常重要。\n",
    "\n",
    "\n",
    "> 另外，我们还发现，把召回阶段的信息传播到排序阶段同样能很好的提升效果，比如推荐来源和所在来源的分数。\n",
    "\n",
    "> **b). Embedding Categorical Features**\n",
    "\n",
    "> NN更适合处理连续特征，因此稀疏的特别是高基数空间的离散特征需要embedding到稠密的向量中。每个维度（比如query/user_id）都有独立的embedding空间，一般来说空间的维度基本与log(去重后值得数量)相当。实际并非为所有的id进行embedding，比如视频id，只需要按照点击排序，选择top N视频进行embedding，其余置为0向量。而对于像“过去点击的视频”这种multivalent特征，与召回阶段的处理相同，进行加权平均即可。\n",
    "\n",
    "> 另外一个值得注意的是，同维度不同feature采用的相同ID的embedding是共享的（比如“过去浏览的视频id” “seed视频id”），这样可以大大加速训练，但显然输入层仍要分别填充。\n",
    "\n",
    "\n",
    "> **c). Normalizing Continuous Features**\n",
    "\n",
    "> 众所周知，NN对输入特征的尺度和分布都是非常敏感的，实际上基本上除了Tree-Based的模型（比如GBDT/RF），机器学习的大多算法都如此。我们发现归一化方法对收敛很关键，推荐一种排序分位归一到[0,1]区间的方法，即$\\bar{x}=\\int_{-\\infty }^{x}df$，累计分位点。\n",
    "\n",
    "> 除此之外，我们还把归一化后的$\\bar{x}$的根号$\\sqrt{x}$和平方$x^{2}$作为网络输入，以期能使网络能够更容易得到特征的次线性（sub-linear）和（super-linear）超线性函数。\n",
    "\n",
    "\n",
    "**建模期望观看时长**\n",
    "\n",
    "> 我们的目标是预测期望观看时长。有点击的为正样本，有PV无点击的为负样本，正样本需要根据观看时长进行加权。因此，我们训练阶段网络最后一层用的是 weighted logistic regression。\n",
    "\n",
    "> 正样本的权重为观看时长 T_{i}，负样本权重为1。这样的话，LR学到的odds为：\n",
    "> ![](https://pic1.zhimg.com/v2-4b76c3502d11f87785fb32712ec4b623_r.jpg)\n",
    "\n",
    "> 其中N是总的样本数量，k是正样本数量，$T_{i}$是第i正样本的观看时长。一般来说，k相对N比较小，因此上式的odds可以转换成$E[T]/(1+P)$，其中P是点击率，点击率一般很小，这样odds接近于$E[T]$，即期望观看时长。因此在线上serving的inference阶段，我们采用$e^{x}$作为激励函数，就是近似的估计期望的观看时长。\n",
    "\n",
    "**不同隐层的实验**\n",
    "\n",
    "> 下图的table1是离线利用hold-out一天数据在不同NN网络结构下的结果。如果用户对模型预估高分的反而没有观看，我们认为是预测错误的观看时长。weighted, per-user loss就是预测错误观看时长占总观看时长的比例。\n",
    "\n",
    "> 我们对网络结构中隐层的宽度和深度方面都做了测试，从下图结果看增加隐层网络宽度和深度都能提升模型效果。而对于1024-->512-->256这个网络，测试的不包含归一化后根号和方式的版本，loss增加了0.2%。而如果把weighted LR替换成LR，效果下降达到4.1%。\n",
    "> ![](https://pic4.zhimg.com/v2-05b935a5fba84dac4f575dd679ddd66a_r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现参考paddlepaddle和tensorflow的实现方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
