{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐系统介绍\n",
    "\n",
    "### 内容与代码整理by[@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "\n",
    "在以下的部分我们将用到[deep autoencoder](https://arxiv.org/abs/1708.01715)去构建一个[Netflix dataset](https://netflixprize.com/)数据集上的推荐系统。\n",
    "\n",
    "这里用到的工具库是facebook的[PyTorch](http://pytorch.org/)，代码部分来源于NVIDIA的[this repo](https://github.com/NVIDIA/DeepRecommender)。.\n",
    "\n",
    "## 推荐系统简介\n",
    "\n",
    "[推荐系统](https://en.wikipedia.org/wiki/Recommender_system)用于在信息过载的互联网时代，帮助更多用户缩短决策路径，找到自己感兴趣的“内容”。\n",
    "\n",
    "一般情况下，我们有几种构建推荐系统的思路：\n",
    "* 协同过滤\n",
    "* 基于内容的推荐\n",
    "* 混合推荐\n",
    "\n",
    "协同过滤与基于内容的推荐，我们就不赘述了，大家在基础课程中可以找到相应的讲解，这里主要集中在用深度学习中的autoencoder去构建协同过滤，用到的是NetFlix的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import requests\n",
    "from utils import get_gpu_name, get_number_processors, get_gpu_memory, get_cuda_version\n",
    "from parameters import *\n",
    "from load_test import run_load_test\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Number of CPU processors: \", get_number_processors())\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(\"GPU memory: \", get_gpu_memory())\n",
    "print(\"CUDA: \", get_cuda_version())\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集: Netflix\n",
    "\n",
    "这个数据集是著名的[Netflix Prize](http://www.netflixprize.com)比赛使用的数据集。包含1.7w+电影上的1亿多的打分，数据集采集于1998年10月到2005年12月之间，每部电影的评分是1-5分\n",
    "\n",
    "数据集可以在[这里](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a). 下载，你可以通过以下命令去解压缩数据:\n",
    "\n",
    "```bash\n",
    "tar -xvf nf_prize_dataset.tar.gz\n",
    "tar -xf download/training_set.tar\n",
    "```\n",
    "\n",
    "在我们下载的文件中，有2个非常重要的文件:\n",
    "\n",
    "1) `training_set.tar`文件是一个包含17770文件的文件夹，一部电影一个文件，第一行是movie_id加一个冒号，后面每一行都是如下形式:\n",
    "\n",
    "`CustomerID, Rating, Date`\n",
    "- MovieIDs取值为1到17770\n",
    "- CustomerIDs取值为1到2649429，但是不连续，有480189名用户\n",
    "- Ratings取值从1到5\n",
    "- Dates日期格式为YYYY-MM-DD.\n",
    "\n",
    "2) 电影的信息文件 [`movie_titles.txt`](data/movie_titles.txt)是如下的格式:\n",
    "\n",
    "`MovieID, YearOfRelease, Title`\n",
    "\n",
    "- MovieID是电影ID，但是和Netflix或者IMDB电影id并不是对应的\n",
    "- YearOfRelease取值从1890到2005\n",
    "- Title是Netflix电影名字\n",
    "\n",
    "### 数据准备\n",
    "\n",
    "第一步是把数据准备成autoencoder能读取的形式，这一步要花费一些实际(1-2个小时)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%%time\n",
    "%run ./data_utils/netflix_data_convert.py $NF_PRIZE_DATASET $NF_DATA"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个脚本把数据集切分成训练集、验证集、测试集，构建的文件有3列: `CustomerID,MovieID,Rating`。整个数据集按照时间维度切分为4个文件: Netflix 3months, Netflix 6 months, Netflix 1 year 和 Netflix full。下面是这些数据文件的一些详细信息:\n",
    "\n",
    "| Dataset  | Netflix 3 months | Netflix 6 months | Netflix 1 year | Netflix full |\n",
    "| -------- | ---------------- | ---------------- | ----------- |  ------------ |\n",
    "| Ratings train | 13,675,402 | 29,179,009 | 41,451,832 | 98,074,901 |\n",
    "| Users train | 311,315 |390,795  | 345,855 | 477,412 |\n",
    "| Items train | 17,736 |17,757  | 16,907 | 17,768 |\n",
    "| Time range train | 2005-09-01 to 2005-11-31 | 2005-06-01 to 2005-11-31 | 2004-06-01 to 2005-05-31 | 1999-12-01 to 2005-11-31\n",
    "|  |  |  |   | |\n",
    "| Ratings test | 2,082,559 | 2,175,535  | 3,888,684| 2,250,481 |\n",
    "| Users test | 160,906 | 169,541  | 197,951| 173,482 |\n",
    "| Items test | 17,261 | 17,290  | 16,506| 17,305 |\n",
    "| Time range test | 2005-12-01 to 2005-12-31 | 2005-12-01 to 2005-12-31 | 2005-06-01 to 2005-06-31 | 2005-12-01 to 2005-12-31\n",
    "\n",
    "我们看一眼其中的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "nf_3m_valid = os.path.join(NF_DATA, 'N3M_VALID', 'n3m.valid.txt')\n",
    "df = pd.read_csv(nf_3m_valid, names=['CustomerID','MovieID','Rating'], sep='\\t')\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "source": [
    "nf_3m_test = os.path.join(NF_DATA, 'N3M_TEST', 'n3m.test.txt')\n",
    "df2 = pd.read_csv(nf_3m_test, names=['CustomerID','MovieID','Rating'], sep='\\t')\n",
    "print(df2.shape)\n",
    "df2.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoder for Collaborative Filtering\n",
    "\n",
    "数据已经有了，我们来聊聊模型。这里用到的[模型](https://arxiv.org/abs/1708.01715)由NVIDIA的小伙伴提出来，是一个6层的深度autoencoder，这里用的激活函数是SELU (scaled exponential linear units)，同时为了提高泛化能力添加了dropout。\n",
    "\n",
    "一个autoencoder其实就是一个完成了下面2种变换的神经网络: $encode(x): R^n \\Rightarrow R^d$ 和 $decoder(z): R^d \\Rightarrow R^n$。autoencoder的最终目标是获得原始数据的一个$d$维表示，同时希望这时候的autoencoder能最小化$x$ 和 $f(x) = decode(encode(x))$之间的差异。下图显示了[论文](https://arxiv.org/abs/1708.01715)里提出的autoencoder结构。Encoder部分有2层$e_1$ 和 $e_2$，decoder 也有2层$d_1$ 和 $d_2$。可以在编码层$z$使用dropout，其实在论文中，实验了不同的层数，从2到12\n",
    "\n",
    ">![](http://nbviewer.jupyter.org/github/miguelgfierro/sciblog_support/blob/master/Intro_to_Recommendation_Systems/data/AutoEncoder.png)\n",
    "\n",
    "在forward pass(前向运算)阶段模型通过他在训练集中的打分$x \\in R^n$获得用户的表示(向量)，其中$n$是items的个数。需要特别注意的是$x$是非常稀疏的，但是在输出侧$y=f(x) \\in R^n$是一个稠密向量，包含了对所有item的打分，这里用到的损失函数是均方误差(RMSE).\n",
    "\n",
    "核心的思想是希望能通过反向传播训练网络最小化输入和输出的误差，从而完成对未知的电影进行预估。在paper里尝试了不同的[激活函数](https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py)，发现在这个任务上ELU, SELU 和 LRELU(注意到这些激活函数在负的那一侧都不是0)，比SIGMOID, RELU, RELU6, 和 TANH效果要好。\n",
    "\n",
    "下面就训练吧，训练的参数可以在[parameters.py](parameters.py)中取到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "%run ./DeepRecommender/run.py --gpu_ids $GPUS \\\n",
    "    --path_to_train_data $TRAIN \\\n",
    "    --path_to_eval_data $EVAL \\\n",
    "    --hidden_layers $HIDDEN \\\n",
    "    --non_linearity_type $ACTIVATION \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --logdir $MODEL_OUTPUT_DIR \\\n",
    "    --drop_prob $DROPOUT \\\n",
    "    --optimizer $OPTIMIZER \\\n",
    "    --lr $LR \\\n",
    "    --weight_decay $WD \\\n",
    "    --aug_step $AUG_STEP \\\n",
    "    --num_epochs $EPOCHS "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "接下来我们要在测试集上进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "%run ./DeepRecommender/infer.py \\\n",
    "--path_to_train_data $TRAIN \\\n",
    "--path_to_eval_data $TEST \\\n",
    "--hidden_layers $HIDDEN \\\n",
    "--non_linearity_type $ACTIVATION \\\n",
    "--save_path  $MODEL_PATH \\\n",
    "--drop_prob $DROPOUT \\\n",
    "--predictions_path $INFER_OUTPUT"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "%run ./DeepRecommender/compute_RMSE.py --path_to_predictions=$INFER_OUTPUT"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "接下来我们来构建一个推荐系统的API，第一步是把用户信息取过来转成一个json的请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "source": [
    "titles = pd.read_csv(MOVIE_TITLES, names=['MovieID','Year','Title'], encoding = \"latin\")\n",
    "titles.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "source": [
    "target = df2[df2['CustomerID'] == 0]\n",
    "target"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "source": [
    "df_customer = pd.merge(target, titles, on='MovieID', how='left', suffixes=('_',''))\n",
    "df_customer.drop(['Title_','Year'], axis=1, inplace=True)\n",
    "df_customer"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户的json信息如下，比如这里是对于`CustomerID`为0的用户，有包含`MovieID`和打分的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "source": [
    "df_query = df_customer.drop(['CustomerID','Title'], axis=1).set_index('MovieID')\n",
    "dict_query = df_query.to_dict()['Rating']\n",
    "dict_query"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API定义在[api.py](api.py)文件中，当服务启动的时候，会加载训练好的模型到内存中。main函数 `/recommend`以json形式接收`dict_query`，通过autoencoder完成计算，返回另外一个json(预测打分)\n",
    "\n",
    "可以在命令行通过下面的方式去启动服务:\n",
    "```bash\n",
    "python api.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "end_point = 'http://127.0.0.1:5000/'\n",
    "end_point_recommend = \"http://127.0.0.1:5000/recommend\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "source": [
    "!curl $end_point"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过`dict_query`来向推荐系统发起请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "source": [
    "headers = {'Content-type':'application/json'}\n",
    "res = requests.post(end_point_recommend, data=json.dumps(dict_query), headers=headers)\n",
    "print(res.ok)\n",
    "print(json.dumps(res.json(), indent=2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 压测\n",
    "如果我们想看一看我们推荐系统的稳定性，可以做一个压测，频繁发起一些请求。大家可以在[load_test.py](load_test.py)中找到压测的代码，我们可以通过调整`NUM`和`CONCURRENT`来控制轮次和并发数。这里测试的服务器`NUM`设定为10，并发为2，最后测试结果大概是4ms左右的响应时间，实际上并发为2的相应速度基本都差不多，如果把并发数提到20的话，响应时间就会上升为12ms\n",
    "\n",
    "在实际应用中，请求并不从同一台电脑发出，所以大家还得加上客户端和服务器之间的通信延迟等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NUM = 10\n",
    "CONCURRENT = 2\n",
    "VERBOSE = True\n",
    "payload = {13:5.0, 191:5.0, 209:5.0}\n",
    "payload_list = [payload]*NUM"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Run:\n",
    "with aiohttp.ClientSession() as session:  # We create a persistent connection\n",
    "    loop = asyncio.get_event_loop()\n",
    "    calc_routes = loop.run_until_complete(run_load_test(end_point_recommend, payload_list, session, CONCURRENT, VERBOSE))\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
