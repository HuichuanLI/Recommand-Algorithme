{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark推荐系统\n",
    "by [@寒小阳](http://blog.csdn.net/han_xiaoyang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark自带了用于推荐的算法，我们来看看这个算法如何完成整个环节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# 基于spark中ALS的推荐系统，针对movielens中电影打分数据做推荐\n",
    "# Edit：寒小阳(hanxiaoyang.ml@gmail.com)\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "def parseRating(line):\n",
    "    \"\"\"\n",
    "        MovieLens的打分格式是userId::movieId::rating::timestamp\n",
    "        我们对格式做一个解析\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))\n",
    "\n",
    "def parseMovie(line):\n",
    "    \"\"\"\n",
    "        对应的电影文件的格式为movieId::movieTitle\n",
    "        解析成int id, 文本\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return int(fields[0]), fields[1]\n",
    "\n",
    "def loadRatings(ratingsFile):\n",
    "    \"\"\"\n",
    "        载入得分\n",
    "    \"\"\"\n",
    "    if not isfile(ratingsFile):\n",
    "        print \"File %s does not exist.\" % ratingsFile\n",
    "        sys.exit(1)\n",
    "    f = open(ratingsFile, 'r')\n",
    "    ratings = filter(lambda r: r[2] > 0, [parseRating(line)[1] for line in f])\n",
    "    f.close()\n",
    "    if not ratings:\n",
    "        print \"No ratings provided.\"\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return ratings\n",
    "\n",
    "def computeRmse(model, data, n):\n",
    "    \"\"\"\n",
    "        评估的时候要用的，计算均方根误差\n",
    "    \"\"\"\n",
    "    predictions = model.predictAll(data.map(lambda x: (x[0], x[1])))\n",
    "    predictionsAndRatings = predictions.map(lambda x: ((x[0], x[1]), x[2])) \\\n",
    "      .join(data.map(lambda x: ((x[0], x[1]), x[2]))) \\\n",
    "      .values()\n",
    "    return sqrt(predictionsAndRatings.map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / float(n))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if (len(sys.argv) != 3):\n",
    "        print \"Usage: /path/to/spark/bin/spark-submit --driver-memory 2g \" + \\\n",
    "          \"MovieLensALS.py movieLensDataDir personalRatingsFile\"\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 设定环境\n",
    "    conf = SparkConf() \\\n",
    "      .setAppName(\"MovieLensALS\") \\\n",
    "      .set(\"spark.executor.memory\", \"2g\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    # 载入打分数据\n",
    "    myRatings = loadRatings(sys.argv[2])\n",
    "    myRatingsRDD = sc.parallelize(myRatings, 1)\n",
    "\n",
    "    movieLensHomeDir = sys.argv[1]\n",
    "\n",
    "    # 得到的ratings为(时间戳最后一位整数, (userId, movieId, rating))格式的RDD\n",
    "    ratings = sc.textFile(join(movieLensHomeDir, \"ratings.dat\")).map(parseRating)\n",
    "\n",
    "    # 得到的movies为(movieId, movieTitle)格式的RDD\n",
    "    movies = dict(sc.textFile(join(movieLensHomeDir, \"movies.dat\")).map(parseMovie).collect())\n",
    "\n",
    "    numRatings = ratings.count()\n",
    "    numUsers = ratings.values().map(lambda r: r[0]).distinct().count()\n",
    "    numMovies = ratings.values().map(lambda r: r[1]).distinct().count()\n",
    "\n",
    "    print \"Got %d ratings from %d users on %d movies.\" % (numRatings, numUsers, numMovies)\n",
    "\n",
    "    # 根据时间戳最后一位把整个数据集分成训练集(60%), 交叉验证集(20%), 和评估集(20%)\n",
    "\n",
    "    # 训练, 交叉验证, 测试 集都是(userId, movieId, rating)格式的RDD\n",
    "\n",
    "    numPartitions = 4\n",
    "    training = ratings.filter(lambda x: x[0] < 6) \\\n",
    "      .values() \\\n",
    "      .union(myRatingsRDD) \\\n",
    "      .repartition(numPartitions) \\\n",
    "      .cache()\n",
    "\n",
    "    validation = ratings.filter(lambda x: x[0] >= 6 and x[0] < 8) \\\n",
    "      .values() \\\n",
    "      .repartition(numPartitions) \\\n",
    "      .cache()\n",
    "\n",
    "    test = ratings.filter(lambda x: x[0] >= 8).values().cache()\n",
    "\n",
    "    numTraining = training.count()\n",
    "    numValidation = validation.count()\n",
    "    numTest = test.count()\n",
    "\n",
    "    print \"Training: %d, validation: %d, test: %d\" % (numTraining, numValidation, numTest)\n",
    "\n",
    "    # 训练模型，在交叉验证集上看效果\n",
    "\n",
    "    ranks = [8, 12]\n",
    "    lambdas = [0.1, 10.0]\n",
    "    numIters = [10, 20]\n",
    "    bestModel = None\n",
    "    bestValidationRmse = float(\"inf\")\n",
    "    bestRank = 0\n",
    "    bestLambda = -1.0\n",
    "    bestNumIter = -1\n",
    "\n",
    "    for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n",
    "        model = ALS.train(training, rank, numIter, lmbda)\n",
    "        validationRmse = computeRmse(model, validation, numValidation)\n",
    "        print \"RMSE (validation) = %f for the model trained with \" % validationRmse + \\\n",
    "              \"rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter)\n",
    "        if (validationRmse < bestValidationRmse):\n",
    "            bestModel = model\n",
    "            bestValidationRmse = validationRmse\n",
    "            bestRank = rank\n",
    "            bestLambda = lmbda\n",
    "            bestNumIter = numIter\n",
    "\n",
    "    testRmse = computeRmse(bestModel, test, numTest)\n",
    "\n",
    "    # 在测试集上评估 交叉验证集上最好的模型\n",
    "    print \"The best model was trained with rank = %d and lambda = %.1f, \" % (bestRank, bestLambda) \\\n",
    "      + \"and numIter = %d, and its RMSE on the test set is %f.\" % (bestNumIter, testRmse)\n",
    "\n",
    "    # 我们把基线模型设定为每次都返回平均得分的模型\n",
    "    meanRating = training.union(validation).map(lambda x: x[2]).mean()\n",
    "    baselineRmse = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / numTest)\n",
    "    improvement = (baselineRmse - testRmse) / baselineRmse * 100\n",
    "    print \"The best model improves the baseline by %.2f\" % (improvement) + \"%.\"\n",
    "\n",
    "    # 个性化的推荐(针对某个用户)\n",
    "\n",
    "    myRatedMovieIds = set([x[1] for x in myRatings])\n",
    "    candidates = sc.parallelize([m for m in movies if m not in myRatedMovieIds])\n",
    "    predictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect()\n",
    "    recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50]\n",
    "\n",
    "    print \"Movies recommended for you:\"\n",
    "    for i in xrange(len(recommendations)):\n",
    "        print (\"%2d: %s\" % (i + 1, movies[recommendations[i][1]])).encode('ascii', 'ignore')\n",
    "\n",
    "    # clean up\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
