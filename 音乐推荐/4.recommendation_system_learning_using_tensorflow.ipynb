{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf.jpeg)\n",
    "真实的场景中，可能我们有非常非常多的训练数据，我们不得不面对一些问题，也是大家比较关心的问题。\n",
    "\n",
    "1）海量的数据无法一次载入内存用于训练。<br>\n",
    "2）数据是每天不断增加的，我们有没有一些增量训练的方式去不断持续迭代更新模型？\n",
    "\n",
    "什么场景下，我们是不把数据全部载入内存优化，而是一个batch一个batch输入进行update参数的？<br>\n",
    "对，我们用tensorflow来完成一个在批量数据上更新，并且可以增量迭代优化的矩阵分解推荐系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.矩阵分解回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](svd_recommendation.png)\n",
    "LFM：把用户再item上打分的行为，看作是有内部依据的，认为和k个factor有关系<br>\n",
    "每一个user i会有一个用户的向量(k维)，每一个item会有一个item的向量(k维)\n",
    "\n",
    "SVD是矩阵分解的一种方式\n",
    "\n",
    "### 预测公式如下\n",
    "$y_{pred[u, i]} = bias_{global} + bias_{user[u]} + bias_{item_[i]} + <embedding_{user[u]}, embedding_{item[i]}>$\n",
    "\n",
    "### 我们需要最小化的loss计算如下（添加正则化项）\n",
    "$\\sum_{u, i} |y_{pred[u, i]} - y_{true[u, i]}|^2 + \\lambda(|embedding_{user[u]}|^2 + |embedding_{item[i]}|^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.获取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们依旧以movielens为例，数据格式为**user item rating timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#这部分代码大家不用跑，因为数据已经下载好了\n",
    "#!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "#!sudo unzip ml-1m.zip -d ./movielens"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据处理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们写点代码完成数据的产出和预处理过程。<br>\n",
    "大家知道tensorflow搭建的模型，训练方式通常是一个batch一个batch训练的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_data_and_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "class ShuffleDataIterator(object):\n",
    "    \"\"\"\n",
    "    随机生成一个batch一个batch数据\n",
    "    \"\"\"\n",
    "    #初始化\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    #总样本量\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    #取出下一个batch\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    #随机生成batch_size个下标，取出对应的样本\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochDataIterator(ShuffleDataIterator):\n",
    "    \"\"\"\n",
    "    顺序产出一个epoch的数据，在测试中可能会用到\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochDataIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们按照下图的方式用tensorflow去搭建一个可增量训练的矩阵分解模型，完成基于矩阵分解的推荐系统。\n",
    "![](tf_svd_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 使用矩阵分解搭建的网络结构\n",
    "def inference_svd(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    #使用CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # 初始化几个bias项\n",
    "        global_bias = tf.get_variable(\"global_bias\", shape=[])\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # bias向量\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # user向量与item向量\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    with tf.device(device):\n",
    "        # 按照实际公式进行计算\n",
    "        # 先对user向量和item向量求内积\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        # 加上几个偏置项\n",
    "        infer = tf.add(infer, global_bias)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # 加上正则化项\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), name=\"svd_regularizer\")\n",
    "    return infer, regularizer\n",
    "\n",
    "# 迭代优化部分\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.1, device=\"/cpu:0\"):\n",
    "    global_step = tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "    # 选择合适的optimizer做优化\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.数据上的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "\n",
    "np.random.seed(13575)\n",
    "\n",
    "# 一批数据的大小\n",
    "BATCH_SIZE = 2000\n",
    "# 用户数\n",
    "USER_NUM = 6040\n",
    "# 电影数\n",
    "ITEM_NUM = 3952\n",
    "# factor维度\n",
    "DIM = 15\n",
    "# 最大迭代轮数\n",
    "EPOCH_MAX = 200\n",
    "# 使用cpu做训练\n",
    "DEVICE = \"/cpu:0\"\n",
    "\n",
    "# 截断\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)\n",
    "\n",
    "# 这个是方便Tensorboard可视化做的summary\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=val)])\n",
    "\n",
    "# 调用上面的函数获取数据\n",
    "def get_data():\n",
    "    df = read_data_and_process(\"./movielens/ml-1m/ratings.dat\", sep=\"::\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * 0.9)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    return df_train, df_test\n",
    "\n",
    "# 实际训练过程\n",
    "def svd(train, test):\n",
    "    samples_per_batch = len(train) // BATCH_SIZE\n",
    "\n",
    "    # 一批一批数据用于训练\n",
    "    iter_train = ShuffleDataIterator([train[\"user\"],\n",
    "                                         train[\"item\"],\n",
    "                                         train[\"rate\"]],\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "    # 测试数据\n",
    "    iter_test = OneEpochDataIterator([test[\"user\"],\n",
    "                                         test[\"item\"],\n",
    "                                         test[\"rate\"]],\n",
    "                                        batch_size=-1)\n",
    "    # user和item batch\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    # 构建graph和训练\n",
    "    infer, regularizer = inference_svd(user_batch, item_batch, user_num=USER_NUM, item_num=ITEM_NUM, dim=DIM,\n",
    "                                           device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.05, device=DEVICE)\n",
    "\n",
    "    # 初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 开始迭代\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"/tmp/svd/log\", graph=sess.graph)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   item_batch: items,\n",
    "                                                                   rate_batch: rates})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items})\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print(\"{:3d} {:f} {:f} {:f}(s)\".format(i // samples_per_batch, train_err, test_err,\n",
    "                                                       end - start))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\", train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\", test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# 获取数据\n",
    "df_train, df_test = get_data()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# 完成实际的训练\n",
    "svd(df_train, df_test)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
