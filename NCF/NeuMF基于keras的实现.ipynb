{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T08:49:43.404337Z",
     "start_time": "2020-08-30T08:49:42.772280Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        ratingList = list(zip(df.userid.tolist(), df.itemid.tolist()))\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        negativeList = df.iloc[:, 1:].values.tolist()\n",
    "        return negativeList\n",
    "\n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        num_users = df.userid.max()\n",
    "        num_items = df.itemid.max()\n",
    "        mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "        interactions = df[['userid', 'itemid']].values.tolist()\n",
    "        # [(0, 2969), (0, 1178), (0, 1574), (0, 957)]\n",
    "        for user, item in interactions:\n",
    "            mat[user, item] = 1.\n",
    "        # [((0, 2969), 1.0), ((0, 1178), 1.0), ((0, 1574), 1.0), ((0, 957), 1.0)]\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:18:18.859884Z",
     "start_time": "2020-08-30T03:18:18.843522Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, topK):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _topK\n",
    "\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _topK = topK\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)],\n",
    "                                 batch_size=100, verbose=0)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    ranklist = heapq.nlargest(_topK, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:18:21.495911Z",
     "start_time": "2020-08-30T03:18:21.480402Z"
    }
   },
   "outputs": [],
   "source": [
    "def MLP(n_users, n_items, layers, dropouts, reg):\n",
    "    num_layer = len(layers)  # Number of layers in the MLP\n",
    "\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user and item embeddings\n",
    "    MLP_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=int(layers[0] / 2),\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=int(layers[0] / 2),\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    # 扁平化\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user))\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item))\n",
    "\n",
    "    vector = concatenate([user_latent, item_latent])\n",
    "\n",
    "    # MLP layers\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], activation=\"relu\", kernel_regularizer=l2(reg), name=\"layer{}\".format(idx))\n",
    "        vector = layer(vector)\n",
    "        vector = Dropout(dropouts[idx - 1])(vector)\n",
    "\n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)\n",
    "\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:18:24.150534Z",
     "start_time": "2020-08-30T03:18:24.120145Z"
    }
   },
   "outputs": [],
   "source": [
    "def GMF(n_users, n_items, n_emb, reg):\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user and item embeddings\n",
    "    MF_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=n_emb,\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    MF_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=n_emb,\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    # Flatten and multiply\n",
    "    user_latent = Flatten()(MF_Embedding_User(user))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item))\n",
    "    predict_vector = multiply([user_latent, item_latent])\n",
    "\n",
    "    #  output layer\n",
    "    prediction = Dense(1, activation='sigmoid',\n",
    "                       kernel_regularizer=l2(reg),\n",
    "                       kernel_initializer='lecun_uniform',\n",
    "                       name='prediction')(predict_vector)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:19:57.310374Z",
     "start_time": "2020-08-30T03:19:57.296993Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_train_instances(train, n_items, n_neg, testNegatives):\n",
    "    user, item, labels = [],[],[]\n",
    "    n_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # test dataset\n",
    "        for t in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in testNegatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    return np.array(user), np.array(item), np.array(labels)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:58:15.339115Z",
     "start_time": "2020-08-30T03:20:00.671835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2503: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Iteration 1: 164.85s, HR = 0.5460, NDCG = 0.3035, loss = 0.3364, validated in 11.84s\n",
      "Iteration 2: 164.55s, HR = 0.6136, NDCG = 0.3416, loss = 0.2956, validated in 16.15s\n",
      "Iteration 3: 158.22s, HR = 0.6161, NDCG = 0.3475, loss = 0.2836, validated in 16.66s\n",
      "Iteration 4: 148.73s, HR = 0.6343, NDCG = 0.3657, loss = 0.2748, validated in 15.02s\n",
      "Iteration 5: 161.11s, HR = 0.6502, NDCG = 0.3780, loss = 0.2677, validated in 14.43s\n",
      "Iteration 6: 142.70s, HR = 0.6531, NDCG = 0.3813, loss = 0.2625, validated in 11.48s\n",
      "Iteration 7: 95.38s, HR = 0.6553, NDCG = 0.3844, loss = 0.2589, validated in 4.93s\n",
      "Iteration 8: 98.71s, HR = 0.6636, NDCG = 0.3894, loss = 0.2557, validated in 5.98s\n",
      "Iteration 9: 86.98s, HR = 0.6680, NDCG = 0.3943, loss = 0.2533, validated in 5.04s\n",
      "Iteration 10: 81.48s, HR = 0.6642, NDCG = 0.3923, loss = 0.2515, validated in 5.08s\n",
      "Iteration 11: 81.73s, HR = 0.6694, NDCG = 0.3968, loss = 0.2497, validated in 4.84s\n",
      "Iteration 12: 81.75s, HR = 0.6611, NDCG = 0.3869, loss = 0.2480, validated in 5.56s\n",
      "Iteration 13: 84.97s, HR = 0.6536, NDCG = 0.3867, loss = 0.2475, validated in 4.66s\n",
      "Iteration 14: 81.61s, HR = 0.6677, NDCG = 0.3944, loss = 0.2465, validated in 5.24s\n",
      "Iteration 15: 81.24s, HR = 0.6709, NDCG = 0.3971, loss = 0.2457, validated in 4.69s\n",
      "Iteration 16: 80.61s, HR = 0.6664, NDCG = 0.3951, loss = 0.2449, validated in 4.58s\n",
      "Iteration 17: 80.50s, HR = 0.6674, NDCG = 0.3898, loss = 0.2442, validated in 4.63s\n",
      "Iteration 18: 80.91s, HR = 0.6666, NDCG = 0.3930, loss = 0.2436, validated in 4.69s\n",
      "Iteration 19: 80.71s, HR = 0.6709, NDCG = 0.3911, loss = 0.2432, validated in 4.54s\n",
      "Iteration 20: 80.47s, HR = 0.6747, NDCG = 0.3990, loss = 0.2429, validated in 4.44s\n",
      "End. Best Iteration 20:  HR = 0.6747, NDCG = 0.3990. \n",
      "The best NeuCF model is saved to models/keras_NeuMF_without_pretrain_trainable_adam.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "import keras\n",
    "\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Dense, Embedding, Input, Dropout, Flatten, concatenate, multiply\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "def NeuMF(n_users, n_items, n_emb, layers, dropouts, reg_mf, reg_mlp, reg_out):\n",
    "\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "\n",
    "    user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    # user and item embeddings [?, 1, 8]\n",
    "    MF_Embedding_User = Embedding(input_dim = n_users, output_dim = n_emb,\n",
    "        name = 'mf_user_embedding', embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_Item = Embedding(input_dim = n_items, output_dim = n_emb,\n",
    "        name = 'mf_item_embedding', embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    \n",
    "    # [?, 1, 32]\n",
    "    MLP_Embedding_User = Embedding(input_dim = n_users, output_dim = int(layers[0]/2),\n",
    "        name = 'mlp_user_embedding', embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg_mlp), input_length=1)\n",
    "    # [?, 1, 32]\n",
    "    MLP_Embedding_Item = Embedding(input_dim = n_items, output_dim = int(layers[0]/2),\n",
    "        name = 'mlp_item_embedding', embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg_mlp), input_length=1)\n",
    "\n",
    "    # GMF part\n",
    "    # 扁平化以后，是一个一维度的向量\n",
    "    # [?, 8]\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item))\n",
    "    # [?, 8]\n",
    "    mf_vector = multiply([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP part\n",
    "    # [?, 32]\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item))\n",
    "    # [?, 64]\n",
    "    mlp_vector = concatenate([mlp_user_latent, mlp_item_latent])\n",
    "    # [32,16,8]\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], activation=\"relu\", kernel_regularizer=l2(reg_mlp), name = \"layer{}\".format(idx))\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "        mlp_vector = Dropout(dropouts[idx-1])(mlp_vector)\n",
    "    \n",
    "    # mlp_vector: [?, 8]\n",
    "    # [?, 16]\n",
    "    predict_vector = concatenate([mf_vector, mlp_vector])\n",
    "\n",
    "    # Final prediction layer\n",
    "    # [?, 1]\n",
    "    prediction = Dense(1, activation='sigmoid',\n",
    "        kernel_regularizer=l2(reg_out),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        name = 'prediction')(predict_vector)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrain_model(model, gmf_model, mlp_model, num_layers):\n",
    "\n",
    "    # MF embeddings\n",
    "    gmf_user_embeddings = gmf_model.get_layer('user_embedding').get_weights()\n",
    "    gmf_item_embeddings = gmf_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mf_user_embedding').set_weights(gmf_user_embeddings)\n",
    "    model.get_layer('mf_item_embedding').set_weights(gmf_item_embeddings)\n",
    "\n",
    "    # MLP embeddings\n",
    "    mlp_user_embeddings = mlp_model.get_layer('user_embedding').get_weights()\n",
    "    mlp_item_embeddings = mlp_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mlp_user_embedding').set_weights(mlp_user_embeddings)\n",
    "    model.get_layer('mlp_item_embedding').set_weights(mlp_item_embeddings)\n",
    "\n",
    "    # MLP layers\n",
    "    for i in range(1, num_layers):\n",
    "        mlp_layer_weights = mlp_model.get_layer(\"layer{}\".format(i)).get_weights()\n",
    "        model.get_layer(\"layer{}\".format(i)).set_weights(mlp_layer_weights)\n",
    "\n",
    "    # Prediction weights\n",
    "    gmf_prediction = gmf_model.get_layer('prediction').get_weights()\n",
    "    mlp_prediction = mlp_model.get_layer('prediction').get_weights()\n",
    "    new_weights = np.concatenate((gmf_prediction[0], mlp_prediction[0]), axis=0)\n",
    "    new_b = gmf_prediction[1] + mlp_prediction[1]\n",
    "    model.get_layer('prediction').set_weights([0.5*new_weights, 0.5*new_b])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # dir\n",
    "    datadir = \"Data_Javier/\"\n",
    "    dataname = \"ml-1m\"\n",
    "    modeldir = \"models\"\n",
    "    \n",
    "    # general parameter\n",
    "    epochs = 20\n",
    "    batch_size = 256\n",
    "    lr = 0.01\n",
    "    learner = \"adam\"\n",
    "    \n",
    "    # GMF\n",
    "    n_emb = 8\n",
    "    reg_mf = 0.0\n",
    "    \n",
    "    # MLP\n",
    "    layers = [64,32,16,8]\n",
    "    reg_mlp = 0.0\n",
    "    dropouts = [0,0,0]\n",
    "    \n",
    "    # Output layer\n",
    "    reg_out = 0.0\n",
    "    \n",
    "    # Pretrained model\n",
    "    freeze = False\n",
    "    \n",
    "    mf_pretrain_name = \"\"\n",
    "    mlp_pretrain_name = \"\"\n",
    "    # mf_pretrain_name = \"keras_GMF_bs_1024_reg_00_lr_0001_n_emb_16.h5\"\n",
    "    # mlp_pretrain_name = \"keras_MLP_bs_256_reg_00_lr_0001_n_emb_128_ll_64_dp_wodp.h5\"\n",
    "\n",
    "    # Experiment\n",
    "    validate_every = 1\n",
    "    save_model = True\n",
    "    n_neg = 4\n",
    "    topK = 10\n",
    "    \n",
    "    \n",
    "    mf_pretrain = os.path.join(modeldir, mf_pretrain_name)\n",
    "    mlp_pretrain = os.path.join(modeldir, mlp_pretrain_name)\n",
    "    with_pretrained = \"with_pretrain\" if os.path.isfile(mf_pretrain) else \"without_pretrain\"\n",
    "    is_frozen = \"frozen\" if freeze else \"trainable\"\n",
    "\n",
    "    modelfname = \"keras_NeuMF\" + \\\n",
    "        \"_\" + with_pretrained + \\\n",
    "        \"_\" + is_frozen + \\\n",
    "        \"_\" + learner + \\\n",
    "        \".h5\"\n",
    "\n",
    "    modelpath = os.path.join(modeldir, modelfname)\n",
    "    resultsdfpath = os.path.join(modeldir, 'results_df.p')\n",
    "\n",
    "    # Loading data\n",
    "    dataset = Dataset(os.path.join(datadir, dataname))\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    n_users, n_items = train.shape    \n",
    "    \n",
    "    # 构建模型\n",
    "    model = NeuMF(n_users, n_items, n_emb, layers, dropouts, reg_mf, reg_mlp, reg_out)\n",
    "    if freeze:\n",
    "            for layer in model.layers[:-2]:\n",
    "                layer.trainable = False\n",
    "        if learner.lower() == \"adagrad\":\n",
    "            model.compile(optimizer=Adagrad(lr=lr), loss='binary_crossentropy')\n",
    "        elif learner.lower() == \"rmsprop\":\n",
    "            model.compile(optimizer=RMSprop(lr=lr), loss='binary_crossentropy')\n",
    "        elif learner.lower() == \"adam\":\n",
    "            model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy')\n",
    "        else:\n",
    "            model.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "            \n",
    "    # 加载预训练模型\n",
    "    if os.path.isfile(mf_pretrain) and os.path.isfile(mlp_pretrain):\n",
    "        gmf_model = GMF(n_users, n_items, n_emb, reg_mf)\n",
    "        gmf_model.load_weights(mf_pretrain)\n",
    "        mlp_model = MLP(n_users, n_items, layers, dropouts, reg_mlp)\n",
    "        mlp_model.load_weights(mlp_pretrain)\n",
    "        model = load_pretrain_model(model, gmf_model, mlp_model, len(layers))\n",
    "        print(\"Load pretrained GMF {} and MLP {} models done. \".format(mf_pretrain, mlp_pretrain))\n",
    "   \n",
    "\n",
    "    best_hr, best_ndcg, best_iter = 0,0,0\n",
    "    for epoch in range(1,epochs+1):\n",
    "        t1 = time()\n",
    "        # 生成数据集\n",
    "        user, item, labels = get_train_instances(train, n_items, n_neg, testNegatives)\n",
    "        # 训练模型\n",
    "        hist = model.fit([user, item], labels, batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        if epoch % validate_every ==0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print(\"Iteration {}: {:.2f}s, HR = {:.4f}, NDCG = {:.4f}, loss = {:.4f}, validated in {:.2f}s\"\n",
    "                .format(epoch, t2-t1, hr, ndcg, loss, time()-t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter, train_time = hr, ndcg, epoch, t2-t1\n",
    "                if save_model:\n",
    "                    model.save_weights(modelpath, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration {}:  HR = {:.4f}, NDCG = {:.4f}. \".format(best_iter, best_hr, best_ndcg))\n",
    "    if save_model:\n",
    "        print(\"The best NeuCF model is saved to {}\".format(modelpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
