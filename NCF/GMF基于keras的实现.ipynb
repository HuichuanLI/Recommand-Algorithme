{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:51:30.496857Z",
     "start_time": "2020-08-30T02:51:30.490367Z"
    }
   },
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        ratingList = list(zip(df.userid.tolist(), df.itemid.tolist()))\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        negativeList = df.iloc[:, 1:].values.tolist()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        num_users = df.userid.max()\n",
    "        num_items = df.itemid.max()\n",
    "        mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "        interactions = df[['userid', 'itemid']].values.tolist()\n",
    "        # [(0, 2969), (0, 1178), (0, 1574), (0, 957)]\n",
    "        for user, item in interactions:\n",
    "            mat[user, item] = 1.\n",
    "        # [((0, 2969), 1.0), ((0, 1178), 1.0), ((0, 1574), 1.0), ((0, 957), 1.0)]\n",
    "        return mat"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:51:32.363445Z",
     "start_time": "2020-08-30T02:51:32.356689Z"
    }
   },
   "source": [
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, topK):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _topK\n",
    "\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _topK = topK\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)],\n",
    "                                 batch_size=100, verbose=0)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    ranklist = heapq.nlargest(_topK, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def get_train_instances(train, n_items, n_neg, testNegatives):\n",
    "    user, item, labels = [],[],[]\n",
    "    n_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # test dataset\n",
    "        for t in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in testNegatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    return np.array(user), np.array(item), np.array(labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:20:23.077214Z",
     "start_time": "2020-08-30T02:51:36.958958Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "import keras\n",
    "import multiprocessing as mp\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Dense, Embedding, Input, merge, Flatten, multiply\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from time import time\n",
    "\n",
    "def GMF(n_users, n_items, n_emb, reg):\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user and item embeddings\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=n_emb,\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=n_emb,\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    # Flatten and multiply\n",
    "    # [?, 8]\n",
    "    user_latent = Flatten()(MF_Embedding_User(user))\n",
    "    # [?, 8]\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item))\n",
    "    # [?, 8]\n",
    "    predict_vector = multiply([user_latent, item_latent])\n",
    "\n",
    "    # output layer\n",
    "    prediction = Dense(1, activation='sigmoid',\n",
    "                       kernel_regularizer=l2(reg),\n",
    "                       kernel_initializer='lecun_uniform',\n",
    "                       name='prediction')(predict_vector)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datadir = \"Data_Javier/\"\n",
    "    dataname = \"ml-1m\"\n",
    "    modeldir = \"models\"\n",
    "    n_emb = 8\n",
    "    reg = 0.0\n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    learner = \"adam\"\n",
    "    lr = 0.01\n",
    "    validate_every = 1\n",
    "    save_model = True\n",
    "    topK = 10\n",
    "    n_neg = 4\n",
    "\n",
    "    modelfname = \"keras_GMF\" + \\\n",
    "                 \"_\".join([\"_bs\", str(batch_size)]) + \\\n",
    "                 \"_\".join([\"_reg\", str(reg).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_lr\", str(lr).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_n_emb\", str(n_emb)]) + \".h5\"\n",
    "    modelpath = os.path.join(modeldir, modelfname)\n",
    "    resultsdfpath = os.path.join(modeldir, 'results_df.p')\n",
    "\n",
    "    dataset = Dataset(os.path.join(datadir, dataname))\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    model = GMF(n_users, n_items, n_emb, reg)\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "    best_hr, best_ndcg, best_iter = 0, 0, 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t1 = time()\n",
    "        user, item, labels = get_train_instances(train, n_items, n_neg, testNegatives)\n",
    "        hist = model.fit([user, item], labels, batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        if epoch % validate_every == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print(\"Iteration {}: {:.2f}s, HR = {:.4f}, NDCG = {:.4f}, loss = {:.4f}, validated in {:.2f}s\"\n",
    "                  .format(epoch, t2 - t1, hr, ndcg, loss, time() - t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter, train_time = hr, ndcg, epoch, t2 - t1\n",
    "                if save_model:\n",
    "                    model.save_weights(modelpath, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration {}:  HR = {:.4f}, NDCG = {:.4f}. \"\n",
    "          .format(best_iter, best_hr, best_ndcg))\n",
    "    if save_model:\n",
    "        print(\"The best GMF model is saved to {}\".format(modelpath))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1] *",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
