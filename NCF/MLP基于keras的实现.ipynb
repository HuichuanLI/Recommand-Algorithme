{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:56:19.125149Z",
     "start_time": "2020-08-30T02:56:19.117674Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        ratingList = list(zip(df.userid.tolist(), df.itemid.tolist()))\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        negativeList = df.iloc[:, 1:].values.tolist()\n",
    "        return negativeList\n",
    "\n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        num_users = df.userid.max()\n",
    "        num_items = df.itemid.max()\n",
    "        mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "        interactions = df[['userid', 'itemid']].values.tolist()\n",
    "        # [(0, 2969), (0, 1178), (0, 1574), (0, 957)]\n",
    "        for user, item in interactions:\n",
    "            mat[user, item] = 1.\n",
    "        # [((0, 2969), 1.0), ((0, 1178), 1.0), ((0, 1574), 1.0), ((0, 957), 1.0)]\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:56:21.406637Z",
     "start_time": "2020-08-30T02:56:21.397041Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, topK):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _topK\n",
    "\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _topK = topK\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)],\n",
    "                                 batch_size=100, verbose=0)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    ranklist = heapq.nlargest(_topK, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def get_train_instances(train, n_items, n_neg, testNegatives):\n",
    "    user, item, labels = [],[],[]\n",
    "    n_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # test dataset\n",
    "        for t in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in testNegatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    return np.array(user), np.array(item), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:37:34.550601Z",
     "start_time": "2020-08-30T02:56:24.210976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3814: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3075: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2503: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Iteration 1: 94.95s, HR = 0.5129, NDCG = 0.2805, loss = 0.3487, validated in 6.23s\n",
      "Iteration 2: 88.64s, HR = 0.5680, NDCG = 0.3198, loss = 0.3093, validated in 5.86s\n",
      "Iteration 3: 88.64s, HR = 0.5970, NDCG = 0.3383, loss = 0.2927, validated in 6.76s\n",
      "Iteration 4: 97.21s, HR = 0.5965, NDCG = 0.3418, loss = 0.2832, validated in 6.68s\n",
      "Iteration 5: 96.90s, HR = 0.6127, NDCG = 0.3517, loss = 0.2772, validated in 8.32s\n",
      "Iteration 6: 100.20s, HR = 0.6161, NDCG = 0.3579, loss = 0.2722, validated in 7.94s\n",
      "Iteration 7: 99.02s, HR = 0.6250, NDCG = 0.3653, loss = 0.2686, validated in 8.58s\n",
      "Iteration 8: 108.48s, HR = 0.6632, NDCG = 0.3805, loss = 0.2655, validated in 7.73s\n",
      "Iteration 9: 96.67s, HR = 0.6290, NDCG = 0.3654, loss = 0.2626, validated in 7.90s\n",
      "Iteration 10: 97.90s, HR = 0.6349, NDCG = 0.3735, loss = 0.2606, validated in 6.85s\n",
      "Iteration 11: 100.53s, HR = 0.6414, NDCG = 0.3743, loss = 0.2589, validated in 5.75s\n",
      "Iteration 12: 95.94s, HR = 0.6434, NDCG = 0.3760, loss = 0.2571, validated in 7.72s\n",
      "Iteration 13: 123.19s, HR = 0.6329, NDCG = 0.3704, loss = 0.2558, validated in 13.18s\n",
      "Iteration 14: 127.44s, HR = 0.6664, NDCG = 0.3841, loss = 0.2544, validated in 15.11s\n",
      "Iteration 15: 144.97s, HR = 0.6482, NDCG = 0.3809, loss = 0.2533, validated in 16.98s\n",
      "Iteration 16: 144.08s, HR = 0.6469, NDCG = 0.3786, loss = 0.2522, validated in 12.94s\n",
      "Iteration 17: 148.02s, HR = 0.6281, NDCG = 0.3760, loss = 0.2512, validated in 8.27s\n",
      "Iteration 18: 134.20s, HR = 0.6442, NDCG = 0.3785, loss = 0.2506, validated in 8.20s\n",
      "Iteration 19: 156.10s, HR = 0.6750, NDCG = 0.3932, loss = 0.2497, validated in 6.92s\n",
      "Iteration 20: 136.46s, HR = 0.6571, NDCG = 0.3541, loss = 0.2518, validated in 6.93s\n",
      "End. Best Iteration 19:  HR = 0.6750, NDCG = 0.3932. \n",
      "The best MLP model is saved to models/keras_MLP_bs_256_reg_00_lr_001_n_emb_32_ll_8_dp_wodp.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "import keras\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Dense, Embedding, Input, Dropout, Flatten, concatenate\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from time import time\n",
    "\n",
    "def MLP(n_users, n_items, layers, dropouts, reg):\n",
    "    num_layer = len(layers)  # MLP 层数\n",
    "\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user 和 item embedding\n",
    "    # [?, 1, 32]\n",
    "    MLP_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=int(layers[0] / 2),\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    # [?, 1, 32]\n",
    "    MLP_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=int(layers[0] / 2),\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    \n",
    "    # [?, 32]\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user))\n",
    "    # [?, 32]\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item))\n",
    "    # [?, 64]\n",
    "    vector = concatenate([user_latent, item_latent])\n",
    "    \n",
    "    # vector: [batch_size, embedding_size] -> [?, 64]\n",
    "    # MLP层\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], activation=\"relu\", kernel_regularizer=l2(reg), name=\"layer{}\".format(idx))\n",
    "        vector = layer(vector)\n",
    "        vector = Dropout(dropouts[idx - 1])(vector)\n",
    "    # vector: [?, 8]\n",
    "    # [?, 1]\n",
    "    # 预测层\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)\n",
    "\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datadir = \"Data_Javier/\"\n",
    "    dataname = \"ml-1m\"\n",
    "    modeldir = \"models\"\n",
    "    layers = [64,32,16,8]\n",
    "    ll = str(layers[-1])  # last layer\n",
    "    dropouts = [0,0,0]\n",
    "    dp = \"wdp\" if dropouts[0] != 0 else \"wodp\"\n",
    "    n_emb = int(layers[0] / 2)\n",
    "    reg = 0.0\n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    learner = \"adam\"\n",
    "    lr = 0.01\n",
    "    validate_every = 1\n",
    "    save_model = True\n",
    "    topK = 10\n",
    "    n_neg = 4\n",
    "\n",
    "    modelfname = \"keras_MLP\" + \\\n",
    "                 \"_\".join([\"_bs\", str(batch_size)]) + \\\n",
    "                 \"_\".join([\"_reg\", str(reg).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_lr\", str(lr).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_n_emb\", str(n_emb)]) + \\\n",
    "                 \"_\".join([\"_ll\", ll]) + \\\n",
    "                 \"_\".join([\"_dp\", dp]) + \\\n",
    "                 \".h5\"\n",
    "    modelpath = os.path.join(modeldir, modelfname)\n",
    "    resultsdfpath = os.path.join(modeldir, 'results_df.p')\n",
    "\n",
    "    # Loading data\n",
    "    dataset = Dataset(os.path.join(datadir, dataname))\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    # Build model\n",
    "    model = MLP(n_users, n_items, layers, dropouts, reg)\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "    best_hr, best_ndcg, best_iter = 0, 0, 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t1 = time()\n",
    "        user, item, labels = get_train_instances(train, n_items, n_neg, testNegatives)\n",
    "        hist = model.fit([user, item], labels, batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        if epoch % validate_every == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print(\"Iteration {}: {:.2f}s, HR = {:.4f}, NDCG = {:.4f}, loss = {:.4f}, validated in {:.2f}s\"\n",
    "                  .format(epoch, t2 - t1, hr, ndcg, loss, time() - t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter, train_time = hr, ndcg, epoch, t2 - t1\n",
    "                if save_model:\n",
    "                    model.save_weights(modelpath, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration {}:  HR = {:.4f}, NDCG = {:.4f}. \"\n",
    "          .format(best_iter, best_hr, best_ndcg))\n",
    "    if save_model:\n",
    "        print(\"The best MLP model is saved to {}\".format(modelpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
