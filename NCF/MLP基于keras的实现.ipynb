{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:56:19.125149Z",
     "start_time": "2020-08-30T02:56:19.117674Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        ratingList = list(zip(df.userid.tolist(), df.itemid.tolist()))\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        negativeList = df.iloc[:, 1:].values.tolist()\n",
    "        return negativeList\n",
    "\n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        num_users = df.userid.max()\n",
    "        num_items = df.itemid.max()\n",
    "        mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "        interactions = df[['userid', 'itemid']].values.tolist()\n",
    "        # [(0, 2969), (0, 1178), (0, 1574), (0, 957)]\n",
    "        for user, item in interactions:\n",
    "            mat[user, item] = 1.\n",
    "        # [((0, 2969), 1.0), ((0, 1178), 1.0), ((0, 1574), 1.0), ((0, 957), 1.0)]\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:56:21.406637Z",
     "start_time": "2020-08-30T02:56:21.397041Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, topK):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _topK\n",
    "\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _topK = topK\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)],\n",
    "                                 batch_size=100, verbose=0)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    ranklist = heapq.nlargest(_topK, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def get_train_instances(train, n_items, n_neg, testNegatives):\n",
    "    user, item, labels = [],[],[]\n",
    "    n_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # test dataset\n",
    "        for t in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in testNegatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    return np.array(user), np.array(item), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:37:34.550601Z",
     "start_time": "2020-08-30T02:56:24.210976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 67.58s, HR = 0.4985, NDCG = 0.2756, loss = 0.3491, validated in 4.13s\n",
      "Iteration 2: 74.78s, HR = 0.5654, NDCG = 0.3184, loss = 0.3102, validated in 5.84s\n",
      "Iteration 3: 64.15s, HR = 0.5964, NDCG = 0.3381, loss = 0.2924, validated in 4.82s\n",
      "Iteration 4: 65.01s, HR = 0.6272, NDCG = 0.3549, loss = 0.2826, validated in 6.78s\n",
      "Iteration 5: 64.51s, HR = 0.6166, NDCG = 0.3586, loss = 0.2755, validated in 4.69s\n",
      "Iteration 6: 67.07s, HR = 0.6257, NDCG = 0.3614, loss = 0.2704, validated in 6.42s\n",
      "Iteration 7: 75.08s, HR = 0.6392, NDCG = 0.3728, loss = 0.2665, validated in 6.16s\n",
      "Iteration 8: 78.11s, HR = 0.6447, NDCG = 0.3764, loss = 0.2631, validated in 7.21s\n",
      "Iteration 9: 74.62s, HR = 0.6430, NDCG = 0.3760, loss = 0.2606, validated in 6.49s\n",
      "Iteration 10: 80.43s, HR = 0.6440, NDCG = 0.3755, loss = 0.2586, validated in 7.99s\n",
      "Iteration 11: 77.98s, HR = 0.6591, NDCG = 0.3853, loss = 0.2564, validated in 7.13s\n",
      "Iteration 12: 79.87s, HR = 0.6414, NDCG = 0.3753, loss = 0.2546, validated in 5.96s\n",
      "Iteration 13: 76.18s, HR = 0.6384, NDCG = 0.3778, loss = 0.2533, validated in 7.01s\n",
      "Iteration 14: 73.20s, HR = 0.6520, NDCG = 0.3832, loss = 0.2519, validated in 5.63s\n",
      "Iteration 15: 72.93s, HR = 0.6512, NDCG = 0.3809, loss = 0.2508, validated in 7.59s\n",
      "Iteration 16: 78.27s, HR = 0.6637, NDCG = 0.3855, loss = 0.2498, validated in 7.08s\n",
      "Iteration 17: 76.91s, HR = 0.6450, NDCG = 0.3829, loss = 0.2487, validated in 5.94s\n",
      "Iteration 18: 72.89s, HR = 0.6508, NDCG = 0.3863, loss = 0.2481, validated in 6.67s\n",
      "Iteration 19: 82.26s, HR = 0.6488, NDCG = 0.3832, loss = 0.2473, validated in 7.81s\n",
      "Iteration 20: 66.18s, HR = 0.6510, NDCG = 0.3843, loss = 0.2465, validated in 5.51s\n",
      "End. Best Iteration 16:  HR = 0.6637, NDCG = 0.3855. \n",
      "The best MLP model is saved to models/keras_MLP_bs_256_reg_00_lr_001_n_emb_32_ll_8_dp_wodp.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "import keras\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Dense, Embedding, Input, Dropout, Flatten, concatenate\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from time import time\n",
    "\n",
    "def MLP(n_users, n_items, layers, dropouts, reg):\n",
    "    num_layer = len(layers)  # MLP 层数\n",
    "\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user 和 item embedding\n",
    "    # [?, 1, 32]\n",
    "    MLP_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=int(layers[0] / 2),\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    # [?, 1, 32]\n",
    "    MLP_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=int(layers[0] / 2),\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    \n",
    "    # [?, 32]\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user))\n",
    "    # [?, 32]\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item))\n",
    "    # [?, 64]\n",
    "    vector = concatenate([user_latent, item_latent])\n",
    "    \n",
    "    # vector: [batch_size, embedding_size] -> [?, 64]\n",
    "    # MLP层\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], activation=\"relu\", kernel_regularizer=l2(reg), name=\"layer{}\".format(idx))\n",
    "        vector = layer(vector)\n",
    "        vector = Dropout(dropouts[idx - 1])(vector)\n",
    "    # vector: [?, 8]\n",
    "    # [?, 1]\n",
    "    # 预测层\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)\n",
    "\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datadir = \"Data_Javier/\"\n",
    "    dataname = \"ml-1m\"\n",
    "    modeldir = \"models\"\n",
    "    layers = [64,32,16,8]\n",
    "    ll = str(layers[-1])  # last layer\n",
    "    dropouts = [0,0,0]\n",
    "    dp = \"wdp\" if dropouts[0] != 0 else \"wodp\"\n",
    "    n_emb = int(layers[0] / 2)\n",
    "    reg = 0.0\n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    learner = \"adam\"\n",
    "    lr = 0.01\n",
    "    validate_every = 1\n",
    "    save_model = True\n",
    "    topK = 10\n",
    "    n_neg = 4\n",
    "\n",
    "    modelfname = \"keras_MLP\" + \\\n",
    "                 \"_\".join([\"_bs\", str(batch_size)]) + \\\n",
    "                 \"_\".join([\"_reg\", str(reg).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_lr\", str(lr).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_n_emb\", str(n_emb)]) + \\\n",
    "                 \"_\".join([\"_ll\", ll]) + \\\n",
    "                 \"_\".join([\"_dp\", dp]) + \\\n",
    "                 \".h5\"\n",
    "    modelpath = os.path.join(modeldir, modelfname)\n",
    "    resultsdfpath = os.path.join(modeldir, 'results_df.p')\n",
    "\n",
    "    # Loading data\n",
    "    dataset = Dataset(os.path.join(datadir, dataname))\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    # Build model\n",
    "    model = MLP(n_users, n_items, layers, dropouts, reg)\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "    best_hr, best_ndcg, best_iter = 0, 0, 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t1 = time()\n",
    "        user, item, labels = get_train_instances(train, n_items, n_neg, testNegatives)\n",
    "        hist = model.fit([user, item], labels, batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        if epoch % validate_every == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print(\"Iteration {}: {:.2f}s, HR = {:.4f}, NDCG = {:.4f}, loss = {:.4f}, validated in {:.2f}s\"\n",
    "                  .format(epoch, t2 - t1, hr, ndcg, loss, time() - t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter, train_time = hr, ndcg, epoch, t2 - t1\n",
    "                if save_model:\n",
    "                    model.save_weights(modelpath, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration {}:  HR = {:.4f}, NDCG = {:.4f}. \"\n",
    "          .format(best_iter, best_hr, best_ndcg))\n",
    "    if save_model:\n",
    "        print(\"The best MLP model is saved to {}\".format(modelpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1] *",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
