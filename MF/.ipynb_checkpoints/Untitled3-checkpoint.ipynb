{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T00:37:42.467072Z",
     "start_time": "2020-08-23T00:37:42.464731Z"
    }
   },
   "source": [
    "## 矩阵分解的目标\n",
    "假设用户A的兴趣涉及到侦探小说，科普图书以及计算机技术书籍。\n",
    "用户B的兴趣比较集中在数学和机器学习方面。\n",
    "![image.png](imgs/用户item矩阵.png)\n",
    "### 那么如何给A和B推荐图书呢?\n",
    "- UserCF：首先需要找到和他们看了同样书的其他用户(兴趣相似的用户),然后给他们推荐那些用户喜欢的其他书。\n",
    "- ItemCF：需要给他们推荐和他们已经看的书相似的书，比如作者B看了很多关于数据挖掘的书，可以给他推荐机器学习或者模式识别方面的书。\n",
    "\n",
    "还有一种方法，可以对书和物品的兴趣进行分类。对于某个用户，首先得到他的兴趣分类，然后从分类中挑选他可能喜欢的物品。\n",
    "\n",
    "总结一下，这个基于兴趣分类的方法大概需要解决3个问题。\n",
    "- 如何给物品进行分类?\n",
    "- 如何确定用户对哪些类的物品感兴趣，以及感兴趣的程度?\n",
    "- 对于一个给定的类，选择哪些属于这个类的物品推荐给用户，以及如何确定这些物品在一个类中的权重?\n",
    "\n",
    "### 传统分类 \n",
    "对于第一个问题的简单解决方案是找编辑给物品分类。以图书为例，每本书出版时，编辑都会给书一个分类。为了给图书分类，出版界普遍遵循中国图书分类法。但是，即使有很系统的分类体系，编辑给出的分类仍然具有以下缺点。\n",
    "- 编辑的意见不能代表各种用户的意见。\n",
    "    - 比如，对于《具体数学》应该属于什么分类，有人认为应该属于数学，有些人认为应该属于计算机。从内容看，这本书是关于数学的，但从用户看，这本书的读大部分是做计算机出身的。编辑的分类大部分是从书的内容出发，而不是从书的读者群出发。\n",
    "- 编辑很难控制分类的粒度。\n",
    "    - 我们知道分类是有不同粒度的，《数据挖掘导论》在粗粒度的分类中可能属于计算机技术，但在细粒度的分类中可能属于数据挖掘。对于不同的用户，我们可能需要不同的粒度。比如对于一位初学者，我们粗粒度地给他做推荐就可以了，而对于--名资深研究人员，我们就需要深人到他的很细分的领域给他做个性化推荐。\n",
    "- 编辑很难给一个物品多个分类。有的书不仅属于一个类，而是可能属于很多的类。\n",
    "- 编辑很难给出多维度的分类。\n",
    "    - 我们知道，分类是可以有很多维度的，比如按照作者分类、按照译者分类、按照出版社分类。比如不同的用户看《具体数学》原因可能不同，有些人是因为它是数学方面的书所以才看的，而有些人是因为它是大师Knuth的著作所以才去看，因此在不同人的眼中这本书属于不同的分类。\n",
    "- 编辑很难决定一个物品在某一个分类中的权重。\n",
    "    - 比如编辑可以很容易地决定《数据挖掘导论》属于数据挖掘类图书，但这本书在这类书中的定位是什么样的，编辑就很难给出一个准确的数字来表示。\n",
    "    \n",
    "### 隐语义模型\n",
    "为了解决上面的问题，研究人员提出:为什么我们不从数据出发，自动地找到那些类，然后进行个性化推荐?于是，隐含语义分析技术( latent variable analysis)出现了。隐含语义分析技术因为采取基于用户行为统计的自动聚类，较好地解决了上面提出的5个问题。\n",
    "- 编辑的意见不能代表各种用户的意见，但隐含语义分析技术的分类来自对用户行为的统计，代表了用户对物品分类的看法。隐含语义分析技术和ItemCF在物品分类方面的思想.类似，如果两个物品被很多用户同时喜欢，那么这两个物品就很有可能属于同一个类。\n",
    "- 编辑很难控制分类的粒度，但隐含语义分析技术允许我们指定最终有多少个分类，这个数字越大，分类的粒度就会越细，反正分类粒度就越粗。\n",
    "- 编辑很难给一个物品多个分类，但隐含语义分析技术会计算出物品属于每个类的权重，因此每个物品都不是硬性地被分到某一个类中。\n",
    "- 编辑很难给出多维度的分类，但隐含语义分析技术给出的每个分类都不是同-个维度的,它是基于用户的共同兴趣计算出来的，如果用户的共同兴趣是某-一个维度，那么LFM给出的类也是相同的维度。\n",
    "\n",
    "### 评分矩阵\n",
    "每一行代表一个用户，每一列代表一个物品，表格里的每一个值代表用户对物品的操作，这个操作可以是评分，点击，点赞。\n",
    "\n",
    "其中，有些格子记录了行为，有些格子是空的。\n",
    "\n",
    "**推荐问题转化成了如何补上那些空格子**\n",
    "\n",
    "![image.png](imgs/movielens评分矩阵.png)\n",
    "\n",
    "\n",
    "\n",
    "### 隐变量\n",
    "基于矩阵分解的推荐算法的核心假设是用隐语义（隐变量）来表达用户和物品，他们的乘积关系就成为了原始的元素。\n",
    "\n",
    "这种假设之所以成立，是因为我们认为实际的交互数据是由一系列的隐变量的影响下产生的（通常隐变量带有统计分布的假设，就是隐变量之间，或者隐变量和显式变量之间的关系，我们往往认为是由某种分布产生的。），这些隐变量代表了用户和物品一部分共有的特征，在物品身上表现为属性特征，在用户身上表现为偏好特征，只不过这些因子并不具有实际意义，也不一定具有非常好的可解释性，每一个维度也没有确定的标签名字，所以才会叫做 “隐变量”。\n",
    "\n",
    "而矩阵分解后得到的两个包含隐变量的小矩阵，一个代表用户的隐含特征，一个代表物品的隐含特征，矩阵的元素值代表着相应用户或物品对各项隐因子的符合程度，有正面的也有负面的。\n",
    "\n",
    "## MF(Funk SVD)\n",
    "直接将评分矩阵分解成两个矩阵相乘， k x n 维度的用户矩阵，每一行是用户的隐式向量表示， n x m 维的物品矩阵，每一列是物品的隐式向量表示，用户和物品向量的内积即为预估的评分。那如何进行分解呢？随机初始化矩阵，使用均方误差作为loss，梯度下降进行学习。这个过程中还可以加入正则项，降低泛化误差。由FunkSVD开始，基于Matrix factor（MF）的方法大放异彩。\n",
    "![image.png](imgs/MF矩阵分解.png)\n",
    "\n",
    "## NCF\n",
    "全称 Neural Collaborative Filtering\n",
    "\n",
    "论文地址：https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "\n",
    "$Loss(X, Y)=\\sum_{i,u}(r_{ui}-x_uy_i^T)^2+λ(\\sum_u||x_u||^2)+λ(\\sum_u||y_i||^2)$\n",
    "![image.png](imgs/隐语义模型.png)\n",
    "\n",
    "\n",
    "## 评分矩阵\n",
    "每一行代表一个用户，每一列代表一个物品，表格里的每一个值代表用户对物品的操作，这个操作可以是评分，点击，点赞。\n",
    "\n",
    "其中，有些格子记录了行为，有些格子是空的。\n",
    "\n",
    "**推荐问题转化成了如何补上那些空格子**\n",
    "\n",
    "![image.png](imgs/movielens评分矩阵.png)\n",
    "\n",
    "## Funk SVD(LMF)\n",
    "直接将评分矩阵分解成两个矩阵相乘， k x n 维度的用户矩阵，每一行是用户的隐式向量表示， n x m 维的物品矩阵，每一列是物品的隐式向量表示，用户和物品向量的内积即为预估的评分。\n",
    "![image.png](imgs/MF矩阵分解.png)\n",
    "\n",
    "那如何进行分解呢？\n",
    "\n",
    "随机初始化矩阵，使用均方误差作为loss，梯度下降进行学习。这个过程中还可以加入正则项，降低泛化误差。\n",
    "\n",
    "$Loss(X, Y)=\\sum_{i,u}(r_{ui}-x_uy_i^T)^2+λ(\\sum_u||x_u||^2)+λ(\\sum_u||y_i||^2)$\n",
    "\n",
    "#### 建模伪代码\n",
    "```python\n",
    "input_user, input_movie -> user_embedding, item_embedding\n",
    "\n",
    "user_embedding = Dense(units=embedding_size, kernel_regularizer=regularizers.l2(0.01))(user_embedding)\n",
    "item_embedding = Dense(units=embedding_size, kernel_regularizer=regularizers.l2(0.01))(item_embedding)\n",
    "\n",
    "prediction = Dot(1)([user_embedding, item_embedding])\n",
    "\n",
    "model = Model(inputs=[input_uer, input_movie], outputs=prediction)\n",
    "```\n",
    "\n",
    "## NCF\n",
    "全称 Neural Collaborative Filtering\n",
    "![image.png](imgs/NCF1.png)\n",
    "\n",
    "#### 建模伪代码\n",
    "```python\n",
    "input_user, input_movie -> user_embedding, item_embedding\n",
    "\n",
    "MF_user_embedding = user_embedding\n",
    "MLP_user_embedding = user_embedding\n",
    "\n",
    "MF_item_embedding = user_embedding\n",
    "MLP_item_embedding = user_embedding\n",
    "\n",
    "\n",
    "embedding_GMF = MF_user_embedding * MF_item_embedding\n",
    "embedding_MLP_1 = concat(MLP_user_embedding, MLP_item_embedding)\n",
    "embedding_MLP_2 = Dense(units=embedding_MLP_1)\n",
    "embedding_MLP_3 = Dense(units=embedding_MLP_2)\n",
    "\n",
    "NeuMF_layer = concat(embedding_GMF, embedding_MLP_3)\n",
    "\n",
    "prediction = Dense(units=1, activation='softmax')(NeuMF_layer)\n",
    "\n",
    "model = Model(inputs=[input_uer, input_movie], outputs=prediction)\n",
    "\n",
    "model.compile(loss='mse', optimizer='Adam')\n",
    "```\n",
    "\n",
    "![image.png](imgs/NCF.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
