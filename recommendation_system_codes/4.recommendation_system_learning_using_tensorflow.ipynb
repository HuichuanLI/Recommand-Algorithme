{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf.jpeg)\n",
    "真实的场景中，可能我们有非常非常多的训练数据，我们不得不面对一些问题，也是大家比较关心的问题。\n",
    "\n",
    "1）海量的数据无法一次载入内存用于训练。<br>\n",
    "2）数据是每天不断增加的，我们有没有一些增量训练的方式去不断持续迭代更新模型？\n",
    "\n",
    "什么场景下，我们是不把数据全部载入内存优化，而是一个batch一个batch输入进行update参数的？<br>\n",
    "对，我们用tensorflow来完成一个在批量数据上更新，并且可以增量迭代优化的矩阵分解推荐系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.矩阵分解回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](svd_recommendation.png)\n",
    "LFM：把用户再item上打分的行为，看作是有内部依据的，认为和k个factor有关系<br>\n",
    "每一个user i会有一个用户的向量(k维)，每一个item会有一个item的向量(k维)\n",
    "\n",
    "SVD是矩阵分解的一种方式\n",
    "\n",
    "### 预测公式如下\n",
    "$y_{pred[u, i]} = bias_{global} + bias_{user[u]} + bias_{item_[i]} + <embedding_{user[u]}, embedding_{item[i]}>$\n",
    "\n",
    "### 我们需要最小化的loss计算如下（添加正则化项）\n",
    "$\\sum_{u, i} |y_{pred[u, i]} - y_{true[u, i]}|^2 + \\lambda(|embedding_{user[u]}|^2 + |embedding_{item[i]}|^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.获取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们依旧以movielens为例，数据格式为**user item rating timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这部分代码大家不用跑，因为数据已经下载好了\n",
    "#!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "#!sudo unzip ml-1m.zip -d ./movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据处理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们写点代码完成数据的产出和预处理过程。<br>\n",
    "大家知道tensorflow搭建的模型，训练方式通常是一个batch一个batch训练的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_data_and_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "class ShuffleDataIterator(object):\n",
    "    \"\"\"\n",
    "    随机生成一个batch一个batch数据\n",
    "    \"\"\"\n",
    "    #初始化\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    #总样本量\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    #取出下一个batch\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    #随机生成batch_size个下标，取出对应的样本\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochDataIterator(ShuffleDataIterator):\n",
    "    \"\"\"\n",
    "    顺序产出一个epoch的数据，在测试中可能会用到\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochDataIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们按照下图的方式用tensorflow去搭建一个可增量训练的矩阵分解模型，完成基于矩阵分解的推荐系统。\n",
    "![](tf_svd_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 使用矩阵分解搭建的网络结构\n",
    "def inference_svd(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    #使用CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # 初始化几个bias项\n",
    "        global_bias = tf.get_variable(\"global_bias\", shape=[])\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # bias向量\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # user向量与item向量\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    with tf.device(device):\n",
    "        # 按照实际公式进行计算\n",
    "        # 先对user向量和item向量求内积\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        # 加上几个偏置项\n",
    "        infer = tf.add(infer, global_bias)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # 加上正则化项\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), name=\"svd_regularizer\")\n",
    "    return infer, regularizer\n",
    "\n",
    "# 迭代优化部分\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.1, device=\"/cpu:0\"):\n",
    "    global_step = tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "    # 选择合适的optimizer做优化\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.数据上的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "\n",
    "np.random.seed(13575)\n",
    "\n",
    "# 一批数据的大小\n",
    "BATCH_SIZE = 2000\n",
    "# 用户数\n",
    "USER_NUM = 6040\n",
    "# 电影数\n",
    "ITEM_NUM = 3952\n",
    "# factor维度\n",
    "DIM = 15\n",
    "# 最大迭代轮数\n",
    "EPOCH_MAX = 200\n",
    "# 使用cpu做训练\n",
    "DEVICE = \"/cpu:0\"\n",
    "\n",
    "# 截断\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)\n",
    "\n",
    "# 这个是方便Tensorboard可视化做的summary\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=val)])\n",
    "\n",
    "# 调用上面的函数获取数据\n",
    "def get_data():\n",
    "    df = read_data_and_process(\"./movielens/ml-1m/ratings.dat\", sep=\"::\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * 0.9)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    return df_train, df_test\n",
    "\n",
    "# 实际训练过程\n",
    "def svd(train, test):\n",
    "    samples_per_batch = len(train) // BATCH_SIZE\n",
    "\n",
    "    # 一批一批数据用于训练\n",
    "    iter_train = ShuffleDataIterator([train[\"user\"],\n",
    "                                         train[\"item\"],\n",
    "                                         train[\"rate\"]],\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "    # 测试数据\n",
    "    iter_test = OneEpochDataIterator([test[\"user\"],\n",
    "                                         test[\"item\"],\n",
    "                                         test[\"rate\"]],\n",
    "                                        batch_size=-1)\n",
    "    # user和item batch\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    # 构建graph和训练\n",
    "    infer, regularizer = inference_svd(user_batch, item_batch, user_num=USER_NUM, item_num=ITEM_NUM, dim=DIM,\n",
    "                                           device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.05, device=DEVICE)\n",
    "\n",
    "    # 初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 开始迭代\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"/tmp/svd/log\", graph=sess.graph)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   item_batch: items,\n",
    "                                                                   rate_batch: rates})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items})\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print(\"{:3d} {:f} {:f} {:f}(s)\".format(i // samples_per_batch, train_err, test_err,\n",
    "                                                       end - start))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\", train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\", test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900188, 4) (100021, 4)\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "df_train, df_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_error val_error elapsed_time\n",
      "  0 2.809092 2.814245 0.030767(s)\n",
      "  1 2.782453 2.449130 0.816012(s)\n",
      "  2 1.673911 1.230770 0.818239(s)\n",
      "  3 1.094629 1.019428 0.801506(s)\n",
      "  4 0.972357 0.955056 0.820380(s)\n",
      "  5 0.934279 0.931868 0.805202(s)\n",
      "  6 0.917778 0.922772 0.820195(s)\n",
      "  7 0.912899 0.918849 0.816041(s)\n",
      "  8 0.909416 0.916609 0.817866(s)\n",
      "  9 0.905903 0.915645 0.803548(s)\n",
      " 10 0.905539 0.915031 0.822293(s)\n",
      " 11 0.906020 0.914658 0.817680(s)\n",
      " 12 0.905828 0.913684 0.807027(s)\n",
      " 13 0.904641 0.913445 0.816479(s)\n",
      " 14 0.903544 0.912101 0.816672(s)\n",
      " 15 0.903661 0.911814 0.810884(s)\n",
      " 16 0.901698 0.910958 0.815182(s)\n",
      " 17 0.901922 0.910442 0.817177(s)\n",
      " 18 0.901818 0.909673 0.822039(s)\n",
      " 19 0.901665 0.908632 0.815462(s)\n",
      " 20 0.898704 0.908108 0.810979(s)\n",
      " 21 0.898182 0.906636 0.797877(s)\n",
      " 22 0.897072 0.905067 0.800199(s)\n",
      " 23 0.893475 0.902746 0.804260(s)\n",
      " 24 0.890784 0.899639 0.806707(s)\n",
      " 25 0.888620 0.897022 0.828493(s)\n",
      " 26 0.884409 0.893831 0.834365(s)\n",
      " 27 0.880300 0.891073 0.839414(s)\n",
      " 28 0.875636 0.888182 0.846145(s)\n",
      " 29 0.873168 0.885293 0.818817(s)\n",
      " 30 0.867848 0.883285 0.825632(s)\n",
      " 31 0.864623 0.880540 0.808099(s)\n",
      " 32 0.859173 0.878052 0.811553(s)\n",
      " 33 0.856752 0.876131 0.817591(s)\n",
      " 34 0.853797 0.874086 0.811504(s)\n",
      " 35 0.850367 0.871518 0.812913(s)\n",
      " 36 0.846165 0.869268 0.815017(s)\n",
      " 37 0.841575 0.866605 0.815969(s)\n",
      " 38 0.837429 0.864576 0.803656(s)\n",
      " 39 0.833781 0.862596 0.802825(s)\n",
      " 40 0.829649 0.860524 0.822510(s)\n",
      " 41 0.824787 0.858340 0.816120(s)\n",
      " 42 0.820235 0.856803 0.825975(s)\n",
      " 43 0.817253 0.855296 0.813568(s)\n",
      " 44 0.814003 0.853899 0.803042(s)\n",
      " 45 0.809298 0.852389 0.812664(s)\n",
      " 46 0.806705 0.851349 0.812845(s)\n",
      " 47 0.802315 0.850185 0.809598(s)\n",
      " 48 0.799913 0.849056 0.825799(s)\n",
      " 49 0.796595 0.848233 0.814594(s)\n",
      " 50 0.793410 0.847708 0.802230(s)\n",
      " 51 0.790517 0.847434 0.808553(s)\n",
      " 52 0.788437 0.847228 0.808407(s)\n",
      " 53 0.785756 0.847140 0.820547(s)\n",
      " 54 0.783955 0.847049 0.816987(s)\n",
      " 55 0.781291 0.846788 0.802100(s)\n",
      " 56 0.779440 0.846515 0.818246(s)\n",
      " 57 0.777782 0.846771 0.813962(s)\n",
      " 58 0.775964 0.846660 0.800619(s)\n",
      " 59 0.775382 0.846398 0.813453(s)\n",
      " 60 0.771583 0.846513 0.828801(s)\n",
      " 61 0.770720 0.846758 0.848235(s)\n",
      " 62 0.769777 0.846473 0.825326(s)\n",
      " 63 0.767153 0.846620 0.833626(s)\n",
      " 64 0.767230 0.846665 0.821234(s)\n",
      " 65 0.765834 0.847007 0.811782(s)\n",
      " 66 0.763741 0.846838 0.830001(s)\n",
      " 67 0.763675 0.847217 0.838178(s)\n",
      " 68 0.762082 0.847183 0.829215(s)\n",
      " 69 0.761257 0.847024 0.837504(s)\n",
      " 70 0.759534 0.847261 0.818754(s)\n",
      " 71 0.760247 0.847136 0.816765(s)\n",
      " 72 0.758160 0.847122 0.810928(s)\n",
      " 73 0.758298 0.847302 0.822301(s)\n",
      " 74 0.757104 0.847541 0.818211(s)\n",
      " 75 0.757011 0.847810 0.825401(s)\n",
      " 76 0.756750 0.847928 0.829165(s)\n",
      " 77 0.755208 0.848169 0.834831(s)\n",
      " 78 0.756212 0.848221 0.837733(s)\n",
      " 79 0.755115 0.848213 0.824955(s)\n",
      " 80 0.753739 0.848462 0.831264(s)\n",
      " 81 0.752863 0.848841 0.814281(s)\n",
      " 82 0.752511 0.849039 0.819370(s)\n",
      " 83 0.753328 0.849022 0.819063(s)\n",
      " 84 0.752422 0.849161 0.833339(s)\n",
      " 85 0.750655 0.849418 0.815384(s)\n",
      " 86 0.751415 0.849434 0.812351(s)\n",
      " 87 0.750742 0.849342 0.812915(s)\n",
      " 88 0.750353 0.849477 0.804506(s)\n",
      " 89 0.751518 0.849563 0.803257(s)\n",
      " 90 0.749395 0.849350 0.823358(s)\n",
      " 91 0.750163 0.849331 0.814389(s)\n",
      " 92 0.749944 0.849390 0.802917(s)\n",
      " 93 0.749335 0.849476 0.815518(s)\n",
      " 94 0.749333 0.849690 0.821764(s)\n",
      " 95 0.749180 0.849666 0.809972(s)\n",
      " 96 0.747516 0.849519 0.808170(s)\n",
      " 97 0.747885 0.849730 0.822996(s)\n",
      " 98 0.747395 0.849964 0.824793(s)\n",
      " 99 0.747468 0.849933 0.821195(s)\n",
      "100 0.747786 0.849828 0.798650(s)\n",
      "101 0.747189 0.849964 0.811156(s)\n",
      "102 0.747287 0.849974 0.824756(s)\n",
      "103 0.748313 0.850054 0.802113(s)\n",
      "104 0.747294 0.850221 0.815457(s)\n",
      "105 0.746409 0.850172 0.813938(s)\n",
      "106 0.747159 0.849859 0.822470(s)\n",
      "107 0.747105 0.849945 0.807643(s)\n",
      "108 0.748091 0.850033 0.811420(s)\n",
      "109 0.746664 0.850085 0.807329(s)\n",
      "110 0.747310 0.850198 0.799192(s)\n",
      "111 0.745524 0.850385 0.815976(s)\n",
      "112 0.746054 0.850370 0.798623(s)\n",
      "113 0.745899 0.850215 0.807174(s)\n",
      "114 0.746425 0.850142 0.799925(s)\n",
      "115 0.745677 0.850121 0.791265(s)\n",
      "116 0.745282 0.850384 0.797932(s)\n",
      "117 0.746231 0.850293 0.815141(s)\n",
      "118 0.744602 0.850275 0.814152(s)\n",
      "119 0.745708 0.850347 0.812612(s)\n",
      "120 0.745962 0.850350 0.827918(s)\n",
      "121 0.745113 0.850192 0.822313(s)\n",
      "122 0.746247 0.850191 0.819115(s)\n",
      "123 0.745155 0.850485 0.811366(s)\n",
      "124 0.745489 0.850370 0.814303(s)\n",
      "125 0.745095 0.850791 0.801710(s)\n",
      "126 0.745875 0.850365 0.810618(s)\n",
      "127 0.744233 0.850489 0.810583(s)\n",
      "128 0.744390 0.850521 0.807130(s)\n",
      "129 0.744629 0.850633 0.807667(s)\n",
      "130 0.744980 0.850644 0.814583(s)\n",
      "131 0.744190 0.850719 0.807896(s)\n",
      "132 0.744626 0.850437 0.805150(s)\n",
      "133 0.744289 0.850550 0.811617(s)\n",
      "134 0.745125 0.850508 0.800303(s)\n",
      "135 0.744895 0.850740 0.817578(s)\n",
      "136 0.744163 0.850807 0.820981(s)\n",
      "137 0.743931 0.850877 0.813394(s)\n",
      "138 0.745101 0.851046 0.802705(s)\n",
      "139 0.744430 0.850826 0.808496(s)\n",
      "140 0.744023 0.850887 0.816072(s)\n",
      "141 0.745199 0.851007 0.809911(s)\n",
      "142 0.744063 0.850918 0.790946(s)\n",
      "143 0.743980 0.850736 0.806861(s)\n",
      "144 0.744905 0.850717 0.816552(s)\n",
      "145 0.742818 0.850795 0.822965(s)\n",
      "146 0.743884 0.850891 0.816171(s)\n",
      "147 0.743480 0.850860 0.828706(s)\n",
      "148 0.743650 0.851042 0.805123(s)\n",
      "149 0.743771 0.850958 0.809513(s)\n",
      "150 0.744125 0.851104 0.800674(s)\n",
      "151 0.743998 0.851095 0.803047(s)\n",
      "152 0.743700 0.850932 0.788182(s)\n",
      "153 0.743144 0.850875 0.818395(s)\n",
      "154 0.744292 0.850819 0.811668(s)\n",
      "155 0.742293 0.850892 0.808520(s)\n",
      "156 0.743740 0.851022 0.848935(s)\n",
      "157 0.743925 0.850941 0.831539(s)\n",
      "158 0.742324 0.851058 0.816591(s)\n",
      "159 0.744002 0.850753 0.808676(s)\n",
      "160 0.743730 0.850912 0.808229(s)\n",
      "161 0.743407 0.850813 0.813254(s)\n",
      "162 0.742365 0.850930 0.818226(s)\n",
      "163 0.743228 0.850995 0.801837(s)\n",
      "164 0.742412 0.850884 0.790542(s)\n",
      "165 0.742850 0.850952 0.813819(s)\n",
      "166 0.744730 0.851063 0.809595(s)\n",
      "167 0.743091 0.851022 0.800910(s)\n",
      "168 0.742283 0.850834 0.811345(s)\n",
      "169 0.743338 0.850743 0.821349(s)\n",
      "170 0.743100 0.850929 0.809533(s)\n",
      "171 0.742771 0.850740 0.806260(s)\n",
      "172 0.743189 0.850863 0.822076(s)\n",
      "173 0.742866 0.850719 0.799617(s)\n",
      "174 0.742484 0.850718 0.805891(s)\n",
      "175 0.742358 0.850597 0.803769(s)\n",
      "176 0.741821 0.850781 0.802290(s)\n",
      "177 0.743183 0.850858 0.810610(s)\n",
      "178 0.742526 0.850792 0.820682(s)\n",
      "179 0.742784 0.850815 0.811155(s)\n",
      "180 0.742902 0.851157 0.813814(s)\n",
      "181 0.742387 0.851164 0.808604(s)\n",
      "182 0.742582 0.851166 0.810794(s)\n",
      "183 0.743379 0.851248 0.809184(s)\n",
      "184 0.742643 0.851188 0.803752(s)\n",
      "185 0.742222 0.851122 0.812889(s)\n",
      "186 0.744064 0.851349 0.799329(s)\n",
      "187 0.743375 0.851187 0.804730(s)\n",
      "188 0.742628 0.851101 0.792427(s)\n",
      "189 0.742189 0.850926 0.794429(s)\n",
      "190 0.742671 0.851224 0.797308(s)\n",
      "191 0.741258 0.851107 0.792662(s)\n",
      "192 0.742265 0.851235 0.792929(s)\n",
      "193 0.741915 0.850993 0.789188(s)\n",
      "194 0.743261 0.850866 0.779860(s)\n",
      "195 0.742721 0.850863 0.798974(s)\n",
      "196 0.743194 0.850972 0.796106(s)\n",
      "197 0.742206 0.850934 0.794043(s)\n",
      "198 0.741358 0.850915 0.788803(s)\n",
      "199 0.741432 0.850976 0.821516(s)\n"
     ]
    }
   ],
   "source": [
    "# 完成实际的训练\n",
    "svd(df_train, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
