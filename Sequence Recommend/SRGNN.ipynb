{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48402f51e46a9e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-23T04:14:04.831333Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import faiss\n",
    "# tf.config.experimental.set_visible_devices([], 'GPU')  # 禁用GPU\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dc7fe172667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class GNN(tf.keras.Model):\n",
    "    def __init__(self, embedding_size, step=1):\n",
    "        super(GNN, self).__init__()\n",
    "        self.step = step\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_size = embedding_size * 2\n",
    "        self.gate_size = embedding_size * 3\n",
    "        \n",
    "        self.w_ih = self.add_weight(shape=[self.input_size, self.gate_size])\n",
    "        self.w_hh = self.add_weight(shape=[self.embedding_size, self.gate_size])\n",
    "        self.b_ih = self.add_weight(shape=[self.gate_size])\n",
    "        self.b_hh = self.add_weight(shape=[self.gate_size])\n",
    "        self.b_iah = self.add_weight(shape=[self.embedding_size])\n",
    "        self.b_ioh = self.add_weight(shape=[self.embedding_size])\n",
    "\n",
    "        self.linear_edge_in = layers.Dense(self.embedding_size)\n",
    "        self.linear_edge_out = layers.Dense(self.embedding_size)\n",
    "\n",
    "    def GNNCell(self, A, hidden):\n",
    "        input_in = tf.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
    "        input_out = tf.matmul(A[:, :, A.shape[1]:], self.linear_edge_out(hidden)) + self.b_ioh\n",
    "        # [batch_size, max_session_len, embedding_size * 2]\n",
    "        inputs = tf.concat([input_in, input_out], axis=2)\n",
    "\n",
    "        # gi.size equals to gh.size, shape of [batch_size, max_session_len, embedding_size * 3]\n",
    "        gi = tf.matmul(inputs, self.w_ih) + self.b_ih\n",
    "        gh = tf.matmul(hidden, self.w_hh) + self.b_hh\n",
    "        # (batch_size, max_session_len, embedding_size)\n",
    "        i_r, i_i, i_n = tf.split(gi, 3, axis=2)\n",
    "        h_r, h_i, h_n = tf.split(gh, 3, axis=2)\n",
    "        reset_gate = tf.sigmoid(i_r + h_r)\n",
    "        input_gate = tf.sigmoid(i_i + h_i)\n",
    "        new_gate = tf.tanh(i_n + reset_gate * h_n)\n",
    "        hy = (1 - input_gate) * hidden + input_gate * new_gate\n",
    "        return hy\n",
    "\n",
    "    def call(self, A, hidden):\n",
    "        for i in range(self.step):\n",
    "            hidden = self.GNNCell(A, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SRGNN(nn.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(SRGNN, self).__init__()\n",
    "\n",
    "        # load parameters info\n",
    "        self.config = config\n",
    "        self.embedding_size = config['embedding_dim']\n",
    "        self.step = config['step']\n",
    "        self.n_items = self.config['n_items']\n",
    "\n",
    "        # define layers and loss\n",
    "        # item embedding\n",
    "        self.item_emb = nn.Embedding(self.n_items, self.embedding_size, padding_idx=0)\n",
    "        # define layers and loss\n",
    "        self.gnn = GNN(self.embedding_size, self.step)\n",
    "        self.linear_one = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.linear_two = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.linear_three = nn.Linear(self.embedding_size, 1, bias_attr=False)\n",
    "        self.linear_transform = nn.Linear(self.embedding_size * 2, self.embedding_size)\n",
    "        self.loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        # parameters initialization\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def gather_indexes(self, output, gather_index):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch\"\"\"\n",
    "#         gather_index = gather_index.view(-1, 1, 1).expand(-1, -1, output.shape[-1])\n",
    "        gather_index = gather_index.reshape([-1, 1, 1])\n",
    "        gather_index = paddle.repeat_interleave(gather_index,output.shape[-1],2)\n",
    "        output_tensor = paddle.take_along_axis(output, gather_index, 1)\n",
    "        return output_tensor.squeeze(1)\n",
    "\n",
    "    def calculate_loss(self,user_emb,pos_item):\n",
    "        all_items = self.item_emb.weight\n",
    "        scores = paddle.matmul(user_emb, all_items.transpose([1, 0]))\n",
    "        return self.loss_fun(scores,pos_item)\n",
    "\n",
    "    def output_items(self):\n",
    "        return self.item_emb.weight\n",
    "\n",
    "    def reset_parameters(self, initializer=None):\n",
    "        for weight in self.parameters():\n",
    "            paddle.nn.initializer.KaimingNormal(weight)\n",
    "\n",
    "    def _get_slice(self, item_seq):\n",
    "        # Mask matrix, shape of [batch_size, max_session_len]\n",
    "        mask = (item_seq>0).astype('int32')\n",
    "        items, n_node, A, alias_inputs = [], [], [], []\n",
    "        max_n_node = item_seq.shape[1]\n",
    "        item_seq = item_seq.cpu().numpy()\n",
    "        for u_input in item_seq:\n",
    "            node = np.unique(u_input)\n",
    "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "            u_A = np.zeros((max_n_node, max_n_node))\n",
    "\n",
    "            for i in np.arange(len(u_input) - 1):\n",
    "                if u_input[i + 1] == 0:\n",
    "                    break\n",
    "\n",
    "                u = np.where(node == u_input[i])[0][0]\n",
    "                v = np.where(node == u_input[i + 1])[0][0]\n",
    "                u_A[u][v] = 1\n",
    "\n",
    "            u_sum_in = np.sum(u_A, 0)\n",
    "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "            u_A_in = np.divide(u_A, u_sum_in)\n",
    "            u_sum_out = np.sum(u_A, 1)\n",
    "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
    "            A.append(u_A)\n",
    "\n",
    "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "        # The relative coordinates of the item node, shape of [batch_size, max_session_len]\n",
    "        alias_inputs = paddle.to_tensor(alias_inputs)\n",
    "        # The connecting matrix, shape of [batch_size, max_session_len, 2 * max_session_len]\n",
    "        A = paddle.to_tensor(A)\n",
    "        # The unique item nodes, shape of [batch_size, max_session_len]\n",
    "        items = paddle.to_tensor(items)\n",
    "\n",
    "        return alias_inputs, A, items, mask\n",
    "\n",
    "    def forward(self, item_seq, mask, item, train=True):\n",
    "        if train:\n",
    "            alias_inputs, A, items, mask = self._get_slice(item_seq)\n",
    "            hidden = self.item_emb(items)\n",
    "            hidden = self.gnn(A, hidden)\n",
    "            alias_inputs = alias_inputs.reshape([-1, alias_inputs.shape[1],1])\n",
    "            alias_inputs = paddle.repeat_interleave(alias_inputs, self.embedding_size, 2)\n",
    "            seq_hidden = paddle.take_along_axis(hidden,alias_inputs,1)\n",
    "            # fetch the last hidden state of last timestamp\n",
    "            item_seq_len = paddle.sum(mask,axis=1)\n",
    "            ht = self.gather_indexes(seq_hidden, item_seq_len - 1)\n",
    "            q1 = self.linear_one(ht).reshape([ht.shape[0], 1, ht.shape[1]])\n",
    "            q2 = self.linear_two(seq_hidden)\n",
    "\n",
    "            alpha = self.linear_three(F.sigmoid(q1 + q2))\n",
    "            a = paddle.sum(alpha * seq_hidden * mask.reshape([mask.shape[0], -1, 1]), 1)\n",
    "            user_emb = self.linear_transform(paddle.concat([a, ht], axis=1))\n",
    "\n",
    "            loss = self.calculate_loss(user_emb,item)\n",
    "            output_dict = {\n",
    "                'user_emb': user_emb,\n",
    "                'loss': loss\n",
    "            }\n",
    "        else:\n",
    "            alias_inputs, A, items, mask = self._get_slice(item_seq)\n",
    "            hidden = self.item_emb(items)\n",
    "            hidden = self.gnn(A, hidden)\n",
    "            alias_inputs = alias_inputs.reshape([-1, alias_inputs.shape[1],1])\n",
    "            alias_inputs = paddle.repeat_interleave(alias_inputs, self.embedding_size, 2)\n",
    "            seq_hidden = paddle.take_along_axis(hidden, alias_inputs,1)\n",
    "            # fetch the last hidden state of last timestamp\n",
    "            item_seq_len = paddle.sum(mask, axis=1)\n",
    "            ht = self.gather_indexes(seq_hidden, item_seq_len - 1)\n",
    "            q1 = self.linear_one(ht).reshape([ht.shape[0], 1, ht.shape[1]])\n",
    "            q2 = self.linear_two(seq_hidden)\n",
    "\n",
    "            alpha = self.linear_three(F.sigmoid(q1 + q2))\n",
    "            a = paddle.sum(alpha * seq_hidden * mask.reshape([mask.shape[0], -1, 1]), 1)\n",
    "            user_emb = self.linear_transform(paddle.concat([a, ht], axis=1))\n",
    "            output_dict = {\n",
    "                'user_emb': user_emb,\n",
    "            }\n",
    "        return output_dict\n"
   ],
   "id": "d0a360b33f342efc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
